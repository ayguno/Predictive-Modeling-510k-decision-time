---
title: "Exploratory analysis of 510k decision time"
author: "Ozan Aygun"
date: "5/16/2018"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.height= 8, fig.width= 8,message=FALSE,warning=FALSE)
```

#Introduction

# Loading and inspecting data

```{r}
# Download the data from:  #https://www.fda.gov/MedicalDevices/ProductsandMedicalProcedures/DeviceApprovalsandClearances/#510kClearances/ucm089428.htm
wdir <- getwd()

# Data from 96
current_96 <- "http://www.accessdata.fda.gov/premarket/ftparea/pmn96cur.zip"
download.file(url = current_96,destfile = paste0(wdir,"/pmn96cur.zip"))
unzip("pmn96cur.zip")
unlink("pmn96cur.zip")

# Data between 91-95
data91_95 <- "http://www.accessdata.fda.gov/premarket/ftparea/pmn9195.zip"
download.file(url = data91_95,destfile = paste0(wdir,"/pmn9195.zip"))
unzip("pmn9195.zip")
unlink("pmn9195.zip")


# Read the data
data_96_current <- read.csv("pmn96cur.txt", sep = "|", stringsAsFactors = FALSE)
data91_95 <- read.csv("pmn9195.txt", sep = "|", stringsAsFactors = FALSE)

# Same columns
identical(names(data_96_current),names(data91_95))

# rBind the data
current <- rbind(data_96_current,data91_95)
library(lubridate)
current$DATERECEIVED <- mdy(current$DATERECEIVED)
current$DECISIONDATE <- mdy(current$DECISIONDATE)

summary(current)

current$DECISIONTIME <- as.numeric(as.character(current$DECISIONDATE - current$DATERECEIVED))
plot(current$DATERECEIVED,current$DECISIONTIME, pch = 19, cex = 0.05, col = "red" )
```

```{r}
current$YEAR <- year(current$DATERECEIVED)
plot(table(current$YEAR))
```

Let's focus on data between 1997 - 2017. We can use cases from 2018 to test the models later on.

```{r, fig.height=20}
library(lubridate)
current <- current[(current$YEAR >= 1997) & (current$YEAR <= 2017) ,]

summary(current)

# Remove one missing entry
current[is.na(current$YEAR),]
current <- current[!is.na(current$YEAR),]

current$WEEKDAY <- weekdays(current$DATERECEIVED)
current$DAYSOFYEAR <- yday(current$DATERECEIVED)
current$DAYSOFMONTH <- mday(current$DATERECEIVED)
current$DAYSOFWEEK <- wday(current$DATERECEIVED)

summary(current)

library(ggplot2)

ggplot(current, aes(x = DAYSOFYEAR , y = DECISIONTIME, col = factor(DECISION)))+
        geom_point(size = 0.5, alpha = 0.5)+
        facet_grid(YEAR ~ .)+
        scale_fill_discrete()
```

There seems not a relationship between decision date and time of the year a submission is received. As we expected, de novo decisions have different timielines:

```{r}
ggplot(current,aes(x = DECISION, y = DECISIONTIME, fill = DECISION))+
        geom_boxplot()+
        scale_color_discrete()
```

```{r}
table(current$DECISION)
```

We will not use de novo cases in our model since they are not originally intended as 510k submissions but they are converted to 510k.

```{r}
current <- current[current$DECISION != "DENG",]

ggplot(current, aes(x = WEEKDAY, y = DECISIONTIME, fill = WEEKDAY))+
        geom_boxplot()
```

WEEKDAY does not matter. 

```{r}
table(current$EXPEDITEDREVIEW)
ggplot(current, aes(x = EXPEDITEDREVIEW, y = DECISIONTIME))+
        geom_boxplot()
```

It does not seem to matter either.  

```{r}
# There are 2538 product codes in this data set
length(unique(current$PRODUCTCODE))
sum(is.na(current$PRODUCTCODE))

# What is the median decision time for different product codes?
library(dplyr)

pdec <- current %>% group_by(PRODUCTCODE) %>% summarise(med.dec.time = median(DECISIONTIME), count = n()) %>% arrange(desc(med.dec.time))


head(pdec)
tail(pdec)
```
```{r}
hist(log10(pdec$count), breaks = 100, col = "navy")
```

Most product codes are rare, but they may contain important information about the decision time. We would like to keep this feature and eventually convert to dummy variables. However, the caveat is that when we split the data set into training and test sets, then about 700 product codes will only appear in one of the sets. 

```{r}
# How about being ane experienced applicant?
length(unique(current$APPLICANT))

# We notice many applicants have more than one clearances to date, let's see if the prior experience shortens the decision time?

current <- current[order(current$DATERECEIVED),]

for(i in 1:nrow(current)){
 current$APPLICANT_PRIOR_CLEARANCE_TO_DATE[i] <- sum(current$APPLICANT[1:i] == current$APPLICANT[i])     
        
}
```

```{r}
hist(current$APPLICANT_PRIOR_CLEARANCE_TO_DATE, col = "navy")
```


```{r}
library(ggplot2)

ggplot(current,aes(x = APPLICANT_PRIOR_CLEARANCE_TO_DATE, y = DECISIONTIME))+
        geom_point(size = 0.2, col = "navy", alpha = 0.08)+
        geom_smooth(method = "lm")
```

It looks like there is some decreasing trend as we expected.

```{r}
library(dplyr)
results <- current %>% group_by(APPLICANT_PRIOR_CLEARANCE_TO_DATE) %>% summarise(median_decision_time = median(DECISIONTIME)) 

ggplot(current, aes(x = APPLICANT_PRIOR_CLEARANCE_TO_DATE, y = log10(DECISIONTIME)))+
        geom_point(col = "blue", alpha = 0.2, size = 0.02)
```

It was misleading because the skew in the data, it looks like noise instead.

How about folow up instances of the same device, does it make any difference?

```{r}
# Most devices only come once, about 5000 devices have multiple clearances
length(unique(current$DEVICENAME))

for(i in 1:nrow(current)){
 current$DEVICENAME_PRIOR_CLEARANCE_TO_DATE[i] <- sum(current$DEVICENAME[1:i] == current$DEVICENAME[i])     
        
}
```

This one also looks noise:

```{r}
ggplot(current,aes(x = DEVICENAME_PRIOR_CLEARANCE_TO_DATE, y = log10(DECISIONTIME)))+
        geom_point(col = "navy", alpha = 0.3, size = 0.5)
```

We don't expect some features would be useful let's remove them:

```{r}
library(dplyr)
current <- dplyr::select(current, -APPLICANT, - CONTACT, - STREET1,
                         - STREET2,- CITY)
```

```{r}
table(current$STATE)
sum(is.na(current$STATE))
sum(current$STATE == "")
```
```{r}
table(current$COUNTRY_CODE)
```

```{r}
summary(lm(DECISIONTIME ~ factor(COUNTRY_CODE), data = current))
par(mfrow = c(2,2))
plot(lm(DECISIONTIME ~ factor(COUNTRY_CODE), data = current))
```

Let's look at that  outlier data points with over 3000 days of decision time:

```{r}
current[which(current$DECISIONTIME > 3000),]
```

These entries might have something special we could not understand. Almost 9 years of decision time is not very realistic. In order to reduce the chance of analyzing erronous data, we will remove these 3 entries which appear to relate same type of product.

```{r}
current <- current[which(current$DECISIONTIME <3000),]
```


```{r}
# Order the factor levels of COUNTRY_CODE based on median decision time of each group
fac <- with(current, reorder(COUNTRY_CODE, DECISIONTIME, median, order = TRUE))
current$COUNTRY_CODE <- factor(current$COUNTRY_CODE, levels = levels(fac))

ggplot(current, aes(x = factor(COUNTRY_CODE), y = DECISIONTIME))+
        geom_boxplot()+
        coord_flip()
```
Does not look significantly different, but worth to keep in the list of features.

```{r}
# Fill the empty states entries with 'NON_US'
current$STATE[which(current$STATE == "")] <- "NON_US"
table(current$STATE)
```

```{r}
fac <- with(current, reorder(STATE,DECISIONTIME,median,order = TRUE))
current$STATE <- factor(current$STATE, levels = levels(fac))

ggplot(current, aes(x = STATE, y = DECISIONTIME))+
        geom_boxplot()+
        coord_flip()
```

Again, states don't look too different, but we will still keep for binary features.

```{r}
# Remove zipcode features
current <-  dplyr::select(current,- ZIP, POSTAL_CODE )
```

```{r}
current %>% group_by(DECISION) %>% summarise(median_dectime = median(DECISIONTIME),
                                             count = n())

```

We don't need to keep this one since since the exact decision code won't be known before any submission.

```{r}
current <- dplyr::select(current, - DECISION)
```

```{r}
table(current$REVIEWADVISECOMM)
fac <- with(current, reorder(REVIEWADVISECOMM,DECISIONTIME,median,order = TRUE))
current$REVIEWADVISECOMM <- factor(current$REVIEWADVISECOMM, levels = levels(fac))

ggplot(current, aes(x = REVIEWADVISECOMM, y = DECISIONTIME))+
        geom_boxplot()
       
```

Let's keep this feature.

```{r}
ggplot(current, aes(x = STATEORSUMM, y = DECISIONTIME))+
        geom_boxplot()
table(current$STATEORSUMM)       
```

This does not have much value, but keep it for now until we think a strategy to retrieve these statement/summary text.

```{r}
identical(as.character(current$REVIEWADVISECOMM), current$CLASSADVISECOMM)
```
```{r}
table(current$CLASSADVISECOMM)
fac <- with(current, reorder(CLASSADVISECOMM,DECISIONTIME,median,order = TRUE))
current$CLASSADVISECOMM <- factor(current$CLASSADVISECOMM, levels = levels(fac))

ggplot(current, aes(x = CLASSADVISECOMM, y = DECISIONTIME))+
        geom_boxplot()
```

Keep for now.

```{r}
# Remove this feature
unique(current$SSPINDICATOR)
current <- dplyr::select(current,-SSPINDICATOR)
```

```{r}
table(current$TYPE)
fac <- with(current, reorder(TYPE,DECISIONTIME,median,order = TRUE))
current$TYPE <- factor(current$TYPE, levels = levels(fac))

ggplot(current, aes(x = TYPE, y = DECISIONTIME))+
        geom_boxplot()
```
As we expected, special 510k's have shorter review time. This is a good feature to keep.

```{r}
# Remove this one
current <- dplyr::select(current, - THIRDPARTY)
```

```{r}
# Keep this one
table(current$EXPEDITEDREVIEW)
ggplot(current, aes(x = EXPEDITEDREVIEW, y = DECISIONTIME))+
        geom_boxplot()

current$EXPEDITEDREVIEW[current$EXPEDITEDREVIEW == ""] <- "N"
```

We will keep Device names for tokenization and feature extraction using NLP.

This is the version of the data set we will keep for some plotting later.

```{r}
# save the current version of the dataset to continue working later
saveRDS(current,"current.rds")
```

# Processing the data set for predictive modeling

Here we will process the features we currently have, to create a new data set where we will only have:

- Target feature (DECISIONTIME)
- Features to be vectorized into dummy variables:
       - attach the feature name to value , then combine them into a single         feature for a given data entry. Thye will become unique features and will be                binarized using tokenization later.
- Features to be tokenized as n_grams (DEVICENAME)
- Two continuous features (APPLICANT anf DEVICE name prior clearance to date)

We will start by removing all other features we won't use for medeling, expect the KNUMBER, which we will keep to explore decision summary mining later.

```{r}
processing <- dplyr::select(current, KNUMBER,DECISIONTIME,STATE,COUNTRY_CODE,
                            REVIEWADVISECOMM,PRODUCTCODE,STATEORSUMM,
                            CLASSADVISECOMM,TYPE,EXPEDITEDREVIEW,DEVICENAME,
                            APPLICANT_PRIOR_CLEARANCE_TO_DATE,
                            DEVICENAME_PRIOR_CLEARANCE_TO_DATE)

# Save the current version of the processing 
saveRDS(processing,"processing.rds")
```

```{r}
###################################################
# Write a function to process the data as we need:
###################################################

binary_processer <- function(processing){

require(dplyr)

temp <- processing

binary_features <- c("STATE","COUNTRY_CODE",
                            "REVIEWADVISECOMM","PRODUCTCODE","STATEORSUMM",
                            "CLASSADVISECOMM","TYPE","EXPEDITEDREVIEW")
# Attach the binary feature names to values, remove any "_" 
for (i in 1:length(binary_features)){
        feature_index <- which(names(temp) == binary_features[i])
        temp[,feature_index] <-  sapply(temp[,feature_index], function(x){
                return(gsub(pattern = "_",replacement = "",x =paste0(binary_features[i],x)))
        })
}

# For each data point, Combine binary features into a single string feature using a 
# single white space for binary vectorization/tokenization later

temp <- within(temp, binarized_features <- tolower(paste(STATE,COUNTRY_CODE,
                            REVIEWADVISECOMM,PRODUCTCODE,STATEORSUMM,
                            CLASSADVISECOMM,TYPE,EXPEDITEDREVIEW, sep = " ")))
# Remove the combined features
temp <- dplyr::select(temp,-c(STATE,COUNTRY_CODE,
                            REVIEWADVISECOMM,PRODUCTCODE,STATEORSUMM,
                            CLASSADVISECOMM,TYPE,EXPEDITEDREVIEW))

return(temp)

}

```

```{r}
# Process the data set using binary processer and save rds and csv versions

processing <- binary_processer(processing)
saveRDS(processing,"processing.rds")
write.csv(processing,file = "processing.csv", row.names = FALSE)
```

# Exploring strategies to obtain text features from publicly available summary/statement data

```{r}
# For the entries where STATAMENTORSUMM == "Summary"" The main pdf directory seems to be at:

pdfmain <- "https://www.accessdata.fda.gov/cdrh_docs/pdf/"


#########################################################################
# PDFs as images : mostly older PDFs that can not be extracted directly
#########################################################################


# For example:
K_number <- "K970011"

pdfurl <- paste0(pdfmain,K_number,".pdf") 

require(pdftools)

# Example to see the pdf_reading function works
txt <- pdf_text("https://arxiv.org/pdf/1406.4806.pdf")

# Note that indeed this function can access to 8 pages of the document, but since
# PDF was not saved in a text searchable format, it can not read the content
pdf_510k <- pdf_text(pdfurl)
pdf_510k

# The pdf is an image object, for this cases we can try to use tesseract
# package for optical text recognition

require(tesseract)
txt <- ocr(image = pdfurl)


# tesserract package can not use pdfs directly, so we need to convert them
# to images beforehand

# This step cpnverts each pdf page into a png file and stores created image file names
# Note that setting dpi (resolution) to a good value is critical to extract useful
# information by using text recognition
images <- pdf_convert(pdf = pdfurl, format = "png", dpi = 400)

txt <- NULL
for(i in 1:length(images)){
txt[i] <- ocr(image = images[i])
cat("Completed extracting image:",i, "\n")
}
txt <- paste0(txt,sep = " ", collapse = "~")
unlink(images)
# This way we can extract useful text from images iteratively
# Retain only alphanumerical characters and whitespace, to reduce storage burden
txt <- gsub("[^A-Za-z0-9 ]","",txt)
# Retrieves very useful information with text recognition:
cat(txt)
# Once we are done with a file we can delete them using unlink()

# Perhaps we can also perform a spellcheck using hunspell package at this point
library(hunspell)
bad_words <- hunspell(txt)
bad_words[[1]]

suggestions <- hunspell_suggest(bad_words[1][[1]])

# Get the first suggestion for each bad word
suggestions <- sapply(suggestions, function(x){
        return(x[1])
})

conversion_table <- data.frame(bad_words = bad_words[1][[1]],
                               suggestions = suggestions, stringsAsFactors = FALSE)

# Note that we still have errors, but it helps for certain words
head(conversion_table,20)

# Don't keep NAs
conversion_table <- conversion_table[complete.cases(conversion_table),]

txt1 = txt
# Last step is to replace all bad words with the selected ones
for (replacing in 1:nrow(conversion_table)){
         txt1 <- gsub(pattern = conversion_table$bad_words[replacing],
             replacement = conversion_table$suggestions[replacing],
             x = txt1)
         
}

txt1 <- tolower(txt1)

##################################################
# Directly extractable PDFs : mostly newer PDFs
##################################################
        
# Another 510k Summary example, which is searchable
K_number <- "K173935"

# Note that for 2017, the mainurl is now different
pdfmain <- "https://www.accessdata.fda.gov/cdrh_docs/pdf17/"

pdfurl <- paste0(pdfmain,K_number,".pdf") 
# Note that wrapping the pdf_url into paste and collapse ~ combines all the pages 
# of the pdf text into a single character string object:
pdf_510k <- paste(pdf_text(pdfurl), sep = " ", collapse = "~")
# head(pdf_510k)

# Note that in this case parsing is sucessfull and it is very fast. 

```

Therefore, we conclude that:

 - For the devices where summary statement is available, this can be reached online.
 - Only certain (perhaps more recent) pdfs are searchable. Others require using text recognition
 - The base url might differ based on the year of the 510k
 
 Based on these observations, we will devise a strategy to programmatically parse these text features and use in NLP.
 
# PDF web scraping for text feature extraction

Our strategy should be parsing the text into a single string for each 510k, and eventually storing in a 3 column data frame, first column is K_number to link the data later on, the second column is the text. Third column counts if and hpw many times the Knumber is found in the scraped text, as a confidence measure.  

- This data will be very hard to store in the memory, therefore, we will continuously write into hard drive as we parse the data.

- Since we don't know which available PDF files will be readable, we will first attempt reading directly from the source, if this fails, then we will then start downloading the pdfs as high resolution png images, perform text recognition, and read and process them afterwards. 

- We should attempt to remove all special characters and retain only alphanumeric characters in lowercase letters in the text before storing them, in order to reduce the storage burden.

- In this script, we need to perform error handling since there may be broken urls that just return error messages. We will use tryCatch() or try() to wrap certain queries.

```{r}
current <- readRDS("current.rds")

# Understanding base filepaths for pdfs

# Some examples:
# 1997
# https://www.accessdata.fda.gov/cdrh_docs/pdf/K974889.pdf

# 1998
# https://www.accessdata.fda.gov/cdrh_docs/pdf/K984616.pdf

# 1999
# https://www.accessdata.fda.gov/cdrh_docs/pdf/K974889.pdf

# 2000
# It looks like no data is available for devices cleared in 2000

# 2001
# https://www.accessdata.fda.gov/cdrh_docs/pdf/K014294.pdf

# 2002
# https://www.accessdata.fda.gov/cdrh_docs/pdf2/K024360.pdf

# 2003
# https://www.accessdata.fda.gov/cdrh_docs/pdf3/K034059.pdf

# 2009
# https://www.accessdata.fda.gov/cdrh_docs/pdf9/K094050.pdf

# 2010
# https://www.accessdata.fda.gov/cdrh_docs/pdf10/K103832.pdf

# 2017
# https://www.accessdata.fda.gov/cdrh_docs/pdf17/K173958.pdf

```

```{r, eval = FALSE}
##########################################
# Parsing PDF files 
##########################################
require(hunspell)
require(pdftools)
require(stringr)
require(tesseract)

current <- readRDS("current.rds")
current <- current[order(current$YEAR, decreasing = TRUE),]
# Keep only documents with potentially available summary statements
current_summary <- current[current$STATEORSUMM == "Summary",]
# Remove documents from 2000, since we learned they are not available
yindx <- substr(as.character(current_summary$KNUMBER),2,3)
remove2000 <- which(yindx != "00")
current_summary <- current_summary[remove2000,]

# Write the first document to start the file
K_number <- current_summary$KNUMBER[1]
# Note that for 2017, the mainurl is now different
pdfmain <- "https://www.accessdata.fda.gov/cdrh_docs/pdf17/"
pdfurl <- paste0(pdfmain,K_number,".pdf") 
# Note that wrapping the pdf_url into paste and collapse ~ combines all the pages 
# of the pdf text into a single character string object:
txt <- paste(pdf_text(pdfurl), sep = " ", collapse = "~")
closeAllConnections()

txt <- tolower(gsub("[^A-Za-z0-9 ]","",txt))
# Remove the extra white space
txt <- gsub("\\s+", " ", str_trim(txt))
count <-str_count(txt,tolower(K_number))
txt_frame <- data.frame(KNUMBER = K_number, txt = txt, 
                        Kcount = count, stringsAsFactors = F)
write.table(txt_frame, "Ksummary_text.csv", col.names = T, row.names = F, sep = ",")

successfull = 1

# Enter main scraping loop in which we will scrape other documents
for (i in 2:nrow(current_summary)){
        
K_number <- current_summary$KNUMBER[i]        
yindx <- substr(as.character(K_number),2,3)

if (yindx >17 ) 
        { yindx <- ""
}else if (yindx < 10)
        { yindx <- substr(as.character(yindx),2,2)
}
 
###################################
# First try to read directly
###################################

pdfurl <- paste0("https://www.accessdata.fda.gov/cdrh_docs/pdf",
                  yindx,"/",K_number,".pdf")
is_this_error <- try(txt <- paste(pdf_text(pdfurl), sep = " ", collapse = "~"))
closeAllConnections()
# Retain alphanumeric letters
txt <- tolower(gsub("[^A-Za-z0-9 ]","",is_this_error))
# Remove the extra white space
txt <- gsub("\\s+", " ", str_trim(txt))
count <-str_count(txt,tolower(K_number))

# if count is successful store the text data
if (count > 0){
       txt_frame <- data.frame(KNUMBER = K_number, txt = txt, 
                                Kcount = count, stringsAsFactors = F)
       
# if file is not accessible try alternative an source
} else if ((txt == "error in openconnectioncon rb cannot open connection") & count == 0){
       
        pdfurl <- paste0("https://www.accessdata.fda.gov/cdrh_docs/reviews/",K_number,".pdf")
        is_this_error <- try(txt <- paste(pdf_text(pdfurl), sep = " ", collapse = "~"))
        closeAllConnections()
        # Retain alphanumeric letters
        txt <- tolower(gsub("[^A-Za-z0-9 ]","",is_this_error))
        # Remove the extra white space
        txt <- gsub("\\s+", " ", str_trim(txt))
        count <-str_count(txt,tolower(K_number))
        # if count is successful store the text data
        if (count > 0){
        txt_frame <- data.frame(KNUMBER = K_number, txt = txt, 
                                Kcount = count, stringsAsFactors = F)
        cat("Alternative source parsed document:", K_number, "\n")
        # if this is again connection problem, this means the file does not exist 
        }else if ((txt == "error in openconnectioncon rb cannot open connection") & count == 0){
        txt_frame = NULL     
        }else{
                # At this point the file may exist in the alternative url, but it might be image
                # Try text recognition
                 cat("The file may exist in the alternative url, but it might be image document:", K_number, "\n")
                 cat("Try text recognition in the alternative url:", K_number, "\n")
                # Start by converting to image(s)
                try(images <- pdf_convert(pdf = pdfurl, format = "png", dpi = 400))
                closeAllConnections()
                txt <- NULL
                try(for(j in 1:length(images)){
                txt[j] <- ocr(image = images[j])
                cat("Completed extracting image:",j, "\n")
                })
                txt <- paste0(txt,sep = " ", collapse = "~")
                unlink(images)
                txt <- gsub("[^A-Za-z0-9 ]","",txt)
                # Perform a spellcheck using hunspell package at this point
                cat("Performing a spellcheck...\n")
                bad_words <- hunspell(txt)
                bad_words[[1]]
                suggestions <- hunspell_suggest(bad_words[1][[1]])
                # Get the first suggestion for each 'bad word'
                suggestions <- sapply(suggestions, function(x){
                return(x[1])})
                if(length(suggestions) > 0){
                       conversion_table <- data.frame(bad_words = bad_words[1][[1]],
                                       suggestions = suggestions, stringsAsFactors = FALSE)
                        # Don't keep NAs
                        conversion_table <- conversion_table[complete.cases(conversion_table),]
                        if (nrow(conversion_table) >0){
                                # Last step is to replace all bad words with the selected ones
                                for (replacing in 1:nrow(conversion_table)){
                                        txt <- gsub(pattern = conversion_table$bad_words[replacing],
                                        replacement = conversion_table$suggestions[replacing],
                                                x = txt)}
                        }
                         
                } 
                cat("...completed spellcheck.\n")
                txt <- gsub("[^A-Za-z0-9 ]","",txt)
                # Remove the extra white space and convert to lowercase
                txt <- tolower(gsub("\\s+", " ", str_trim(txt)))
                count <-str_count(txt,tolower(K_number))
                
                if(count > 0 ){
                        cat("Text recognition from the alternative url is sucessful for:", K_number, "\n")
                        txt_frame <- data.frame(KNUMBER = K_number, txt = txt, 
                                                Kcount = count, stringsAsFactors = F)
                }else{
                        txt_frame <- NULL 
                }
                        
        }
        
# if the file seem to be accesible from the first link but could't read the pdf, 
# the file could be image OR the text in the original link does not contain the K_number
# First look at if alternative link has text that can contain K_number, if yes store it        
# If the above fails try to recover data using text recognition. 
# If all fails, check for another common adobe error, if txt is not that error, store it otherwise skip       
} else if ((txt != "error in openconnectioncon rb cannot open connection") & count == 0){
        
        # First look at if alternative link has text that can contain K_number, if yes store it
        pdfurl <- paste0("https://www.accessdata.fda.gov/cdrh_docs/reviews/",K_number,".pdf")
        is_this_error <- try(txt <- paste(pdf_text(pdfurl), sep = " ", collapse = "~"))
        closeAllConnections()
        # Retain alphanumeric letters
        txt1 <- tolower(gsub("[^A-Za-z0-9 ]","",is_this_error))
        # Remove the extra white space
        txt1 <- gsub("\\s+", " ", str_trim(txt1))
        count <-str_count(txt1,tolower(K_number))
        # if count is successful store the text data
        if (count > 0){
        txt_frame <- data.frame(KNUMBER = K_number, txt = txt1, 
                                Kcount = count, stringsAsFactors = F)
        cat("Alternative source parsed document:", K_number, "\n")
        # if this is again connection problem, this means the file does not exist 
        }else if ((txt1 == "error in openconnectioncon rb cannot open connection") & count == 0){
                txt_frame = NULL     
        }else{
                # At this point the file may exist in the alternative url, but it might be image
                # Try text recognition
                 cat("The file may exist in the alternative url, but it might be image document:", K_number, "\n")
                 cat("Try text recognition in the alternative url:", K_number, "\n")
                # Start by converting to image(s)
                try(images <- pdf_convert(pdf = pdfurl, format = "png", dpi = 400))
                closeAllConnections()
                txt3 <- NULL
                try(for(j in 1:length(images)){
                txt3[j] <- ocr(image = images[j])
                cat("Completed extracting image:",j, "\n")
                })
                txt3 <- paste0(txt3,sep = " ", collapse = "~")
                unlink(images)
                txt3 <- gsub("[^A-Za-z0-9 ]","",txt3)
                # Perform a spellcheck using hunspell package at this point
                cat("Performing a spellcheck...\n")
                bad_words <- hunspell(txt3)
                suggestions <- hunspell_suggest(bad_words[1][[1]])
                # Get the first suggestion for each 'bad word'
                suggestions <- sapply(suggestions, function(x){
                return(x[1])})
                if(length(suggestions) > 0){
                       conversion_table <- data.frame(bad_words = bad_words[1][[1]],
                                       suggestions = suggestions, stringsAsFactors = FALSE)
                        # Don't keep NAs
                        conversion_table <- conversion_table[complete.cases(conversion_table),]
                        if (nrow(conversion_table) >0){
                                # Last step is to replace all bad words with the selected ones
                                for (replacing in 1:nrow(conversion_table)){
                                        txt3 <- gsub(pattern = conversion_table$bad_words[replacing],
                                        replacement = conversion_table$suggestions[replacing],
                                                x = txt3)}
                        }
                         
                } 
                cat("...completed spellcheck.\n")
                # Remove the extra white space and convert to lowercase
                txt3 <- tolower(gsub("\\s+", " ", str_trim(txt3)))
                count <-str_count(txt3,tolower(K_number))
                
                if(count > 0 ){
                        cat("Try text recognition from the alternative url is sucessful for:", K_number, "\n")
                        txt_frame <- data.frame(KNUMBER = K_number, txt = txt3, 
                                                Kcount = count, stringsAsFactors = F)
                }else{
                        txt_frame <- NULL 
                }
        
        }
        
        # If the above fails try to recover data using text recognition from the first link
        if(is.null(txt_frame)){
                # Get back the first link
                pdfurl <- paste0("https://www.accessdata.fda.gov/cdrh_docs/pdf",
                  yindx,"/",K_number,".pdf")
                # Start by converting to image(s)
                try(images <- pdf_convert(pdf = pdfurl, format = "png", dpi = 400))
                closeAllConnections()
                txt2 <- NULL
                try(for(j in 1:length(images)){
                txt2[j] <- ocr(image = images[j])
                cat("Completed extracting image:",j, "\n")
                })
                txt2 <- paste0(txt2,sep = " ", collapse = "~")
                unlink(images)
                txt2 <- gsub("[^A-Za-z0-9 ]","",txt2)
                # Perform a spellcheck using hunspell package at this point
                cat("Performing a spellcheck...\n")
                bad_words <- hunspell(txt2)
                suggestions <- hunspell_suggest(bad_words[1][[1]])
                # Get the first suggestion for each 'bad word'
                suggestions <- sapply(suggestions, function(x){
                return(x[1])})
                if(length(suggestions) > 0){
                       conversion_table <- data.frame(bad_words = bad_words[1][[1]],
                                       suggestions = suggestions, stringsAsFactors = FALSE)
                        # Don't keep NAs
                        conversion_table <- conversion_table[complete.cases(conversion_table),]
                        if (nrow(conversion_table) >0){
                                # Last step is to replace all bad words with the selected ones
                                for (replacing in 1:nrow(conversion_table)){
                                        txt2 <- gsub(pattern = conversion_table$bad_words[replacing],
                                        replacement = conversion_table$suggestions[replacing],
                                                x = txt2)}
                        }
                         
                } 
                cat("...completed spellcheck.\n")
                # Remove the extra white space and convert to lowercase
                txt2 <- tolower(gsub("\\s+", " ", str_trim(txt2)))
                count <-str_count(txt2,tolower(K_number))
                
                if(count > 0 ){
                        cat("Text recognition from the first url is sucessful for:", K_number, "\n")
                        txt_frame <- data.frame(KNUMBER = K_number, txt = txt2, 
                                                Kcount = count, stringsAsFactors = F)
                        # If all fails, check for another common adobe error. If txt is not that error, store it, otherwise skip
                }else if (txt != "for the best experience open this pdf portfolio in acrobat x or adobe reader x or later get adobe reader now"){
                cat("All attempts failed to recover K_number, but storing adobe error-free, potentially useful text for:", K_number, "\n")
                txt_frame <- data.frame(KNUMBER = K_number, txt = txt, 
                                                Kcount = count, stringsAsFactors = F)
                }else{
                        cat("Adobe error is detected for:", K_number, "\n")
                        txt_frame <- NULL 
                }
        }                        
}


if(!is.null(txt_frame)){
  # Continue writing into the same file by appending into it 
        write.table(txt_frame, "Ksummary_text.csv", ,sep = ",", append = T, 
             col.names = F, row.names = F)    
        succesfull = successfull + 1
        cat("Completed parsing document:", K_number, "\n")      
        
}else{
        cat("All attempts failed parsing document:", K_number, "\n") 
        cat("Skipping document:", K_number, "\n") 
}

cat("The number of documents scanned so far: ", i ,"\n")
cat("The number of succesful scans so far: ", successfull ,"\n") 
cat(strrep("*", 40),"\n")

}


```


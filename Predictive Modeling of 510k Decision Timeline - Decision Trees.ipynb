{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision-tree based algorithms for prediction\n",
    "\n",
    "In this part of the work, we will try to train decision-tree based algorithms to make predicitions about the timeline.\n",
    "\n",
    "### Building feature extraction pipeline\n",
    "\n",
    "Tree-based algorithms would require feature selection, so we will need to use a reasonable number of features. We will start by including 300 best features using our tokenization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the work into a dedicated workspace \"ridge\"\n",
    "disk_tree = \"tree\"\n",
    "import os\n",
    "if not os.path.exists(disk_tree):\n",
    "    os.makedirs(disk_tree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree1 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression, 300))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load Training and Validation sets\n",
    "disk = \"D:\\Data_science\\GitHub\\Predictive-Modeling-510k-decision-time\"\n",
    "# Validation set \n",
    "with open(disk+\"\\X_val.pkl\",\"rb\") as f:\n",
    "    X_val=pickle.load(f)\n",
    "\n",
    "with open(disk+\"\\y_val.pkl\",\"rb\") as f:\n",
    "    y_val=pickle.load(f)\n",
    "    \n",
    "# Training set (Locked down)\n",
    "with open(disk+\"\\X_train.pkl\",\"rb\") as f:\n",
    "    X_train=pickle.load(f)\n",
    "\n",
    "with open(disk+\"\\y_train.pkl\",\"rb\") as f:\n",
    "    y_train=pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.7833333333333333 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree = pipeline510k_tree1.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree = pipeline510k_tree1.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 300)\n",
      "(15899, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree.shape)\n",
    "print(X_val_trans_tree.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regresion\n",
    "\n",
    "We will first train an untuned Gradient Boosting algorithm to see performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7446            2.62m\n",
      "         2           0.7138            2.48m\n",
      "         3           0.6883            2.46m\n",
      "         4           0.6669            2.41m\n",
      "         5           0.6498            2.41m\n",
      "         6           0.6349            2.42m\n",
      "         7           0.6224            2.42m\n",
      "         8           0.6123            2.42m\n",
      "         9           0.6037            2.40m\n",
      "        10           0.5963            2.40m\n",
      "        20           0.5585            2.29m\n",
      "        30           0.5420            2.15m\n",
      "        40           0.5296            2.01m\n",
      "        50           0.5196            1.89m\n",
      "        60           0.5103            1.76m\n",
      "        70           0.5023            1.63m\n",
      "        80           0.4953            1.48m\n",
      "        90           0.4888            1.35m\n",
      "       100           0.4827            1.21m\n",
      "       200           0.4350            0.00s\n",
      "Median Absolute Error:  37.452053685765335\n",
      "Completed model fit and predictions in: 2.25 minutes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "gbm1 = GradientBoostingRegressor(verbose = 1, n_estimators= 200, max_depth=5)\n",
    "\n",
    "gbm1.fit(X_train_trans_tree, np.log(y_train))\n",
    "preds = gbm1.predict(X_val_trans_tree)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed: 49.8min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 75.3min\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed: 115.8min\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 125.9min\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed: 170.8min\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed: 183.3min\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed: 204.8min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 217.8min\n",
      "[Parallel(n_jobs=6)]: Done 108 out of 108 | elapsed: 250.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 108 out of 108 | elapsed: 250.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed GridSearch in: 259.8333333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,500,1000],\n",
    "    'max_depth':[5,10,15],\n",
    "    'learning_rate': [0.1,0.25,0.75,1]\n",
    "}\n",
    "\n",
    "# We have 8 CPU cores, we will use 6 for this task\n",
    "gbmSearch1 = GridSearchCV(estimator= GradientBoostingRegressor(), \n",
    "                            param_grid= param_grid,\n",
    "                            n_jobs = 6,\n",
    "                            cv = 3,\n",
    "                            verbose = 10, scoring= 'neg_median_absolute_error'\n",
    "                            )\n",
    "gbmSearch1.fit(X_train_trans_tree, np.log(y_train))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed GridSearch in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=10, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbmSearch1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.38059952933966"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "# Let's see the validation MAE (in original target scale) from the best estimator\n",
    "preds = gbmSearch1.best_estimator_.predict(X_val_trans_tree)\n",
    "median_absolute_error(y_val,np.exp(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting\n",
    "\n",
    "Let's try to use relatively new ligtGBM package to see the performance. The package has a sklearn interface we will use to perform hyperparameter optimization as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  36.951519001311084\n",
      "Completed model fit and predictions in: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression')\n",
    "\n",
    "lgbm1.fit(X_train_trans_tree, np.log(y_train))\n",
    "preds = lgbm1.predict(X_val_trans_tree)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that lightGBM is indeed extremely faster and performed better than even the tuned traditional GBM. We will experiment this algorithm and attempt hyperparameter optimization. Let's define a new pipeline without feature selection to also experiment the impact of adding more features on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree2 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()) # Scale the features\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.75 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree2 = pipeline510k_tree2.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree2 = pipeline510k_tree2.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 262146)\n",
      "(15899, 262146)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree2.shape)\n",
    "print(X_val_trans_tree2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  29.467624324930185\n",
      "Completed model fit and predictions in: 1.3666666666666667 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "Xt = SelectKBest(f_regression,10000).fit(X_train_trans_tree2,y_train).transform(X_train_trans_tree2)\n",
    "Xv = SelectKBest(f_regression,10000).fit(X_train_trans_tree2,y_train).transform(X_val_trans_tree2)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                          n_estimators=200, reg_alpha= 0.1,\n",
    "                          boosting_type= 'dart')\n",
    "\n",
    "lgbm1.fit(Xt, np.log(y_train))\n",
    "preds = lgbm1.predict(Xv)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lgbm is robust to overfitting when adding more features. Let's try to first search for the number of features to be included in the fixed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model using 300\n",
      "Completed model fit and predictions using 300 in: 0.1 minutes.\n",
      "Median Absolute Error:  33.107162652136324\n",
      "**************************************************\n",
      "Training model using 5000\n",
      "Completed model fit and predictions using 5000 in: 0.8833333333333333 minutes.\n",
      "Median Absolute Error:  29.633671997998164\n",
      "**************************************************\n",
      "Training model using 10000\n",
      "Completed model fit and predictions using 10000 in: 1.4 minutes.\n",
      "Median Absolute Error:  29.467624324930185\n",
      "**************************************************\n",
      "Training model using 20000\n",
      "Completed model fit and predictions using 20000 in: 1.6666666666666667 minutes.\n",
      "Median Absolute Error:  29.30898855444397\n",
      "**************************************************\n",
      "Training model using 50000\n",
      "Completed model fit and predictions using 50000 in: 1.9 minutes.\n",
      "Median Absolute Error:  29.209825132888795\n",
      "**************************************************\n",
      "Training model using 100000\n",
      "Completed model fit and predictions using 100000 in: 2.0166666666666666 minutes.\n",
      "Median Absolute Error:  29.166814835270287\n",
      "**************************************************\n",
      "Training model using 200000\n",
      "Completed model fit and predictions using 200000 in: 2.066666666666667 minutes.\n",
      "Median Absolute Error:  29.19324890053349\n",
      "**************************************************\n",
      "Training model using 262146\n",
      "Completed model fit and predictions using 262146 in: 2.066666666666667 minutes.\n",
      "Median Absolute Error:  29.478673965840073\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "n_features_list = [300,5000,10000,20000,50000,100000,200000,262146]\n",
    "mae_list = []\n",
    "time_list = []\n",
    "\n",
    "for n_features in n_features_list:\n",
    "    print(\"Training model using \"+ str(n_features))\n",
    "    \n",
    "    # Testing feature selection based on training set\n",
    "    Xt = SelectKBest(f_regression,n_features).fit(X_train_trans_tree2,y_train).transform(X_train_trans_tree2)\n",
    "    Xv = SelectKBest(f_regression,n_features).fit(X_train_trans_tree2,y_train).transform(X_val_trans_tree2)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Fixed model structure \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                              n_estimators=200, reg_alpha= 0.1,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(Xt, np.log(y_train))\n",
    "    preds = lgbm1.predict(Xv)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    mae_list.append(mae)\n",
    "    time_list.append((end-start).seconds/60)\n",
    "\n",
    "    print(\"Completed model fit and predictions using \"+ str(n_features) + \" in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "    print(\"Median Absolute Error: \", str(mae))\n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like model starts to overfit beyond 100000 training features. We will define the new pipeline, transform the data sets before further hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM Feature extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree3 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression, 100000))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.7666666666666667 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree3 = pipeline510k_tree3.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree3 = pipeline510k_tree3.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 100000)\n",
      "(15899, 100000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree3.shape)\n",
    "print(X_val_trans_tree3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 192 candidates, totalling 1152 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed: 40.0min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 45.6min\n",
      "[Parallel(n_jobs=6)]: Done 101 tasks      | elapsed: 53.6min\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed: 67.4min\n",
      "[Parallel(n_jobs=6)]: Done 133 tasks      | elapsed: 77.6min\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed: 88.1min\n",
      "[Parallel(n_jobs=6)]: Done 169 tasks      | elapsed: 109.2min\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed: 119.2min\n",
      "[Parallel(n_jobs=6)]: Done 209 tasks      | elapsed: 134.5min\n",
      "[Parallel(n_jobs=6)]: Done 230 tasks      | elapsed: 147.4min\n",
      "[Parallel(n_jobs=6)]: Done 253 tasks      | elapsed: 166.2min\n",
      "[Parallel(n_jobs=6)]: Done 276 tasks      | elapsed: 183.2min\n",
      "[Parallel(n_jobs=6)]: Done 301 tasks      | elapsed: 207.8min\n",
      "[Parallel(n_jobs=6)]: Done 326 tasks      | elapsed: 230.0min\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed: 261.4min\n",
      "[Parallel(n_jobs=6)]: Done 380 tasks      | elapsed: 287.1min\n",
      "[Parallel(n_jobs=6)]: Done 409 tasks      | elapsed: 316.0min\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed: 337.7min\n",
      "[Parallel(n_jobs=6)]: Done 469 tasks      | elapsed: 372.9min\n",
      "[Parallel(n_jobs=6)]: Done 500 tasks      | elapsed: 416.7min\n",
      "[Parallel(n_jobs=6)]: Done 533 tasks      | elapsed: 453.6min\n",
      "[Parallel(n_jobs=6)]: Done 566 tasks      | elapsed: 506.1min\n",
      "[Parallel(n_jobs=6)]: Done 601 tasks      | elapsed: 547.0min\n",
      "[Parallel(n_jobs=6)]: Done 636 tasks      | elapsed: 583.7min\n",
      "[Parallel(n_jobs=6)]: Done 673 tasks      | elapsed: 636.1min\n",
      "[Parallel(n_jobs=6)]: Done 710 tasks      | elapsed: 696.0min\n",
      "[Parallel(n_jobs=6)]: Done 749 tasks      | elapsed: 774.8min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed: 840.9min\n",
      "[Parallel(n_jobs=6)]: Done 829 tasks      | elapsed: 926.1min\n",
      "[Parallel(n_jobs=6)]: Done 870 tasks      | elapsed: 1009.0min\n",
      "[Parallel(n_jobs=6)]: Done 913 tasks      | elapsed: 1143.1min\n",
      "[Parallel(n_jobs=6)]: Done 956 tasks      | elapsed: 1269.8min\n",
      "[Parallel(n_jobs=6)]: Done 1001 tasks      | elapsed: 1381.4min\n",
      "[Parallel(n_jobs=6)]: Done 1046 tasks      | elapsed: 1511.5min\n",
      "[Parallel(n_jobs=6)]: Done 1093 tasks      | elapsed: 1682.7min\n",
      "[Parallel(n_jobs=6)]: Done 1140 tasks      | elapsed: 1884.7min\n",
      "[Parallel(n_jobs=6)]: Done 1152 out of 1152 | elapsed: 1904.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed GridSearch in: 467.56666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,150,200,250,400,500],\n",
    "    'num_leaves':[150,200,300,400],\n",
    "    'reg_alpha': [0,0.01,0.05,0.1,0.5,1,5,10],\n",
    "    'objective': ['regression'],\n",
    "    'boosting_type': ['dart']\n",
    "}\n",
    "\n",
    "# We have 8 CPU cores, we will use 6 for this task\n",
    "lgbmSearch1 = GridSearchCV(estimator= lgb.LGBMRegressor(), \n",
    "                            param_grid= param_grid,\n",
    "                            n_jobs = 6,\n",
    "                            cv = 6,\n",
    "                            verbose = 10, scoring= 'neg_median_absolute_error'\n",
    "                            )\n",
    "\n",
    "lgbmSearch1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed GridSearch in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=500,\n",
       "       n_jobs=-1, num_leaves=150, objective='regression',\n",
       "       random_state=None, reg_alpha=0.1, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbmSearch1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.38894595686144295"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbmSearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.504873585145397"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "# Let's see the validation MAE (in original target scale) from the best estimator\n",
    "preds = lgbmSearch1.best_estimator_.predict(X_val_trans_tree3)\n",
    "median_absolute_error(y_val,np.exp(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the 4 parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(num_leaves,n_estimators,reg_alpha):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(num_leaves),\n",
    "                              n_estimators= int(n_estimators), reg_alpha= reg_alpha,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 01m16s | \u001b[35m -35.12445\u001b[0m | \u001b[32m      161.3516\u001b[0m | \u001b[32m    206.2586\u001b[0m | \u001b[32m     0.0548\u001b[0m | \n",
      "    2 | 01m58s | \u001b[35m -31.10728\u001b[0m | \u001b[32m      266.5394\u001b[0m | \u001b[32m    246.0457\u001b[0m | \u001b[32m     0.8177\u001b[0m | \n",
      "    3 | 02m06s | \u001b[35m -30.57605\u001b[0m | \u001b[32m      274.9119\u001b[0m | \u001b[32m    292.5025\u001b[0m | \u001b[32m     0.8854\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    4 | 01m47s | \u001b[35m -30.11682\u001b[0m | \u001b[32m      300.0000\u001b[0m | \u001b[32m    150.0000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "    5 | 03m14s | \u001b[35m -29.74980\u001b[0m | \u001b[32m      300.0000\u001b[0m | \u001b[32m    300.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    6 | 02m16s |  -30.23047 |       300.0000 |     300.0000 |      1.0000 | \n",
      "    7 | 01m38s |  -30.90307 |       150.0000 |     300.0000 |      0.0000 | \n",
      "    8 | 01m05s |  -31.16249 |       150.0000 |     150.0000 |      0.0000 | \n",
      "    9 | 02m34s | \u001b[35m -29.10945\u001b[0m | \u001b[32m      231.1680\u001b[0m | \u001b[32m    300.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "   10 | 02m44s |  -29.77941 |       300.0000 |     227.7304 |      0.0000 | \n",
      "   11 | 02m40s |  -29.13963 |       245.7091 |     300.0000 |      0.0000 | \n",
      "   12 | 02m35s |  -29.20123 |       237.4534 |     300.0000 |      0.0000 | \n",
      "   13 | 01m36s |  -29.24059 |       231.9763 |     150.0000 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (150, 300),\n",
    "    'n_estimators' : (150,300),\n",
    "    'reg_alpha': (0,1)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=3,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian optimization is promising to find parameter that may improve the performance. Let's try another round of optimization cycle by focusing on a narrower hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m51s | \u001b[35m -29.02788\u001b[0m | \u001b[32m      244.9824\u001b[0m | \u001b[32m    331.2563\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    2 | 02m36s |  -29.04927 |       221.6442 |     360.4335 |      0.0098 | \n",
      "    3 | 03m05s |  -29.39139 |       244.5303 |     394.5018 |      0.0034 | \n",
      "    4 | 02m32s |  -29.36064 |       246.5605 |     298.3245 |      0.0009 | \n",
      "    5 | 02m53s |  -29.93163 |       241.6704 |     375.4623 |      0.0039 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    6 | 02m18s |  -29.10648 |       220.0000 |     290.0000 |      0.0000 | \n",
      "    7 | 02m52s |  -29.40106 |       220.0000 |     400.0000 |      0.0000 | \n",
      "    8 | 02m28s | \u001b[35m -29.02445\u001b[0m | \u001b[32m      220.0000\u001b[0m | \u001b[32m    334.1820\u001b[0m | \u001b[32m     0.0100\u001b[0m | \n",
      "    9 | 03m13s |  -29.80390 |       250.0000 |     400.0000 |      0.0100 | \n",
      "   10 | 02m44s |  -29.44771 |       250.0000 |     290.0000 |      0.0000 | \n",
      "   11 | 02m55s |  -29.49083 |       250.0000 |     318.5344 |      0.0000 | \n",
      "   12 | 02m42s |  -29.07049 |       220.0000 |     348.3952 |      0.0000 | \n",
      "   13 | 02m26s | \u001b[35m -28.82805\u001b[0m | \u001b[32m      220.0000\u001b[0m | \u001b[32m    313.9619\u001b[0m | \u001b[32m     0.0100\u001b[0m | \n",
      "   14 | 02m48s |  -29.04563 |       220.0000 |     375.1665 |      0.0000 | \n",
      "   15 | 02m30s |  -29.25785 |       220.0000 |     319.4859 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (290, 400),\n",
    "    'n_estimators' : (220,250),\n",
    "    'reg_alpha': (0,0.01)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to find parameters that further improved performance. Let's try to further narrow down the hyperparameter space and perform another round of Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m29s | \u001b[35m -29.10137\u001b[0m | \u001b[32m      224.9824\u001b[0m | \u001b[32m    312.5034\u001b[0m | \u001b[32m     0.0001\u001b[0m | \n",
      "    2 | 02m17s |  -29.23025 |       201.6442 |     328.4183 |      0.0245 | \n",
      "    3 | 02m34s |  -29.46624 |       224.5303 |     347.0010 |      0.0086 | \n",
      "    4 | 02m21s | \u001b[35m -28.79092\u001b[0m | \u001b[32m      226.5605\u001b[0m | \u001b[32m    294.5406\u001b[0m | \u001b[32m     0.0024\u001b[0m | \n",
      "    5 | 02m30s |  -29.39501 |       221.6704 |     336.6158 |      0.0099 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    6 | 02m05s |  -29.11512 |       200.0636 |     290.0450 |      0.0122 | \n",
      "    7 | 02m30s |  -28.87265 |       230.0000 |     290.0000 |      0.0000 | \n",
      "    8 | 02m22s |  -29.21013 |       200.0000 |     350.0000 |      0.0250 | \n",
      "    9 | 02m20s |  -29.08370 |       220.0908 |     292.8533 |      0.0250 | \n",
      "   10 | 02m28s |  -29.09202 |       230.0000 |     300.9593 |      0.0250 | \n",
      "   11 | 02m50s |  -29.08896 |       230.0000 |     350.0000 |      0.0000 | \n",
      "   12 | 02m14s |  -29.23849 |       200.0000 |     311.5937 |      0.0000 | \n",
      "   13 | 02m28s |  -29.01811 |       225.4564 |     296.3131 |      0.0010 | \n",
      "   14 | 02m35s |  -29.08392 |       230.0000 |     324.0520 |      0.0250 | \n",
      "   15 | 02m19s |  -29.30205 |       224.3218 |     290.0000 |      0.0250 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (290, 350),\n",
    "    'n_estimators' : (200,230),\n",
    "    'reg_alpha': (0,0.025)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization stage give us another lead for optimization. We will perform another round of optimization cycle using these 3 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m20s | \u001b[35m -28.92014\u001b[0m | \u001b[32m      224.9824\u001b[0m | \u001b[32m    279.3764\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    2 | 02m01s |  -29.27668 |       201.6442 |     286.0076 |      0.0049 | \n",
      "    3 | 02m20s |  -29.15800 |       224.5303 |     293.7504 |      0.0017 | \n",
      "    4 | 02m15s |  -29.11658 |       226.5605 |     271.8919 |      0.0005 | \n",
      "    5 | 02m16s |  -29.14482 |       221.6704 |     289.4232 |      0.0020 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    6 | 02m03s |  -28.96436 |       200.0000 |     270.0000 |      0.0000 | \n",
      "    7 | 02m21s |  -29.26499 |       230.0000 |     283.7620 |      0.0050 | \n",
      "    8 | 02m11s |  -28.95894 |       200.0000 |     295.0000 |      0.0000 | \n",
      "    9 | 02m18s |  -29.04792 |       217.1388 |     276.4416 |      0.0000 | \n",
      "   10 | 02m15s |  -29.25602 |       221.5866 |     280.0949 |      0.0050 | \n",
      "   11 | 02m18s | \u001b[35m -28.83104\u001b[0m | \u001b[32m      214.6859\u001b[0m | \u001b[32m    295.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "   12 | 02m12s |  -29.01606 |       213.2324 |     270.0000 |      0.0000 | \n",
      "   13 | 02m25s |  -28.97788 |       230.0000 |     276.3328 |      0.0000 | \n",
      "   14 | 02m24s |  -29.31890 |       218.5193 |     295.0000 |      0.0000 | \n",
      "   15 | 02m29s |  -29.03612 |       230.0000 |     295.0000 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (270, 295),\n",
    "    'n_estimators' : (200,230),\n",
    "    'reg_alpha': (0,0.005)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we are converging to some optimal values with these parameters. Now let's try to assign the optimal values for these 3 parameters and see if we can tune the learning rate to further improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(learning_rate):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(294.54),\n",
    "                              learning_rate= learning_rate,\n",
    "                              n_estimators= int(226.56), reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   learning_rate | \n",
      "    1 | 02m20s | \u001b[35m -29.10472\u001b[0m | \u001b[32m         0.0938\u001b[0m | \n",
      "    2 | 02m24s |  -29.20536 |          0.0964 | \n",
      "    3 | 02m27s | \u001b[35m -28.94624\u001b[0m | \u001b[32m         0.0995\u001b[0m | \n",
      "    4 | 02m23s |  -29.10463 |          0.0908 | \n",
      "    5 | 02m24s |  -29.15096 |          0.0978 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   learning_rate | \n",
      "    6 | 02m23s |  -29.25657 |          0.1000 | \n",
      "    7 | 02m27s | \u001b[35m -28.81794\u001b[0m | \u001b[32m         0.0900\u001b[0m | \n",
      "    8 | 02m24s |  -28.95310 |          0.0989 | \n",
      "    9 | 02m24s |  -28.84389 |          0.0983 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "   10 | 02m21s |  -29.08047 |          0.0922 | \n",
      "   11 | 02m21s |  -29.17043 |          0.0951 | \n",
      "   12 | 02m21s |  -29.41761 |          0.0905 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "   13 | 02m22s |  -29.06929 |          0.0914 | \n",
      "   14 | 02m22s |  -29.40204 |          0.0930 | \n",
      "   15 | 02m22s |  -29.21425 |          0.0944 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': (0.09, 0.15)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try a slightly wider learning rate space along with other parameters to see if we can get some lead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(learning_rate, \n",
    "               reg_lambda,min_split_gain,max_depth,\n",
    "               colsample_bytree,min_child_samples,min_child_weight,subsample):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(294.54),\n",
    "                              learning_rate= learning_rate,reg_lambda= reg_lambda,\n",
    "                              min_split_gain = min_split_gain, max_depth = int(max_depth),\n",
    "                              colsample_bytree = colsample_bytree,\n",
    "                              min_child_samples = int(min_child_samples),\n",
    "                              min_child_weight = min_child_weight,subsample = subsample,\n",
    "                              n_estimators= int(226.56), reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n",
      "    1 | 02m12s | \u001b[35m -29.13360\u001b[0m | \u001b[32m            0.9356\u001b[0m | \u001b[32m         0.1250\u001b[0m | \u001b[32m    42.3435\u001b[0m | \u001b[32m            17.3298\u001b[0m | \u001b[32m            0.0033\u001b[0m | \u001b[32m          0.0036\u001b[0m | \u001b[32m      0.0135\u001b[0m | \u001b[32m     0.6253\u001b[0m | \n",
      "    2 | 02m11s |  -29.74794 |             0.8311 |          0.1568 |     38.8017 |             16.3328 |             0.0027 |           0.0023 |       0.0033 |      0.8233 | \n",
      "    3 | 01m34s |  -30.94449 |             0.6996 |          0.1940 |     32.6418 |             23.2452 |             0.0022 |           0.0025 |       0.0096 |      0.6406 | \n",
      "    4 | 01m55s |  -29.35169 |             0.8275 |          0.0891 |     36.3681 |             25.9945 |             0.0042 |           0.0015 |       0.0136 |      0.9233 | \n",
      "    5 | 01m34s |  -30.85810 |             0.6651 |          0.1732 |     41.4745 |             23.1107 |             0.0045 |           0.0004 |       0.0043 |      0.5450 | \n",
      "    6 | 02m05s |  -30.29918 |             0.7499 |          0.1799 |     41.9683 |             17.4670 |             0.0013 |           0.0026 |       0.0079 |      0.8363 | \n",
      "    7 | 00m53s |  -29.14366 |             0.9363 |          0.0866 |     12.6346 |             24.5434 |             0.0013 |           0.0027 |       0.0041 |      0.7096 | \n",
      "    8 | 02m10s |  -30.43446 |             0.7713 |          0.1781 |     43.2717 |             16.8779 |             0.0008 |           0.0032 |       0.0040 |      0.8502 | \n",
      "    9 | 01m08s |  -30.45334 |             0.7669 |          0.1862 |     16.9824 |             19.6596 |             0.0027 |           0.0015 |       0.0069 |      0.6684 | \n",
      "   10 | 00m37s |  -30.28254 |             0.6448 |          0.1667 |     11.0216 |             26.4109 |             0.0008 |           0.0040 |       0.0137 |      0.6155 | \n",
      "   11 | 01m40s |  -29.25655 |             0.8335 |          0.0803 |     33.8375 |             23.3358 |             0.0010 |           0.0018 |       0.0176 |      0.6135 | \n",
      "   12 | 01m47s |  -31.26134 |             0.8395 |          0.1977 |     31.0373 |             15.7278 |             0.0030 |           0.0036 |       0.0090 |      0.5512 | \n",
      "   13 | 01m37s | \u001b[35m -28.89579\u001b[0m | \u001b[32m            0.8018\u001b[0m | \u001b[32m         0.1212\u001b[0m | \u001b[32m    17.6388\u001b[0m | \u001b[32m            16.5435\u001b[0m | \u001b[32m            0.0040\u001b[0m | \u001b[32m          0.0029\u001b[0m | \u001b[32m      0.0164\u001b[0m | \u001b[32m     0.8367\u001b[0m | \n",
      "   14 | 01m29s | \u001b[35m -28.85138\u001b[0m | \u001b[32m            0.9264\u001b[0m | \u001b[32m         0.0914\u001b[0m | \u001b[32m    21.0359\u001b[0m | \u001b[32m            29.8613\u001b[0m | \u001b[32m            0.0032\u001b[0m | \u001b[32m          0.0026\u001b[0m | \u001b[32m      0.0137\u001b[0m | \u001b[32m     0.8001\u001b[0m | \n",
      "   15 | 01m48s |  -29.53078 |             0.9838 |          0.1274 |     42.8848 |             19.8564 |             0.0024 |           0.0015 |       0.0161 |      0.5109 | \n",
      "   16 | 01m13s |  -29.22252 |             0.9526 |          0.0806 |     14.4557 |             24.8236 |             0.0048 |           0.0008 |       0.0052 |      0.8486 | \n",
      "   17 | 02m00s |  -30.04175 |             0.8775 |          0.1684 |     48.8247 |             24.1891 |             0.0039 |           0.0008 |       0.0115 |      0.8728 | \n",
      "   18 | 01m31s |  -31.14708 |             0.9826 |          0.1947 |     46.8444 |             29.4122 |             0.0012 |           0.0016 |       0.0031 |      0.5208 | \n",
      "   19 | 00m52s |  -30.29079 |             0.7960 |          0.1785 |     15.2055 |             27.4165 |             0.0049 |           0.0036 |       0.0190 |      0.6393 | \n",
      "   20 | 01m39s |  -28.85449 |             0.6116 |          0.1214 |     21.0428 |             19.8555 |             0.0019 |           0.0026 |       0.0037 |      0.8752 | \n",
      "   21 | 01m40s |  -29.35848 |             0.8304 |          0.1254 |     37.0940 |             24.6912 |             0.0014 |           0.0000 |       0.0010 |      0.6006 | \n",
      "   22 | 01m41s |  -30.10289 |             0.9054 |          0.1742 |     24.1090 |             19.5265 |             0.0034 |           0.0005 |       0.0052 |      0.7396 | \n",
      "   23 | 01m49s |  -28.99129 |             0.9321 |          0.0903 |     26.7401 |             16.3092 |             0.0024 |           0.0030 |       0.0033 |      0.5628 | \n",
      "   24 | 01m40s |  -29.46385 |             0.9021 |          0.1455 |     20.4532 |             24.5845 |             0.0030 |           0.0007 |       0.0163 |      0.9240 | \n",
      "   25 | 01m23s | \u001b[35m -28.74084\u001b[0m | \u001b[32m            0.9586\u001b[0m | \u001b[32m         0.0995\u001b[0m | \u001b[32m    13.6127\u001b[0m | \u001b[32m            19.1086\u001b[0m | \u001b[32m            0.0041\u001b[0m | \u001b[32m          0.0024\u001b[0m | \u001b[32m      0.0028\u001b[0m | \u001b[32m     0.9659\u001b[0m | \n",
      "   26 | 01m47s |  -28.95104 |             0.6553 |          0.1148 |     30.6065 |             27.9700 |             0.0045 |           0.0016 |       0.0144 |      0.9325 | \n",
      "   27 | 02m11s |  -28.97898 |             0.8849 |          0.0854 |     26.1042 |             16.8731 |             0.0018 |           0.0048 |       0.0085 |      0.9498 | \n",
      "   28 | 02m00s |  -29.31586 |             0.8298 |          0.1199 |     43.4320 |             22.7592 |             0.0045 |           0.0021 |       0.0037 |      0.7979 | \n",
      "   29 | 01m22s |  -29.72844 |             0.9348 |          0.1684 |     18.2780 |             25.2428 |             0.0011 |           0.0017 |       0.0029 |      0.8422 | \n",
      "   30 | 02m01s |  -30.60461 |             0.8078 |          0.1807 |     38.2619 |             16.6560 |             0.0019 |           0.0035 |       0.0103 |      0.6706 | \n",
      "   31 | 01m42s |  -29.87523 |             0.8604 |          0.1651 |     21.3120 |             17.2848 |             0.0007 |           0.0002 |       0.0185 |      0.6912 | \n",
      "   32 | 02m10s |  -29.05420 |             0.7682 |          0.1026 |     30.9650 |             16.8535 |             0.0034 |           0.0037 |       0.0000 |      0.9944 | \n",
      "   33 | 02m08s |  -30.69601 |             0.8667 |          0.1983 |     46.1167 |             19.9258 |             0.0007 |           0.0029 |       0.0028 |      0.8469 | \n",
      "   34 | 02m14s |  -29.33093 |             0.9740 |          0.1266 |     42.4831 |             18.9715 |             0.0027 |           0.0005 |       0.0072 |      0.8317 | \n",
      "   35 | 01m25s |  -29.52940 |             0.8525 |          0.1404 |     22.3831 |             26.5469 |             0.0041 |           0.0001 |       0.0192 |      0.6950 | \n",
      "   36 | 01m07s |  -29.81287 |             0.9131 |          0.1716 |     12.1974 |             17.0081 |             0.0042 |           0.0029 |       0.0007 |      0.9096 | \n",
      "   37 | 02m04s |  -30.09012 |             0.9255 |          0.1620 |     30.6315 |             17.7488 |             0.0010 |           0.0001 |       0.0124 |      0.8710 | \n",
      "   38 | 01m51s |  -29.68947 |             0.8608 |          0.1666 |     25.3376 |             17.9720 |             0.0027 |           0.0013 |       0.0067 |      0.8176 | \n",
      "   39 | 02m13s |  -28.74800 |             0.7076 |          0.1057 |     44.4434 |             15.8678 |             0.0030 |           0.0025 |       0.0119 |      0.8692 | \n",
      "   40 | 00m45s |  -29.49149 |             0.8257 |          0.1716 |     10.9435 |             17.2822 |             0.0049 |           0.0018 |       0.0200 |      0.7400 | \n",
      "   41 | 01m42s |  -28.99370 |             0.8915 |          0.0890 |     43.2424 |             27.8354 |             0.0027 |           0.0026 |       0.0085 |      0.6263 | \n",
      "   42 | 01m34s |  -29.34975 |             0.6031 |          0.1533 |     23.4245 |             22.1061 |             0.0034 |           0.0012 |       0.0191 |      0.8629 | \n",
      "   43 | 01m31s |  -29.86491 |             0.8365 |          0.1311 |     37.1300 |             26.2841 |             0.0029 |           0.0015 |       0.0031 |      0.5255 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   44 | 01m39s |  -28.98230 |             0.7921 |          0.1081 |     25.0538 |             19.4725 |             0.0047 |           0.0035 |       0.0043 |      0.6442 | \n",
      "   45 | 00m44s |  -29.36821 |             0.8430 |          0.1459 |     12.7252 |             23.4023 |             0.0015 |           0.0036 |       0.0009 |      0.5585 | \n",
      "   46 | 02m06s |  -29.86613 |             0.8070 |          0.1492 |     39.5888 |             19.2136 |             0.0018 |           0.0033 |       0.0012 |      0.9285 | \n",
      "   47 | 00m45s |  -29.33804 |             0.9268 |          0.1458 |     12.6378 |             25.5054 |             0.0028 |           0.0005 |       0.0103 |      0.6133 | \n",
      "   48 | 02m09s | \u001b[35m -28.70211\u001b[0m | \u001b[32m            0.6909\u001b[0m | \u001b[32m         0.0986\u001b[0m | \u001b[32m    49.8906\u001b[0m | \u001b[32m            18.3368\u001b[0m | \u001b[32m            0.0014\u001b[0m | \u001b[32m          0.0048\u001b[0m | \u001b[32m      0.0023\u001b[0m | \u001b[32m     0.8961\u001b[0m | \n",
      "   49 | 01m57s |  -29.72739 |             0.7199 |          0.1559 |     42.4657 |             20.4128 |             0.0007 |           0.0050 |       0.0174 |      0.7257 | \n",
      "   50 | 01m40s | \u001b[35m -28.69685\u001b[0m | \u001b[32m            0.8139\u001b[0m | \u001b[32m         0.0931\u001b[0m | \u001b[32m    26.3029\u001b[0m | \u001b[32m            27.1407\u001b[0m | \u001b[32m            0.0044\u001b[0m | \u001b[32m          0.0005\u001b[0m | \u001b[32m      0.0171\u001b[0m | \u001b[32m     0.7785\u001b[0m | \n",
      "   51 | 01m48s |  -28.96002 |             0.8711 |          0.0938 |     35.4703 |             21.6266 |             0.0016 |           0.0041 |       0.0089 |      0.5968 | \n",
      "   52 | 01m53s |  -29.49812 |             0.7399 |          0.1395 |     31.8903 |             19.5232 |             0.0021 |           0.0018 |       0.0192 |      0.7930 | \n",
      "   53 | 01m39s |  -29.31423 |             0.8317 |          0.1253 |     19.7922 |             17.8023 |             0.0006 |           0.0017 |       0.0189 |      0.7918 | \n",
      "   54 | 00m43s |  -30.76911 |             0.9940 |          0.1975 |     12.3974 |             20.4807 |             0.0028 |           0.0011 |       0.0003 |      0.5139 | \n",
      "   55 | 01m19s |  -29.89489 |             0.7825 |          0.1656 |     25.0841 |             24.5737 |             0.0039 |           0.0000 |       0.0165 |      0.5580 | \n",
      "   56 | 01m45s |  -28.94468 |             0.8351 |          0.0874 |     33.7979 |             26.3114 |             0.0044 |           0.0044 |       0.0122 |      0.7637 | \n",
      "   57 | 01m29s |  -30.77621 |             0.7866 |          0.1880 |     31.4298 |             20.6998 |             0.0013 |           0.0029 |       0.0156 |      0.5046 | \n",
      "   58 | 01m46s |  -31.04145 |             0.8260 |          0.1937 |     34.1555 |             23.2086 |             0.0044 |           0.0044 |       0.0089 |      0.6712 | \n",
      "   59 | 02m03s |  -29.75333 |             0.6716 |          0.1795 |     24.9349 |             15.3856 |             0.0010 |           0.0042 |       0.0142 |      0.9979 | \n",
      "   60 | 01m54s |  -29.13378 |             0.9588 |          0.0985 |     43.8247 |             24.3870 |             0.0036 |           0.0021 |       0.0022 |      0.7553 | \n",
      "   61 | 01m02s |  -29.56269 |             0.8760 |          0.1584 |     12.5042 |             19.6345 |             0.0020 |           0.0032 |       0.0098 |      0.9075 | \n",
      "   62 | 01m23s |  -30.67019 |             0.7988 |          0.1768 |     20.5022 |             17.7142 |             0.0017 |           0.0022 |       0.0055 |      0.5971 | \n",
      "   63 | 01m12s |  -28.71205 |             0.8635 |          0.0921 |     16.0663 |             22.2031 |             0.0036 |           0.0049 |       0.0063 |      0.6500 | \n",
      "   64 | 01m40s |  -28.97012 |             0.9489 |          0.0921 |     31.1574 |             28.1766 |             0.0025 |           0.0035 |       0.0090 |      0.7075 | \n",
      "   65 | 02m00s |  -28.89050 |             0.9041 |          0.1079 |     38.8654 |             20.5769 |             0.0037 |           0.0024 |       0.0037 |      0.8080 | \n",
      "   66 | 01m45s |  -28.96675 |             0.7512 |          0.1390 |     28.8324 |             23.8981 |             0.0007 |           0.0008 |       0.0118 |      0.8737 | \n",
      "   67 | 01m32s |  -29.12904 |             0.9129 |          0.1235 |     22.7591 |             20.8021 |             0.0034 |           0.0012 |       0.0093 |      0.6491 | \n",
      "   68 | 01m16s |  -29.14223 |             0.8946 |          0.0839 |     24.1888 |             29.5168 |             0.0008 |           0.0017 |       0.0081 |      0.5255 | \n",
      "   69 | 02m00s | \u001b[35m -28.69469\u001b[0m | \u001b[32m            0.8241\u001b[0m | \u001b[32m         0.1041\u001b[0m | \u001b[32m    27.3447\u001b[0m | \u001b[32m            16.1454\u001b[0m | \u001b[32m            0.0029\u001b[0m | \u001b[32m          0.0027\u001b[0m | \u001b[32m      0.0155\u001b[0m | \u001b[32m     0.8143\u001b[0m | \n",
      "   70 | 01m35s |  -28.84861 |             0.7571 |          0.1030 |     27.3617 |             22.2885 |             0.0006 |           0.0041 |       0.0115 |      0.6296 | \n",
      "   71 | 01m31s |  -29.51150 |             0.6867 |          0.1276 |     46.3349 |             28.2661 |             0.0033 |           0.0042 |       0.0118 |      0.5485 | \n",
      "   72 | 01m02s |  -28.70380 |             0.8525 |          0.1055 |     14.8109 |             28.8123 |             0.0037 |           0.0014 |       0.0131 |      0.8116 | \n",
      "   73 | 02m01s |  -28.75789 |             0.6644 |          0.1043 |     45.3763 |             17.2625 |             0.0048 |           0.0002 |       0.0086 |      0.7500 | \n",
      "   74 | 02m02s |  -30.36036 |             0.6098 |          0.1904 |     36.3103 |             18.6054 |             0.0042 |           0.0007 |       0.0012 |      0.9859 | \n",
      "   75 | 02m07s |  -29.49087 |             0.6743 |          0.1383 |     46.9285 |             15.4779 |             0.0015 |           0.0036 |       0.0149 |      0.7496 | \n",
      "   76 | 02m15s |  -28.98823 |             0.7992 |          0.1065 |     43.3133 |             16.1110 |             0.0047 |           0.0040 |       0.0181 |      0.9896 | \n",
      "   77 | 01m27s |  -29.23111 |             0.9073 |          0.0933 |     32.1608 |             29.7719 |             0.0024 |           0.0015 |       0.0116 |      0.5416 | \n",
      "   78 | 01m45s |  -29.36448 |             0.9928 |          0.0824 |     21.8325 |             18.4457 |             0.0032 |           0.0041 |       0.0051 |      0.7185 | \n",
      "   79 | 02m16s |  -29.07460 |             0.8273 |          0.1074 |     46.2009 |             17.3272 |             0.0006 |           0.0003 |       0.0081 |      0.9776 | \n",
      "   80 | 01m48s |  -29.31770 |             0.6983 |          0.0818 |     30.1128 |             21.0027 |             0.0040 |           0.0011 |       0.0111 |      0.7931 | \n",
      "   81 | 01m42s |  -29.13082 |             0.6226 |          0.0889 |     29.5645 |             15.5137 |             0.0021 |           0.0036 |       0.0040 |      0.5113 | \n",
      "   82 | 02m02s |  -29.35957 |             0.6473 |          0.1399 |     30.8430 |             15.0328 |             0.0035 |           0.0015 |       0.0140 |      0.8665 | \n",
      "   83 | 01m50s |  -28.80892 |             0.6593 |          0.1160 |     44.3274 |             29.3313 |             0.0019 |           0.0038 |       0.0121 |      0.9293 | \n",
      "   84 | 01m31s |  -29.01849 |             0.6314 |          0.0840 |     23.3705 |             29.8177 |             0.0031 |           0.0016 |       0.0148 |      0.8584 | \n",
      "   85 | 01m14s |  -29.06616 |             0.7866 |          0.0856 |     14.1086 |             21.1354 |             0.0036 |           0.0003 |       0.0063 |      0.8243 | \n",
      "   86 | 02m15s |  -29.43394 |             0.6215 |          0.1528 |     43.3543 |             15.4970 |             0.0016 |           0.0004 |       0.0183 |      0.9721 | \n",
      "   87 | 00m53s |  -29.74969 |             0.9155 |          0.1670 |     11.7254 |             19.3563 |             0.0044 |           0.0034 |       0.0128 |      0.8420 | \n",
      "   88 | 02m02s |  -29.70486 |             0.8495 |          0.1347 |     45.8477 |             15.0116 |             0.0016 |           0.0019 |       0.0192 |      0.5385 | \n",
      "   89 | 00m49s |  -29.07129 |             0.6616 |          0.0873 |     11.7713 |             25.4509 |             0.0006 |           0.0015 |       0.0016 |      0.8220 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   90 | 01m36s |  -28.92105 |             0.9262 |          0.1176 |     17.5505 |             18.9114 |             0.0022 |           0.0049 |       0.0191 |      0.8737 | \n",
      "   91 | 02m05s |  -29.30116 |             0.6298 |          0.0817 |     43.9227 |             17.3082 |             0.0040 |           0.0016 |       0.0179 |      0.9373 | \n",
      "   92 | 01m37s |  -29.88873 |             0.6372 |          0.1776 |     23.2396 |             17.7917 |             0.0029 |           0.0013 |       0.0133 |      0.7529 | \n",
      "   93 | 01m19s | \u001b[35m -28.53989\u001b[0m | \u001b[32m            0.6014\u001b[0m | \u001b[32m         0.0983\u001b[0m | \u001b[32m    24.3895\u001b[0m | \u001b[32m            25.1544\u001b[0m | \u001b[32m            0.0024\u001b[0m | \u001b[32m          0.0044\u001b[0m | \u001b[32m      0.0046\u001b[0m | \u001b[32m     0.5759\u001b[0m | \n",
      "   94 | 00m51s |  -29.06401 |             0.6442 |          0.0898 |     11.9899 |             15.2537 |             0.0043 |           0.0043 |       0.0084 |      0.5784 | \n",
      "   95 | 02m12s |  -28.94863 |             0.9679 |          0.1119 |     46.0504 |             20.9527 |             0.0019 |           0.0008 |       0.0166 |      0.9885 | \n",
      "   96 | 01m58s |  -28.96979 |             0.7926 |          0.1094 |     36.6021 |             17.9261 |             0.0032 |           0.0011 |       0.0183 |      0.7019 | \n",
      "   97 | 02m26s |  -30.57701 |             0.9627 |          0.1922 |     44.9535 |             15.6771 |             0.0048 |           0.0004 |       0.0160 |      0.9103 | \n",
      "   98 | 01m10s |  -28.91474 |             0.8801 |          0.0927 |     13.9022 |             20.6457 |             0.0032 |           0.0031 |       0.0200 |      0.8573 | \n",
      "   99 | 01m08s |  -29.27430 |             0.7671 |          0.1059 |     19.7298 |             22.6613 |             0.0029 |           0.0038 |       0.0187 |      0.5033 | \n",
      "  100 | 01m46s |  -28.72042 |             0.6271 |          0.1042 |     31.6652 |             21.0458 |             0.0026 |           0.0031 |       0.0010 |      0.7868 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n",
      "  101 | 00m59s |  -29.57831 |             1.0000 |          0.0800 |     10.0000 |             30.0000 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': (0.08, 0.2),\n",
    "    'reg_lambda':(0,0.02),\n",
    "    'min_split_gain': (0,0.005),\n",
    "    'max_depth':(10,50),\n",
    "    'colsample_bytree':(0.6,1),\n",
    "    'min_child_samples':(15,30),\n",
    "    'min_child_weight':(0.0005,0.005),\n",
    "    'subsample':(0.5,1)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=100,n_iter=50,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(learning_rate, \n",
    "               reg_lambda,min_split_gain,max_depth,\n",
    "               colsample_bytree,min_child_samples,min_child_weight,subsample):\n",
    "    from sklearn.metrics import median_absolute_error\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(294.54),\n",
    "                              learning_rate= learning_rate,reg_lambda= reg_lambda,\n",
    "                              min_split_gain = min_split_gain, max_depth = int(max_depth),\n",
    "                              colsample_bytree = colsample_bytree,\n",
    "                              min_child_samples = int(min_child_samples),\n",
    "                              min_child_weight = min_child_weight,subsample = subsample,\n",
    "                              n_estimators= int(226.56), reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n",
      "    1 | 01m58s | \u001b[35m -29.53943\u001b[0m | \u001b[32m            0.9356\u001b[0m | \u001b[32m         0.1250\u001b[0m | \u001b[32m    42.3435\u001b[0m | \u001b[32m            17.3298\u001b[0m | \u001b[32m            0.0033\u001b[0m | \u001b[32m          0.0036\u001b[0m | \u001b[32m      0.0135\u001b[0m | \u001b[32m     0.6253\u001b[0m | \n",
      "    2 | 02m04s |  -29.95296 |             0.8311 |          0.1568 |     38.8017 |             16.3328 |             0.0027 |           0.0023 |       0.0033 |      0.8233 | \n",
      "    3 | 01m30s |  -30.49631 |             0.6996 |          0.1940 |     32.6418 |             23.2452 |             0.0022 |           0.0025 |       0.0096 |      0.6406 | \n",
      "    4 | 01m52s | \u001b[35m -29.08578\u001b[0m | \u001b[32m            0.8275\u001b[0m | \u001b[32m         0.0891\u001b[0m | \u001b[32m    36.3681\u001b[0m | \u001b[32m            25.9945\u001b[0m | \u001b[32m            0.0042\u001b[0m | \u001b[32m          0.0015\u001b[0m | \u001b[32m      0.0136\u001b[0m | \u001b[32m     0.9233\u001b[0m | \n",
      "    5 | 01m30s |  -30.61318 |             0.6651 |          0.1732 |     41.4745 |             23.1107 |             0.0045 |           0.0004 |       0.0043 |      0.5450 | \n",
      "    6 | 02m01s |  -30.16985 |             0.7499 |          0.1799 |     41.9683 |             17.4670 |             0.0013 |           0.0026 |       0.0079 |      0.8363 | \n",
      "    7 | 00m50s | \u001b[35m -29.06226\u001b[0m | \u001b[32m            0.9363\u001b[0m | \u001b[32m         0.0866\u001b[0m | \u001b[32m    12.6346\u001b[0m | \u001b[32m            24.5434\u001b[0m | \u001b[32m            0.0013\u001b[0m | \u001b[32m          0.0027\u001b[0m | \u001b[32m      0.0041\u001b[0m | \u001b[32m     0.7096\u001b[0m | \n",
      "    8 | 02m03s |  -30.17945 |             0.7713 |          0.1781 |     43.2717 |             16.8779 |             0.0008 |           0.0032 |       0.0040 |      0.8502 | \n",
      "    9 | 01m04s |  -30.31489 |             0.7669 |          0.1862 |     16.9824 |             19.6596 |             0.0027 |           0.0015 |       0.0069 |      0.6684 | \n",
      "   10 | 00m34s |  -30.32819 |             0.6448 |          0.1667 |     11.0216 |             26.4109 |             0.0008 |           0.0040 |       0.0137 |      0.6155 | \n",
      "   11 | 01m35s |  -29.50982 |             0.8335 |          0.0803 |     33.8375 |             23.3358 |             0.0010 |           0.0018 |       0.0176 |      0.6135 | \n",
      "   12 | 01m40s |  -30.76246 |             0.8395 |          0.1977 |     31.0373 |             15.7278 |             0.0030 |           0.0036 |       0.0090 |      0.5512 | \n",
      "   13 | 01m31s | \u001b[35m -29.05356\u001b[0m | \u001b[32m            0.8018\u001b[0m | \u001b[32m         0.1212\u001b[0m | \u001b[32m    17.6388\u001b[0m | \u001b[32m            16.5435\u001b[0m | \u001b[32m            0.0040\u001b[0m | \u001b[32m          0.0029\u001b[0m | \u001b[32m      0.0164\u001b[0m | \u001b[32m     0.8367\u001b[0m | \n",
      "   14 | 01m23s | \u001b[35m -28.90610\u001b[0m | \u001b[32m            0.9264\u001b[0m | \u001b[32m         0.0914\u001b[0m | \u001b[32m    21.0359\u001b[0m | \u001b[32m            29.8613\u001b[0m | \u001b[32m            0.0032\u001b[0m | \u001b[32m          0.0026\u001b[0m | \u001b[32m      0.0137\u001b[0m | \u001b[32m     0.8001\u001b[0m | \n",
      "   15 | 01m42s |  -29.47039 |             0.9838 |          0.1274 |     42.8848 |             19.8564 |             0.0024 |           0.0015 |       0.0161 |      0.5109 | \n",
      "   16 | 01m11s |  -29.44510 |             0.9526 |          0.0806 |     14.4557 |             24.8236 |             0.0048 |           0.0008 |       0.0052 |      0.8486 | \n",
      "   17 | 01m57s |  -30.28353 |             0.8775 |          0.1684 |     48.8247 |             24.1891 |             0.0039 |           0.0008 |       0.0115 |      0.8728 | \n",
      "   18 | 01m27s |  -31.00421 |             0.9826 |          0.1947 |     46.8444 |             29.4122 |             0.0012 |           0.0016 |       0.0031 |      0.5208 | \n",
      "   19 | 00m50s |  -30.28468 |             0.7960 |          0.1785 |     15.2055 |             27.4165 |             0.0049 |           0.0036 |       0.0190 |      0.6393 | \n",
      "   20 | 01m34s |  -29.00873 |             0.6116 |          0.1214 |     21.0428 |             19.8555 |             0.0019 |           0.0026 |       0.0037 |      0.8752 | \n",
      "   21 | 01m34s |  -29.42075 |             0.8304 |          0.1254 |     37.0940 |             24.6912 |             0.0014 |           0.0000 |       0.0010 |      0.6006 | \n",
      "   22 | 01m35s |  -29.88892 |             0.9054 |          0.1742 |     24.1090 |             19.5265 |             0.0034 |           0.0005 |       0.0052 |      0.7396 | \n",
      "   23 | 01m43s |  -28.97749 |             0.9321 |          0.0903 |     26.7401 |             16.3092 |             0.0024 |           0.0030 |       0.0033 |      0.5628 | \n",
      "   24 | 01m29s |  -29.68363 |             0.9021 |          0.1455 |     20.4532 |             24.5845 |             0.0030 |           0.0007 |       0.0163 |      0.9240 | \n",
      "   25 | 01m14s | \u001b[35m -28.72768\u001b[0m | \u001b[32m            0.9586\u001b[0m | \u001b[32m         0.0995\u001b[0m | \u001b[32m    13.6127\u001b[0m | \u001b[32m            19.1086\u001b[0m | \u001b[32m            0.0041\u001b[0m | \u001b[32m          0.0024\u001b[0m | \u001b[32m      0.0028\u001b[0m | \u001b[32m     0.9659\u001b[0m | \n",
      "   26 | 01m37s |  -28.92453 |             0.6553 |          0.1148 |     30.6065 |             27.9700 |             0.0045 |           0.0016 |       0.0144 |      0.9325 | \n",
      "   27 | 02m00s |  -29.28080 |             0.8849 |          0.0854 |     26.1042 |             16.8731 |             0.0018 |           0.0048 |       0.0085 |      0.9498 | \n",
      "   28 | 01m50s |  -28.78927 |             0.8298 |          0.1199 |     43.4320 |             22.7592 |             0.0045 |           0.0021 |       0.0037 |      0.7979 | \n",
      "   29 | 01m16s |  -29.87184 |             0.9348 |          0.1684 |     18.2780 |             25.2428 |             0.0011 |           0.0017 |       0.0029 |      0.8422 | \n",
      "   30 | 01m53s |  -30.52932 |             0.8078 |          0.1807 |     38.2619 |             16.6560 |             0.0019 |           0.0035 |       0.0103 |      0.6706 | \n",
      "   31 | 01m31s |  -29.84966 |             0.8604 |          0.1651 |     21.3120 |             17.2848 |             0.0007 |           0.0002 |       0.0185 |      0.6912 | \n",
      "   32 | 02m04s |  -29.03862 |             0.7682 |          0.1026 |     30.9650 |             16.8535 |             0.0034 |           0.0037 |       0.0000 |      0.9944 | \n",
      "   33 | 02m01s |  -30.29330 |             0.8667 |          0.1983 |     46.1167 |             19.9258 |             0.0007 |           0.0029 |       0.0028 |      0.8469 | \n",
      "   34 | 02m04s |  -29.04134 |             0.9740 |          0.1266 |     42.4831 |             18.9715 |             0.0027 |           0.0005 |       0.0072 |      0.8317 | \n",
      "   35 | 01m18s |  -29.58355 |             0.8525 |          0.1404 |     22.3831 |             26.5469 |             0.0041 |           0.0001 |       0.0192 |      0.6950 | \n",
      "   36 | 01m01s |  -29.96418 |             0.9131 |          0.1716 |     12.1974 |             17.0081 |             0.0042 |           0.0029 |       0.0007 |      0.9096 | \n",
      "   37 | 01m57s |  -29.58697 |             0.9255 |          0.1620 |     30.6315 |             17.7488 |             0.0010 |           0.0001 |       0.0124 |      0.8710 | \n",
      "   38 | 01m46s |  -29.88960 |             0.8608 |          0.1666 |     25.3376 |             17.9720 |             0.0027 |           0.0013 |       0.0067 |      0.8176 | \n",
      "   39 | 02m05s |  -29.03564 |             0.7076 |          0.1057 |     44.4434 |             15.8678 |             0.0030 |           0.0025 |       0.0119 |      0.8692 | \n",
      "   40 | 00m41s |  -29.87310 |             0.8257 |          0.1716 |     10.9435 |             17.2822 |             0.0049 |           0.0018 |       0.0200 |      0.7400 | \n",
      "   41 | 01m35s |  -29.03307 |             0.8915 |          0.0890 |     43.2424 |             27.8354 |             0.0027 |           0.0026 |       0.0085 |      0.6263 | \n",
      "   42 | 01m28s |  -29.50757 |             0.6031 |          0.1533 |     23.4245 |             22.1061 |             0.0034 |           0.0012 |       0.0191 |      0.8629 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   43 | 01m26s |  -29.79672 |             0.8365 |          0.1311 |     37.1300 |             26.2841 |             0.0029 |           0.0015 |       0.0031 |      0.5255 | \n",
      "   44 | 01m34s |  -28.82613 |             0.7921 |          0.1081 |     25.0538 |             19.4725 |             0.0047 |           0.0035 |       0.0043 |      0.6442 | \n",
      "   45 | 00m40s |  -29.79024 |             0.8430 |          0.1459 |     12.7252 |             23.4023 |             0.0015 |           0.0036 |       0.0009 |      0.5585 | \n",
      "   46 | 02m00s |  -29.69154 |             0.8070 |          0.1492 |     39.5888 |             19.2136 |             0.0018 |           0.0033 |       0.0012 |      0.9285 | \n",
      "   47 | 00m41s |  -29.73565 |             0.9268 |          0.1458 |     12.6378 |             25.5054 |             0.0028 |           0.0005 |       0.0103 |      0.6133 | \n",
      "   48 | 01m59s |  -28.99055 |             0.6909 |          0.0986 |     49.8906 |             18.3368 |             0.0014 |           0.0048 |       0.0023 |      0.8961 | \n",
      "   49 | 01m46s |  -29.66706 |             0.7199 |          0.1559 |     42.4657 |             20.4128 |             0.0007 |           0.0050 |       0.0174 |      0.7257 | \n",
      "   50 | 01m34s |  -28.99033 |             0.8139 |          0.0931 |     26.3029 |             27.1407 |             0.0044 |           0.0005 |       0.0171 |      0.7785 | \n",
      "   51 | 01m41s |  -29.18053 |             0.8711 |          0.0938 |     35.4703 |             21.6266 |             0.0016 |           0.0041 |       0.0089 |      0.5968 | \n",
      "   52 | 01m47s |  -29.28508 |             0.7399 |          0.1395 |     31.8903 |             19.5232 |             0.0021 |           0.0018 |       0.0192 |      0.7930 | \n",
      "   53 | 01m35s |  -29.11469 |             0.8317 |          0.1253 |     19.7922 |             17.8023 |             0.0006 |           0.0017 |       0.0189 |      0.7918 | \n",
      "   54 | 00m41s |  -30.46299 |             0.9940 |          0.1975 |     12.3974 |             20.4807 |             0.0028 |           0.0011 |       0.0003 |      0.5139 | \n",
      "   55 | 01m16s |  -29.89398 |             0.7825 |          0.1656 |     25.0841 |             24.5737 |             0.0039 |           0.0000 |       0.0165 |      0.5580 | \n",
      "   56 | 01m40s |  -29.10235 |             0.8351 |          0.0874 |     33.7979 |             26.3114 |             0.0044 |           0.0044 |       0.0122 |      0.7637 | \n",
      "   57 | 01m25s |  -31.04701 |             0.7866 |          0.1880 |     31.4298 |             20.6998 |             0.0013 |           0.0029 |       0.0156 |      0.5046 | \n",
      "   58 | 01m35s |  -30.62488 |             0.8260 |          0.1937 |     34.1555 |             23.2086 |             0.0044 |           0.0044 |       0.0089 |      0.6712 | \n",
      "   59 | 01m53s |  -29.95044 |             0.6716 |          0.1795 |     24.9349 |             15.3856 |             0.0010 |           0.0042 |       0.0142 |      0.9979 | \n",
      "   60 | 01m48s |  -29.06044 |             0.9588 |          0.0985 |     43.8247 |             24.3870 |             0.0036 |           0.0021 |       0.0022 |      0.7553 | \n",
      "   61 | 00m58s |  -29.60206 |             0.8760 |          0.1584 |     12.5042 |             19.6345 |             0.0020 |           0.0032 |       0.0098 |      0.9075 | \n",
      "   62 | 01m17s |  -30.36202 |             0.7988 |          0.1768 |     20.5022 |             17.7142 |             0.0017 |           0.0022 |       0.0055 |      0.5971 | \n",
      "   63 | 01m08s |  -28.90678 |             0.8635 |          0.0921 |     16.0663 |             22.2031 |             0.0036 |           0.0049 |       0.0063 |      0.6500 | \n",
      "   64 | 01m34s |  -29.10054 |             0.9489 |          0.0921 |     31.1574 |             28.1766 |             0.0025 |           0.0035 |       0.0090 |      0.7075 | \n",
      "   65 | 01m54s |  -28.79095 |             0.9041 |          0.1079 |     38.8654 |             20.5769 |             0.0037 |           0.0024 |       0.0037 |      0.8080 | \n",
      "   66 | 01m40s |  -29.17967 |             0.7512 |          0.1390 |     28.8324 |             23.8981 |             0.0007 |           0.0008 |       0.0118 |      0.8737 | \n",
      "   67 | 01m28s |  -28.99009 |             0.9129 |          0.1235 |     22.7591 |             20.8021 |             0.0034 |           0.0012 |       0.0093 |      0.6491 | \n",
      "   68 | 01m11s |  -29.22558 |             0.8946 |          0.0839 |     24.1888 |             29.5168 |             0.0008 |           0.0017 |       0.0081 |      0.5255 | \n",
      "   69 | 01m53s |  -28.91109 |             0.8241 |          0.1041 |     27.3447 |             16.1454 |             0.0029 |           0.0027 |       0.0155 |      0.8143 | \n",
      "   70 | 01m30s |  -29.20574 |             0.7571 |          0.1030 |     27.3617 |             22.2885 |             0.0006 |           0.0041 |       0.0115 |      0.6296 | \n",
      "   71 | 01m25s |  -29.62106 |             0.6867 |          0.1276 |     46.3349 |             28.2661 |             0.0033 |           0.0042 |       0.0118 |      0.5485 | \n",
      "   72 | 00m57s | \u001b[35m -28.68992\u001b[0m | \u001b[32m            0.8525\u001b[0m | \u001b[32m         0.1055\u001b[0m | \u001b[32m    14.8109\u001b[0m | \u001b[32m            28.8123\u001b[0m | \u001b[32m            0.0037\u001b[0m | \u001b[32m          0.0014\u001b[0m | \u001b[32m      0.0131\u001b[0m | \u001b[32m     0.8116\u001b[0m | \n",
      "   73 | 01m53s |  -28.99666 |             0.6644 |          0.1043 |     45.3763 |             17.2625 |             0.0048 |           0.0002 |       0.0086 |      0.7500 | \n",
      "   74 | 01m54s |  -30.16808 |             0.6098 |          0.1904 |     36.3103 |             18.6054 |             0.0042 |           0.0007 |       0.0012 |      0.9859 | \n",
      "   75 | 02m00s |  -29.38298 |             0.6743 |          0.1383 |     46.9285 |             15.4779 |             0.0015 |           0.0036 |       0.0149 |      0.7496 | \n",
      "   76 | 02m08s |  -28.95741 |             0.7992 |          0.1065 |     43.3133 |             16.1110 |             0.0047 |           0.0040 |       0.0181 |      0.9896 | \n",
      "   77 | 01m25s |  -29.24673 |             0.9073 |          0.0933 |     32.1608 |             29.7719 |             0.0024 |           0.0015 |       0.0116 |      0.5416 | \n",
      "   78 | 01m42s |  -29.44983 |             0.9928 |          0.0824 |     21.8325 |             18.4457 |             0.0032 |           0.0041 |       0.0051 |      0.7185 | \n",
      "   79 | 02m09s |  -28.89470 |             0.8273 |          0.1074 |     46.2009 |             17.3272 |             0.0006 |           0.0003 |       0.0081 |      0.9776 | \n",
      "   80 | 01m42s |  -29.11369 |             0.6983 |          0.0818 |     30.1128 |             21.0027 |             0.0040 |           0.0011 |       0.0111 |      0.7931 | \n",
      "   81 | 01m37s |  -29.05948 |             0.6226 |          0.0889 |     29.5645 |             15.5137 |             0.0021 |           0.0036 |       0.0040 |      0.5113 | \n",
      "   82 | 01m55s |  -29.13724 |             0.6473 |          0.1399 |     30.8430 |             15.0328 |             0.0035 |           0.0015 |       0.0140 |      0.8665 | \n",
      "   83 | 01m43s |  -29.00717 |             0.6593 |          0.1160 |     44.3274 |             29.3313 |             0.0019 |           0.0038 |       0.0121 |      0.9293 | \n",
      "   84 | 01m26s |  -29.13694 |             0.6314 |          0.0840 |     23.3705 |             29.8177 |             0.0031 |           0.0016 |       0.0148 |      0.8584 | \n",
      "   85 | 01m10s |  -29.22391 |             0.7866 |          0.0856 |     14.1086 |             21.1354 |             0.0036 |           0.0003 |       0.0063 |      0.8243 | \n",
      "   86 | 02m07s |  -29.59589 |             0.6215 |          0.1528 |     43.3543 |             15.4970 |             0.0016 |           0.0004 |       0.0183 |      0.9721 | \n",
      "   87 | 00m49s |  -29.69939 |             0.9155 |          0.1670 |     11.7254 |             19.3563 |             0.0044 |           0.0034 |       0.0128 |      0.8420 | \n",
      "   88 | 01m55s |  -29.98836 |             0.8495 |          0.1347 |     45.8477 |             15.0116 |             0.0016 |           0.0019 |       0.0192 |      0.5385 | \n",
      "   89 | 00m46s |  -28.81722 |             0.6616 |          0.0873 |     11.7713 |             25.4509 |             0.0006 |           0.0015 |       0.0016 |      0.8220 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   90 | 01m31s |  -28.79706 |             0.9262 |          0.1176 |     17.5505 |             18.9114 |             0.0022 |           0.0049 |       0.0191 |      0.8737 | \n",
      "   91 | 01m58s |  -29.25875 |             0.6298 |          0.0817 |     43.9227 |             17.3082 |             0.0040 |           0.0016 |       0.0179 |      0.9373 | \n",
      "   92 | 01m31s |  -29.75598 |             0.6372 |          0.1776 |     23.2396 |             17.7917 |             0.0029 |           0.0013 |       0.0133 |      0.7529 | \n",
      "   93 | 01m14s |  -28.82422 |             0.6014 |          0.0983 |     24.3895 |             25.1544 |             0.0024 |           0.0044 |       0.0046 |      0.5759 | \n",
      "   94 | 00m48s |  -29.17533 |             0.6442 |          0.0898 |     11.9899 |             15.2537 |             0.0043 |           0.0043 |       0.0084 |      0.5784 | \n",
      "   95 | 02m05s |  -29.33089 |             0.9679 |          0.1119 |     46.0504 |             20.9527 |             0.0019 |           0.0008 |       0.0166 |      0.9885 | \n",
      "   96 | 01m52s |  -28.96373 |             0.7926 |          0.1094 |     36.6021 |             17.9261 |             0.0032 |           0.0011 |       0.0183 |      0.7019 | \n",
      "   97 | 02m18s |  -30.28474 |             0.9627 |          0.1922 |     44.9535 |             15.6771 |             0.0048 |           0.0004 |       0.0160 |      0.9103 | \n",
      "   98 | 01m06s | \u001b[35m -28.55718\u001b[0m | \u001b[32m            0.8801\u001b[0m | \u001b[32m         0.0927\u001b[0m | \u001b[32m    13.9022\u001b[0m | \u001b[32m            20.6457\u001b[0m | \u001b[32m            0.0032\u001b[0m | \u001b[32m          0.0031\u001b[0m | \u001b[32m      0.0200\u001b[0m | \u001b[32m     0.8573\u001b[0m | \n",
      "   99 | 01m04s |  -28.96789 |             0.7671 |          0.1059 |     19.7298 |             22.6613 |             0.0029 |           0.0038 |       0.0187 |      0.5033 | \n",
      "  100 | 01m39s |  -28.60158 |             0.6271 |          0.1042 |     31.6652 |             21.0458 |             0.0026 |           0.0031 |       0.0010 |      0.7868 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n",
      "  101 | 00m53s |  -29.57831 |             1.0000 |          0.0800 |     10.0000 |             30.0000 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  102 | 01m50s |  -29.65769 |             0.6000 |          0.0800 |     50.0000 |             30.0000 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  103 | 01m54s |  -29.38015 |             0.6000 |          0.0800 |     50.0000 |             15.0000 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  104 | 01m57s |  -29.65089 |             1.0000 |          0.0800 |     39.0152 |             30.0000 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  105 | 00m59s |  -29.66507 |             1.0000 |          0.0800 |     10.0000 |             23.2593 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  106 | 02m25s |  -29.55586 |             1.0000 |          0.0800 |     35.4885 |             15.0000 |             0.0050 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  107 | 01m23s |  -29.40551 |             0.6000 |          0.0800 |     16.9036 |             30.0000 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  108 | 01m11s |  -29.64846 |             1.0000 |          0.0800 |     10.0000 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  109 | 01m52s |  -29.61193 |             1.0000 |          0.0800 |     28.0811 |             30.0000 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  110 | 02m08s |  -29.51414 |             1.0000 |          0.0800 |     19.7855 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  111 | 01m53s |  -29.48550 |             1.0000 |          0.0800 |     15.0040 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  112 | 01m09s |  -29.74351 |             0.6000 |          0.0800 |     13.9126 |             30.0000 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  113 | 02m03s |  -29.39824 |             0.6000 |          0.0800 |     50.0000 |             21.3900 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  114 | 01m59s |  -29.52999 |             0.6000 |          0.0800 |     27.2820 |             19.9012 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  115 | 02m06s |  -29.62674 |             1.0000 |          0.0800 |     40.0956 |             15.0000 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  116 | 01m50s |  -29.42404 |             0.6000 |          0.0800 |     35.3862 |             30.0000 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  117 | 01m29s |  -29.73248 |             0.6000 |          0.0800 |     42.2679 |             30.0000 |             0.0050 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  118 | 02m01s |  -29.53612 |             0.6000 |          0.0800 |     32.8154 |             20.6365 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  119 | 01m58s |  -29.54771 |             0.6000 |          0.0800 |     31.0043 |             21.7715 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  120 | 02m00s |  -29.47446 |             0.6000 |          0.0800 |     30.7734 |             19.6892 |             0.0050 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  121 | 02m24s |  -29.55310 |             1.0000 |          0.0800 |     28.2132 |             15.0000 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  122 | 01m43s |  -29.63187 |             1.0000 |          0.0800 |     50.0000 |             26.1327 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  123 | 01m49s |  -29.54659 |             1.0000 |          0.0800 |     37.2253 |             21.5697 |             0.0005 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  124 | 01m01s |  -29.23834 |             0.6000 |          0.0800 |     10.0000 |             19.2006 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  125 | 02m04s |  -29.60609 |             0.6000 |          0.0800 |     32.4044 |             18.1201 |             0.0050 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  126 | 01m56s |  -29.68653 |             0.6000 |          0.0800 |     45.2930 |             26.2882 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  127 | 01m58s |  -29.77919 |             1.0000 |          0.0800 |     18.6262 |             18.1760 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  128 | 02m02s |  -29.34001 |             0.6000 |          0.0800 |     22.3090 |             15.0000 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  129 | 01m26s |  -29.56050 |             0.6000 |          0.0800 |     28.8010 |             26.1659 |             0.0050 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  130 | 01m40s |  -29.19891 |             1.0000 |          0.0800 |     16.7759 |             23.3345 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  131 | 00m56s |  -29.48237 |             0.6000 |          0.0800 |     10.0000 |             25.1340 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  132 | 01m51s |  -29.38141 |             1.0000 |          0.0800 |     16.2984 |             18.1000 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  133 | 01m25s |  -29.75323 |             0.6000 |          0.0800 |     17.3232 |             15.0000 |             0.0050 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  134 | 01m00s |  -29.67093 |             1.0000 |          0.0800 |     15.3626 |             30.0000 |             0.0050 |           0.0000 |       0.0000 |      0.5000 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  135 | 01m58s |  -29.61118 |             0.6000 |          0.0800 |     28.9270 |             21.4075 |             0.0050 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  136 | 02m14s |  -29.45036 |             1.0000 |          0.0800 |     45.0867 |             22.8028 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  137 | 01m49s |  -29.67421 |             0.6000 |          0.0800 |     30.5751 |             30.0000 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  138 | 01m17s |  -29.50529 |             0.6000 |          0.0800 |     25.7441 |             30.0000 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  139 | 02m02s |  -29.54336 |             0.6000 |          0.0800 |     31.7523 |             20.8936 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  140 | 01m38s |  -29.61644 |             0.6000 |          0.0800 |     32.3066 |             21.6746 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  141 | 01m33s |  -29.41780 |             0.6000 |          0.0800 |     33.0738 |             24.8245 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  142 | 01m43s |  -29.64880 |             0.6000 |          0.0800 |     33.5352 |             19.5608 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  143 | 01m10s |  -29.49025 |             1.0000 |          0.0800 |     19.1961 |             30.0000 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  144 | 02m04s |  -29.60686 |             0.6000 |          0.0800 |     28.6341 |             17.5502 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  145 | 02m08s |  -29.70196 |             1.0000 |          0.0800 |     41.2984 |             26.1799 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  146 | 02m29s |  -29.84027 |             1.0000 |          0.0800 |     38.0684 |             15.0000 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  147 | 01m38s |  -29.54957 |             0.6000 |          0.0800 |     31.3990 |             21.4130 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  148 | 01m47s |  -31.33997 |             1.0000 |          0.2000 |     50.0000 |             23.5535 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  149 | 02m02s |  -29.54915 |             1.0000 |          0.0800 |     31.0939 |             26.4758 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  150 | 02m02s |  -29.59275 |             1.0000 |          0.0800 |     27.6038 |             24.5337 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  151 | 00m59s |  -30.76184 |             0.6000 |          0.2000 |     13.3113 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  152 | 01m34s |  -30.99440 |             1.0000 |          0.2000 |     44.6581 |             30.0000 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  153 | 01m34s |  -29.50477 |             0.6000 |          0.0800 |     25.6402 |             20.9495 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  154 | 02m01s |  -29.55316 |             1.0000 |          0.0800 |     21.3132 |             21.8097 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  155 | 01m19s |  -31.23756 |             1.0000 |          0.2000 |     24.5414 |             26.8427 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  156 | 01m22s |  -29.54353 |             0.6000 |          0.0800 |     22.8064 |             24.5593 |             0.0050 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  157 | 01m46s |  -29.27349 |             0.6000 |          0.0800 |     26.7190 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  158 | 01m48s |  -29.41429 |             0.6000 |          0.0800 |     50.0000 |             19.6762 |             0.0050 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  159 | 01m57s |  -29.50065 |             0.6000 |          0.0800 |     44.3243 |             28.0408 |             0.0050 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  160 | 02m05s |  -30.28880 |             0.6000 |          0.2000 |     47.3831 |             25.9862 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  161 | 01m45s |  -29.72783 |             0.6000 |          0.0800 |     20.2567 |             26.9170 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  162 | 01m51s |  -29.49059 |             0.6000 |          0.0800 |     27.5006 |             27.3455 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  163 | 01m11s |  -30.87163 |             1.0000 |          0.2000 |     22.3451 |             30.0000 |             0.0005 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  164 | 01m40s |  -29.57501 |             0.6000 |          0.0800 |     20.3389 |             30.0000 |             0.0050 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  165 | 01m54s |  -29.58867 |             1.0000 |          0.0800 |     24.5069 |             30.0000 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  166 | 01m39s |  -29.47279 |             1.0000 |          0.0800 |     14.2747 |             20.0759 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  167 | 01m17s |  -29.56292 |             0.6000 |          0.0800 |     18.2143 |             22.0192 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  168 | 02m15s |  -29.61833 |             0.6000 |          0.0800 |     33.2948 |             15.0000 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  169 | 01m28s |  -31.37404 |             1.0000 |          0.2000 |     33.7457 |             30.0000 |             0.0050 |           0.0000 |       0.0000 |      0.5000 | \n",
      "  170 | 01m54s |  -29.48120 |             0.6000 |          0.0800 |     37.0714 |             30.0000 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  171 | 02m08s |  -29.77840 |             1.0000 |          0.0800 |     33.4519 |             24.3113 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  172 | 01m02s |  -29.55944 |             0.6000 |          0.0800 |     13.6558 |             20.1521 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  173 | 01m31s |  -29.69337 |             1.0000 |          0.0800 |     13.4881 |             20.7661 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  174 | 01m02s |  -30.80941 |             1.0000 |          0.2000 |     14.1054 |             20.7133 |             0.0050 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  175 | 00m46s |  -30.67249 |             0.6000 |          0.2000 |     10.0000 |             21.1836 |             0.0050 |           0.0000 |       0.0000 |      0.5000 | \n",
      "  176 | 01m31s |  -29.47178 |             0.6000 |          0.0800 |     14.0335 |             20.5801 |             0.0050 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  177 | 01m04s |  -29.83473 |             1.0000 |          0.0800 |     12.6896 |             18.7711 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  178 | 01m33s |  -29.48998 |             0.6000 |          0.0800 |     15.1096 |             23.1041 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  179 | 02m35s |  -29.71620 |             1.0000 |          0.0800 |     47.8423 |             15.0000 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  180 | 02m08s |  -29.68047 |             0.6000 |          0.0800 |     47.2140 |             22.6285 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  181 | 00m56s |  -29.54936 |             1.0000 |          0.0800 |     10.0000 |             18.1076 |             0.0050 |           0.0050 |       0.0000 |      0.5000 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  182 | 01m58s |  -29.75296 |             1.0000 |          0.0800 |     18.3259 |             20.3435 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  183 | 02m13s |  -29.78985 |             1.0000 |          0.0800 |     38.8432 |             23.5158 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  184 | 01m53s |  -29.48164 |             0.6000 |          0.0800 |     31.7424 |             18.7727 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  185 | 02m05s |  -29.50105 |             0.6000 |          0.0800 |     29.5009 |             20.0465 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  186 | 01m39s |  -29.47479 |             0.6000 |          0.0800 |     35.0925 |             26.3703 |             0.0005 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  187 | 01m57s |  -29.36732 |             1.0000 |          0.0800 |     40.0828 |             20.7641 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  188 | 02m00s |  -29.39621 |             1.0000 |          0.0800 |     37.7117 |             19.1398 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  189 | 01m50s |  -29.34478 |             1.0000 |          0.0800 |     26.0942 |             19.2945 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  190 | 02m37s |  -30.29646 |             1.0000 |          0.2000 |     50.0000 |             17.3094 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  191 | 00m54s |  -29.65936 |             0.6000 |          0.0800 |     11.6959 |             24.5121 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  192 | 01m59s |  -29.51446 |             0.6000 |          0.0800 |     19.3441 |             17.0546 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  193 | 01m48s |  -29.42249 |             0.6000 |          0.0800 |     21.9677 |             28.1430 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  194 | 02m02s |  -30.29322 |             0.6000 |          0.2000 |     42.2430 |             27.8711 |             0.0050 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  195 | 01m50s |  -29.86788 |             1.0000 |          0.0800 |     43.2278 |             26.1023 |             0.0005 |           0.0000 |       0.0000 |      0.5000 | \n",
      "  196 | 01m31s |  -29.62177 |             1.0000 |          0.0800 |     14.1103 |             28.3086 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  197 | 02m13s |  -30.04217 |             0.6000 |          0.2000 |     29.7275 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  198 | 01m59s |  -29.68577 |             1.0000 |          0.0800 |     23.5550 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  199 | 02m04s |  -29.53949 |             0.6000 |          0.0800 |     25.4757 |             19.6241 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  200 | 01m30s |  -29.48564 |             0.6000 |          0.0800 |     15.3850 |             28.7142 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': (0.08, 0.2),\n",
    "    'reg_lambda':(0,0.02),\n",
    "    'min_split_gain': (0,0.005),\n",
    "    'max_depth':(10,50),\n",
    "    'colsample_bytree':(0.6,1),\n",
    "    'min_child_samples':(15,30),\n",
    "    'min_child_weight':(0.0005,0.005),\n",
    "    'subsample':(0.5,1)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=100,n_iter=100,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random initialization stages found a combination of parameters that improved model performance. Let's fit the model to see the performance once again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n",
    "\n",
    "  93 | 01m19s |  -28.53989 |             0.6014 |          0.0983 |     24.3895 |             25.1544 |             0.0024 |           0.0044 |       0.0046 |      0.5759 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 294,\n",
    "                              learning_rate= 0.0983,reg_lambda= 0.0046,\n",
    "                              min_split_gain = 0.0044, max_depth = 24,\n",
    "                              colsample_bytree = 0.6014,\n",
    "                              min_child_samples = 25,\n",
    "                              min_child_weight = 0.0024,subsample =  0.5759,\n",
    "                              n_estimators= 226, reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision-tree based algorithms for prediction\n",
    "\n",
    "In this part of the work, we will try to train decision-tree based algorithms to make predicitions about the timeline.\n",
    "\n",
    "### Building feature extraction pipeline\n",
    "\n",
    "Tree-based algorithms would require feature selection, so we will need to use a reasonable number of features. We will start by including 300 best features using our tokenization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the work into a dedicated workspace \"ridge\"\n",
    "disk_tree = \"tree\"\n",
    "import os\n",
    "if not os.path.exists(disk_tree):\n",
    "    os.makedirs(disk_tree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree1 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression, 300))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load Training and Validation sets\n",
    "disk = \"D:\\Data_science\\GitHub\\Predictive-Modeling-510k-decision-time\"\n",
    "# Validation set \n",
    "with open(disk+\"\\X_val.pkl\",\"rb\") as f:\n",
    "    X_val=pickle.load(f)\n",
    "\n",
    "with open(disk+\"\\y_val.pkl\",\"rb\") as f:\n",
    "    y_val=pickle.load(f)\n",
    "    \n",
    "# Training set (Locked down)\n",
    "with open(disk+\"\\X_train.pkl\",\"rb\") as f:\n",
    "    X_train=pickle.load(f)\n",
    "\n",
    "with open(disk+\"\\y_train.pkl\",\"rb\") as f:\n",
    "    y_train=pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.7833333333333333 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree = pipeline510k_tree1.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree = pipeline510k_tree1.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 300)\n",
      "(15899, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree.shape)\n",
    "print(X_val_trans_tree.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regresion\n",
    "\n",
    "We will first train an untuned Gradient Boosting algorithm to see performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7446            2.62m\n",
      "         2           0.7138            2.48m\n",
      "         3           0.6883            2.46m\n",
      "         4           0.6669            2.41m\n",
      "         5           0.6498            2.41m\n",
      "         6           0.6349            2.42m\n",
      "         7           0.6224            2.42m\n",
      "         8           0.6123            2.42m\n",
      "         9           0.6037            2.40m\n",
      "        10           0.5963            2.40m\n",
      "        20           0.5585            2.29m\n",
      "        30           0.5420            2.15m\n",
      "        40           0.5296            2.01m\n",
      "        50           0.5196            1.89m\n",
      "        60           0.5103            1.76m\n",
      "        70           0.5023            1.63m\n",
      "        80           0.4953            1.48m\n",
      "        90           0.4888            1.35m\n",
      "       100           0.4827            1.21m\n",
      "       200           0.4350            0.00s\n",
      "Median Absolute Error:  37.452053685765335\n",
      "Completed model fit and predictions in: 2.25 minutes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "gbm1 = GradientBoostingRegressor(verbose = 1, n_estimators= 200, max_depth=5)\n",
    "\n",
    "gbm1.fit(X_train_trans_tree, np.log(y_train))\n",
    "preds = gbm1.predict(X_val_trans_tree)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed: 49.8min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 75.3min\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed: 115.8min\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 125.9min\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed: 170.8min\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed: 183.3min\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed: 204.8min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 217.8min\n",
      "[Parallel(n_jobs=6)]: Done 108 out of 108 | elapsed: 250.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 108 out of 108 | elapsed: 250.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed GridSearch in: 259.8333333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,500,1000],\n",
    "    'max_depth':[5,10,15],\n",
    "    'learning_rate': [0.1,0.25,0.75,1]\n",
    "}\n",
    "\n",
    "# We have 8 CPU cores, we will use 6 for this task\n",
    "gbmSearch1 = GridSearchCV(estimator= GradientBoostingRegressor(), \n",
    "                            param_grid= param_grid,\n",
    "                            n_jobs = 6,\n",
    "                            cv = 3,\n",
    "                            verbose = 10, scoring= 'neg_median_absolute_error'\n",
    "                            )\n",
    "gbmSearch1.fit(X_train_trans_tree, np.log(y_train))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed GridSearch in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=10, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbmSearch1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.38059952933966"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "# Let's see the validation MAE (in original target scale) from the best estimator\n",
    "preds = gbmSearch1.best_estimator_.predict(X_val_trans_tree)\n",
    "median_absolute_error(y_val,np.exp(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting\n",
    "\n",
    "Let's try to use relatively new ligtGBM package to see the performance. The package has a sklearn interface we will use to perform hyperparameter optimization as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  36.951519001311084\n",
      "Completed model fit and predictions in: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression')\n",
    "\n",
    "lgbm1.fit(X_train_trans_tree, np.log(y_train))\n",
    "preds = lgbm1.predict(X_val_trans_tree)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that lightGBM is indeed extremely faster and performed better than even the tuned traditional GBM. We will experiment this algorithm and attempt hyperparameter optimization. Let's define a new pipeline without feature selection to also experiment the impact of adding more features on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree2 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()) # Scale the features\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.75 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree2 = pipeline510k_tree2.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree2 = pipeline510k_tree2.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 262146)\n",
      "(15899, 262146)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree2.shape)\n",
    "print(X_val_trans_tree2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  29.467624324930185\n",
      "Completed model fit and predictions in: 1.3666666666666667 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "Xt = SelectKBest(f_regression,10000).fit(X_train_trans_tree2,y_train).transform(X_train_trans_tree2)\n",
    "Xv = SelectKBest(f_regression,10000).fit(X_train_trans_tree2,y_train).transform(X_val_trans_tree2)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                          n_estimators=200, reg_alpha= 0.1,\n",
    "                          boosting_type= 'dart')\n",
    "\n",
    "lgbm1.fit(Xt, np.log(y_train))\n",
    "preds = lgbm1.predict(Xv)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lgbm is robust to overfitting when adding more features. Let's try to first search for the number of features to be included in the fixed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model using 300\n",
      "Completed model fit and predictions using 300 in: 0.1 minutes.\n",
      "Median Absolute Error:  33.107162652136324\n",
      "**************************************************\n",
      "Training model using 5000\n",
      "Completed model fit and predictions using 5000 in: 0.8833333333333333 minutes.\n",
      "Median Absolute Error:  29.633671997998164\n",
      "**************************************************\n",
      "Training model using 10000\n",
      "Completed model fit and predictions using 10000 in: 1.4 minutes.\n",
      "Median Absolute Error:  29.467624324930185\n",
      "**************************************************\n",
      "Training model using 20000\n",
      "Completed model fit and predictions using 20000 in: 1.6666666666666667 minutes.\n",
      "Median Absolute Error:  29.30898855444397\n",
      "**************************************************\n",
      "Training model using 50000\n",
      "Completed model fit and predictions using 50000 in: 1.9 minutes.\n",
      "Median Absolute Error:  29.209825132888795\n",
      "**************************************************\n",
      "Training model using 100000\n",
      "Completed model fit and predictions using 100000 in: 2.0166666666666666 minutes.\n",
      "Median Absolute Error:  29.166814835270287\n",
      "**************************************************\n",
      "Training model using 200000\n",
      "Completed model fit and predictions using 200000 in: 2.066666666666667 minutes.\n",
      "Median Absolute Error:  29.19324890053349\n",
      "**************************************************\n",
      "Training model using 262146\n",
      "Completed model fit and predictions using 262146 in: 2.066666666666667 minutes.\n",
      "Median Absolute Error:  29.478673965840073\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "n_features_list = [300,5000,10000,20000,50000,100000,200000,262146]\n",
    "mae_list = []\n",
    "time_list = []\n",
    "\n",
    "for n_features in n_features_list:\n",
    "    print(\"Training model using \"+ str(n_features))\n",
    "    \n",
    "    # Testing feature selection based on training set\n",
    "    Xt = SelectKBest(f_regression,n_features).fit(X_train_trans_tree2,y_train).transform(X_train_trans_tree2)\n",
    "    Xv = SelectKBest(f_regression,n_features).fit(X_train_trans_tree2,y_train).transform(X_val_trans_tree2)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Fixed model structure \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                              n_estimators=200, reg_alpha= 0.1,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(Xt, np.log(y_train))\n",
    "    preds = lgbm1.predict(Xv)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    mae_list.append(mae)\n",
    "    time_list.append((end-start).seconds/60)\n",
    "\n",
    "    print(\"Completed model fit and predictions using \"+ str(n_features) + \" in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "    print(\"Median Absolute Error: \", str(mae))\n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like model starts to overfit beyond 100000 training features. We will define the new pipeline, transform the data sets before further hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM Feature extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree3 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression, 100000))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.8 minutes.\n",
      "Completed processing X_val in: 0.2 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree3 = pipeline510k_tree3.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree3 = pipeline510k_tree3.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 100000)\n",
      "(15899, 100000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree3.shape)\n",
    "print(X_val_trans_tree3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 192 candidates, totalling 1152 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed: 40.0min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 45.6min\n",
      "[Parallel(n_jobs=6)]: Done 101 tasks      | elapsed: 53.6min\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed: 67.4min\n",
      "[Parallel(n_jobs=6)]: Done 133 tasks      | elapsed: 77.6min\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed: 88.1min\n",
      "[Parallel(n_jobs=6)]: Done 169 tasks      | elapsed: 109.2min\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed: 119.2min\n",
      "[Parallel(n_jobs=6)]: Done 209 tasks      | elapsed: 134.5min\n",
      "[Parallel(n_jobs=6)]: Done 230 tasks      | elapsed: 147.4min\n",
      "[Parallel(n_jobs=6)]: Done 253 tasks      | elapsed: 166.2min\n",
      "[Parallel(n_jobs=6)]: Done 276 tasks      | elapsed: 183.2min\n",
      "[Parallel(n_jobs=6)]: Done 301 tasks      | elapsed: 207.8min\n",
      "[Parallel(n_jobs=6)]: Done 326 tasks      | elapsed: 230.0min\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed: 261.4min\n",
      "[Parallel(n_jobs=6)]: Done 380 tasks      | elapsed: 287.1min\n",
      "[Parallel(n_jobs=6)]: Done 409 tasks      | elapsed: 316.0min\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed: 337.7min\n",
      "[Parallel(n_jobs=6)]: Done 469 tasks      | elapsed: 372.9min\n",
      "[Parallel(n_jobs=6)]: Done 500 tasks      | elapsed: 416.7min\n",
      "[Parallel(n_jobs=6)]: Done 533 tasks      | elapsed: 453.6min\n",
      "[Parallel(n_jobs=6)]: Done 566 tasks      | elapsed: 506.1min\n",
      "[Parallel(n_jobs=6)]: Done 601 tasks      | elapsed: 547.0min\n",
      "[Parallel(n_jobs=6)]: Done 636 tasks      | elapsed: 583.7min\n",
      "[Parallel(n_jobs=6)]: Done 673 tasks      | elapsed: 636.1min\n",
      "[Parallel(n_jobs=6)]: Done 710 tasks      | elapsed: 696.0min\n",
      "[Parallel(n_jobs=6)]: Done 749 tasks      | elapsed: 774.8min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed: 840.9min\n",
      "[Parallel(n_jobs=6)]: Done 829 tasks      | elapsed: 926.1min\n",
      "[Parallel(n_jobs=6)]: Done 870 tasks      | elapsed: 1009.0min\n",
      "[Parallel(n_jobs=6)]: Done 913 tasks      | elapsed: 1143.1min\n",
      "[Parallel(n_jobs=6)]: Done 956 tasks      | elapsed: 1269.8min\n",
      "[Parallel(n_jobs=6)]: Done 1001 tasks      | elapsed: 1381.4min\n",
      "[Parallel(n_jobs=6)]: Done 1046 tasks      | elapsed: 1511.5min\n",
      "[Parallel(n_jobs=6)]: Done 1093 tasks      | elapsed: 1682.7min\n",
      "[Parallel(n_jobs=6)]: Done 1140 tasks      | elapsed: 1884.7min\n",
      "[Parallel(n_jobs=6)]: Done 1152 out of 1152 | elapsed: 1904.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed GridSearch in: 467.56666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,150,200,250,400,500],\n",
    "    'num_leaves':[150,200,300,400],\n",
    "    'reg_alpha': [0,0.01,0.05,0.1,0.5,1,5,10],\n",
    "    'objective': ['regression'],\n",
    "    'boosting_type': ['dart']\n",
    "}\n",
    "\n",
    "# We have 8 CPU cores, we will use 6 for this task\n",
    "lgbmSearch1 = GridSearchCV(estimator= lgb.LGBMRegressor(), \n",
    "                            param_grid= param_grid,\n",
    "                            n_jobs = 6,\n",
    "                            cv = 6,\n",
    "                            verbose = 10, scoring= 'neg_median_absolute_error'\n",
    "                            )\n",
    "\n",
    "lgbmSearch1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed GridSearch in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=500,\n",
       "       n_jobs=-1, num_leaves=150, objective='regression',\n",
       "       random_state=None, reg_alpha=0.1, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbmSearch1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.38894595686144295"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbmSearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.504873585145397"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "# Let's see the validation MAE (in original target scale) from the best estimator\n",
    "preds = lgbmSearch1.best_estimator_.predict(X_val_trans_tree3)\n",
    "median_absolute_error(y_val,np.exp(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the 4 parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(num_leaves,n_estimators,reg_alpha):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(num_leaves),\n",
    "                              n_estimators= int(n_estimators), reg_alpha= reg_alpha,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 01m16s | \u001b[35m -35.12445\u001b[0m | \u001b[32m      161.3516\u001b[0m | \u001b[32m    206.2586\u001b[0m | \u001b[32m     0.0548\u001b[0m | \n",
      "    2 | 01m58s | \u001b[35m -31.10728\u001b[0m | \u001b[32m      266.5394\u001b[0m | \u001b[32m    246.0457\u001b[0m | \u001b[32m     0.8177\u001b[0m | \n",
      "    3 | 02m06s | \u001b[35m -30.57605\u001b[0m | \u001b[32m      274.9119\u001b[0m | \u001b[32m    292.5025\u001b[0m | \u001b[32m     0.8854\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    4 | 01m47s | \u001b[35m -30.11682\u001b[0m | \u001b[32m      300.0000\u001b[0m | \u001b[32m    150.0000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "    5 | 03m14s | \u001b[35m -29.74980\u001b[0m | \u001b[32m      300.0000\u001b[0m | \u001b[32m    300.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    6 | 02m16s |  -30.23047 |       300.0000 |     300.0000 |      1.0000 | \n",
      "    7 | 01m38s |  -30.90307 |       150.0000 |     300.0000 |      0.0000 | \n",
      "    8 | 01m05s |  -31.16249 |       150.0000 |     150.0000 |      0.0000 | \n",
      "    9 | 02m34s | \u001b[35m -29.10945\u001b[0m | \u001b[32m      231.1680\u001b[0m | \u001b[32m    300.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "   10 | 02m44s |  -29.77941 |       300.0000 |     227.7304 |      0.0000 | \n",
      "   11 | 02m40s |  -29.13963 |       245.7091 |     300.0000 |      0.0000 | \n",
      "   12 | 02m35s |  -29.20123 |       237.4534 |     300.0000 |      0.0000 | \n",
      "   13 | 01m36s |  -29.24059 |       231.9763 |     150.0000 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (150, 300),\n",
    "    'n_estimators' : (150,300),\n",
    "    'reg_alpha': (0,1)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=3,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian optimization is promising to find parameter that may improve the performance. Let's try another round of optimization cycle by focusing on a narrower hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m51s | \u001b[35m -29.02788\u001b[0m | \u001b[32m      244.9824\u001b[0m | \u001b[32m    331.2563\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    2 | 02m36s |  -29.04927 |       221.6442 |     360.4335 |      0.0098 | \n",
      "    3 | 03m05s |  -29.39139 |       244.5303 |     394.5018 |      0.0034 | \n",
      "    4 | 02m32s |  -29.36064 |       246.5605 |     298.3245 |      0.0009 | \n",
      "    5 | 02m53s |  -29.93163 |       241.6704 |     375.4623 |      0.0039 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    6 | 02m18s |  -29.10648 |       220.0000 |     290.0000 |      0.0000 | \n",
      "    7 | 02m52s |  -29.40106 |       220.0000 |     400.0000 |      0.0000 | \n",
      "    8 | 02m28s | \u001b[35m -29.02445\u001b[0m | \u001b[32m      220.0000\u001b[0m | \u001b[32m    334.1820\u001b[0m | \u001b[32m     0.0100\u001b[0m | \n",
      "    9 | 03m13s |  -29.80390 |       250.0000 |     400.0000 |      0.0100 | \n",
      "   10 | 02m44s |  -29.44771 |       250.0000 |     290.0000 |      0.0000 | \n",
      "   11 | 02m55s |  -29.49083 |       250.0000 |     318.5344 |      0.0000 | \n",
      "   12 | 02m42s |  -29.07049 |       220.0000 |     348.3952 |      0.0000 | \n",
      "   13 | 02m26s | \u001b[35m -28.82805\u001b[0m | \u001b[32m      220.0000\u001b[0m | \u001b[32m    313.9619\u001b[0m | \u001b[32m     0.0100\u001b[0m | \n",
      "   14 | 02m48s |  -29.04563 |       220.0000 |     375.1665 |      0.0000 | \n",
      "   15 | 02m30s |  -29.25785 |       220.0000 |     319.4859 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (290, 400),\n",
    "    'n_estimators' : (220,250),\n",
    "    'reg_alpha': (0,0.01)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to find parameters that further improved performance. Let's try to further narrow down the hyperparameter space and perform another round of Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m29s | \u001b[35m -29.10137\u001b[0m | \u001b[32m      224.9824\u001b[0m | \u001b[32m    312.5034\u001b[0m | \u001b[32m     0.0001\u001b[0m | \n",
      "    2 | 02m17s |  -29.23025 |       201.6442 |     328.4183 |      0.0245 | \n",
      "    3 | 02m34s |  -29.46624 |       224.5303 |     347.0010 |      0.0086 | \n",
      "    4 | 02m21s | \u001b[35m -28.79092\u001b[0m | \u001b[32m      226.5605\u001b[0m | \u001b[32m    294.5406\u001b[0m | \u001b[32m     0.0024\u001b[0m | \n",
      "    5 | 02m30s |  -29.39501 |       221.6704 |     336.6158 |      0.0099 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    6 | 02m05s |  -29.11512 |       200.0636 |     290.0450 |      0.0122 | \n",
      "    7 | 02m30s |  -28.87265 |       230.0000 |     290.0000 |      0.0000 | \n",
      "    8 | 02m22s |  -29.21013 |       200.0000 |     350.0000 |      0.0250 | \n",
      "    9 | 02m20s |  -29.08370 |       220.0908 |     292.8533 |      0.0250 | \n",
      "   10 | 02m28s |  -29.09202 |       230.0000 |     300.9593 |      0.0250 | \n",
      "   11 | 02m50s |  -29.08896 |       230.0000 |     350.0000 |      0.0000 | \n",
      "   12 | 02m14s |  -29.23849 |       200.0000 |     311.5937 |      0.0000 | \n",
      "   13 | 02m28s |  -29.01811 |       225.4564 |     296.3131 |      0.0010 | \n",
      "   14 | 02m35s |  -29.08392 |       230.0000 |     324.0520 |      0.0250 | \n",
      "   15 | 02m19s |  -29.30205 |       224.3218 |     290.0000 |      0.0250 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (290, 350),\n",
    "    'n_estimators' : (200,230),\n",
    "    'reg_alpha': (0,0.025)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization stage give us another lead for optimization. We will perform another round of optimization cycle using these 3 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m20s | \u001b[35m -28.92014\u001b[0m | \u001b[32m      224.9824\u001b[0m | \u001b[32m    279.3764\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    2 | 02m01s |  -29.27668 |       201.6442 |     286.0076 |      0.0049 | \n",
      "    3 | 02m20s |  -29.15800 |       224.5303 |     293.7504 |      0.0017 | \n",
      "    4 | 02m15s |  -29.11658 |       226.5605 |     271.8919 |      0.0005 | \n",
      "    5 | 02m16s |  -29.14482 |       221.6704 |     289.4232 |      0.0020 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    6 | 02m03s |  -28.96436 |       200.0000 |     270.0000 |      0.0000 | \n",
      "    7 | 02m21s |  -29.26499 |       230.0000 |     283.7620 |      0.0050 | \n",
      "    8 | 02m11s |  -28.95894 |       200.0000 |     295.0000 |      0.0000 | \n",
      "    9 | 02m18s |  -29.04792 |       217.1388 |     276.4416 |      0.0000 | \n",
      "   10 | 02m15s |  -29.25602 |       221.5866 |     280.0949 |      0.0050 | \n",
      "   11 | 02m18s | \u001b[35m -28.83104\u001b[0m | \u001b[32m      214.6859\u001b[0m | \u001b[32m    295.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "   12 | 02m12s |  -29.01606 |       213.2324 |     270.0000 |      0.0000 | \n",
      "   13 | 02m25s |  -28.97788 |       230.0000 |     276.3328 |      0.0000 | \n",
      "   14 | 02m24s |  -29.31890 |       218.5193 |     295.0000 |      0.0000 | \n",
      "   15 | 02m29s |  -29.03612 |       230.0000 |     295.0000 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (270, 295),\n",
    "    'n_estimators' : (200,230),\n",
    "    'reg_alpha': (0,0.005)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we are converging to some optimal values with these parameters. Now let's try to assign the optimal values for these 3 parameters and see if we can tune the learning rate to further improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(learning_rate):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(294.54),\n",
    "                              learning_rate= learning_rate,\n",
    "                              n_estimators= int(226.56), reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   learning_rate | \n",
      "    1 | 02m20s | \u001b[35m -29.10472\u001b[0m | \u001b[32m         0.0938\u001b[0m | \n",
      "    2 | 02m24s |  -29.20536 |          0.0964 | \n",
      "    3 | 02m27s | \u001b[35m -28.94624\u001b[0m | \u001b[32m         0.0995\u001b[0m | \n",
      "    4 | 02m23s |  -29.10463 |          0.0908 | \n",
      "    5 | 02m24s |  -29.15096 |          0.0978 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   learning_rate | \n",
      "    6 | 02m23s |  -29.25657 |          0.1000 | \n",
      "    7 | 02m27s | \u001b[35m -28.81794\u001b[0m | \u001b[32m         0.0900\u001b[0m | \n",
      "    8 | 02m24s |  -28.95310 |          0.0989 | \n",
      "    9 | 02m24s |  -28.84389 |          0.0983 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "   10 | 02m21s |  -29.08047 |          0.0922 | \n",
      "   11 | 02m21s |  -29.17043 |          0.0951 | \n",
      "   12 | 02m21s |  -29.41761 |          0.0905 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "   13 | 02m22s |  -29.06929 |          0.0914 | \n",
      "   14 | 02m22s |  -29.40204 |          0.0930 | \n",
      "   15 | 02m22s |  -29.21425 |          0.0944 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': (0.09, 0.15)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try a slightly wider learning rate space along with other parameters to see if we can get some lead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(learning_rate, \n",
    "               reg_lambda,min_split_gain,max_depth,\n",
    "               colsample_bytree,min_child_samples,min_child_weight,subsample):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(294.54),\n",
    "                              learning_rate= learning_rate,reg_lambda= reg_lambda,\n",
    "                              min_split_gain = min_split_gain, max_depth = int(max_depth),\n",
    "                              colsample_bytree = colsample_bytree,\n",
    "                              min_child_samples = int(min_child_samples),\n",
    "                              min_child_weight = min_child_weight,subsample = subsample,\n",
    "                              n_estimators= int(226.56), reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n",
      "    1 | 02m12s | \u001b[35m -29.13360\u001b[0m | \u001b[32m            0.9356\u001b[0m | \u001b[32m         0.1250\u001b[0m | \u001b[32m    42.3435\u001b[0m | \u001b[32m            17.3298\u001b[0m | \u001b[32m            0.0033\u001b[0m | \u001b[32m          0.0036\u001b[0m | \u001b[32m      0.0135\u001b[0m | \u001b[32m     0.6253\u001b[0m | \n",
      "    2 | 02m11s |  -29.74794 |             0.8311 |          0.1568 |     38.8017 |             16.3328 |             0.0027 |           0.0023 |       0.0033 |      0.8233 | \n",
      "    3 | 01m34s |  -30.94449 |             0.6996 |          0.1940 |     32.6418 |             23.2452 |             0.0022 |           0.0025 |       0.0096 |      0.6406 | \n",
      "    4 | 01m55s |  -29.35169 |             0.8275 |          0.0891 |     36.3681 |             25.9945 |             0.0042 |           0.0015 |       0.0136 |      0.9233 | \n",
      "    5 | 01m34s |  -30.85810 |             0.6651 |          0.1732 |     41.4745 |             23.1107 |             0.0045 |           0.0004 |       0.0043 |      0.5450 | \n",
      "    6 | 02m05s |  -30.29918 |             0.7499 |          0.1799 |     41.9683 |             17.4670 |             0.0013 |           0.0026 |       0.0079 |      0.8363 | \n",
      "    7 | 00m53s |  -29.14366 |             0.9363 |          0.0866 |     12.6346 |             24.5434 |             0.0013 |           0.0027 |       0.0041 |      0.7096 | \n",
      "    8 | 02m10s |  -30.43446 |             0.7713 |          0.1781 |     43.2717 |             16.8779 |             0.0008 |           0.0032 |       0.0040 |      0.8502 | \n",
      "    9 | 01m08s |  -30.45334 |             0.7669 |          0.1862 |     16.9824 |             19.6596 |             0.0027 |           0.0015 |       0.0069 |      0.6684 | \n",
      "   10 | 00m37s |  -30.28254 |             0.6448 |          0.1667 |     11.0216 |             26.4109 |             0.0008 |           0.0040 |       0.0137 |      0.6155 | \n",
      "   11 | 01m40s |  -29.25655 |             0.8335 |          0.0803 |     33.8375 |             23.3358 |             0.0010 |           0.0018 |       0.0176 |      0.6135 | \n",
      "   12 | 01m47s |  -31.26134 |             0.8395 |          0.1977 |     31.0373 |             15.7278 |             0.0030 |           0.0036 |       0.0090 |      0.5512 | \n",
      "   13 | 01m37s | \u001b[35m -28.89579\u001b[0m | \u001b[32m            0.8018\u001b[0m | \u001b[32m         0.1212\u001b[0m | \u001b[32m    17.6388\u001b[0m | \u001b[32m            16.5435\u001b[0m | \u001b[32m            0.0040\u001b[0m | \u001b[32m          0.0029\u001b[0m | \u001b[32m      0.0164\u001b[0m | \u001b[32m     0.8367\u001b[0m | \n",
      "   14 | 01m29s | \u001b[35m -28.85138\u001b[0m | \u001b[32m            0.9264\u001b[0m | \u001b[32m         0.0914\u001b[0m | \u001b[32m    21.0359\u001b[0m | \u001b[32m            29.8613\u001b[0m | \u001b[32m            0.0032\u001b[0m | \u001b[32m          0.0026\u001b[0m | \u001b[32m      0.0137\u001b[0m | \u001b[32m     0.8001\u001b[0m | \n",
      "   15 | 01m48s |  -29.53078 |             0.9838 |          0.1274 |     42.8848 |             19.8564 |             0.0024 |           0.0015 |       0.0161 |      0.5109 | \n",
      "   16 | 01m13s |  -29.22252 |             0.9526 |          0.0806 |     14.4557 |             24.8236 |             0.0048 |           0.0008 |       0.0052 |      0.8486 | \n",
      "   17 | 02m00s |  -30.04175 |             0.8775 |          0.1684 |     48.8247 |             24.1891 |             0.0039 |           0.0008 |       0.0115 |      0.8728 | \n",
      "   18 | 01m31s |  -31.14708 |             0.9826 |          0.1947 |     46.8444 |             29.4122 |             0.0012 |           0.0016 |       0.0031 |      0.5208 | \n",
      "   19 | 00m52s |  -30.29079 |             0.7960 |          0.1785 |     15.2055 |             27.4165 |             0.0049 |           0.0036 |       0.0190 |      0.6393 | \n",
      "   20 | 01m39s |  -28.85449 |             0.6116 |          0.1214 |     21.0428 |             19.8555 |             0.0019 |           0.0026 |       0.0037 |      0.8752 | \n",
      "   21 | 01m40s |  -29.35848 |             0.8304 |          0.1254 |     37.0940 |             24.6912 |             0.0014 |           0.0000 |       0.0010 |      0.6006 | \n",
      "   22 | 01m41s |  -30.10289 |             0.9054 |          0.1742 |     24.1090 |             19.5265 |             0.0034 |           0.0005 |       0.0052 |      0.7396 | \n",
      "   23 | 01m49s |  -28.99129 |             0.9321 |          0.0903 |     26.7401 |             16.3092 |             0.0024 |           0.0030 |       0.0033 |      0.5628 | \n",
      "   24 | 01m40s |  -29.46385 |             0.9021 |          0.1455 |     20.4532 |             24.5845 |             0.0030 |           0.0007 |       0.0163 |      0.9240 | \n",
      "   25 | 01m23s | \u001b[35m -28.74084\u001b[0m | \u001b[32m            0.9586\u001b[0m | \u001b[32m         0.0995\u001b[0m | \u001b[32m    13.6127\u001b[0m | \u001b[32m            19.1086\u001b[0m | \u001b[32m            0.0041\u001b[0m | \u001b[32m          0.0024\u001b[0m | \u001b[32m      0.0028\u001b[0m | \u001b[32m     0.9659\u001b[0m | \n",
      "   26 | 01m47s |  -28.95104 |             0.6553 |          0.1148 |     30.6065 |             27.9700 |             0.0045 |           0.0016 |       0.0144 |      0.9325 | \n",
      "   27 | 02m11s |  -28.97898 |             0.8849 |          0.0854 |     26.1042 |             16.8731 |             0.0018 |           0.0048 |       0.0085 |      0.9498 | \n",
      "   28 | 02m00s |  -29.31586 |             0.8298 |          0.1199 |     43.4320 |             22.7592 |             0.0045 |           0.0021 |       0.0037 |      0.7979 | \n",
      "   29 | 01m22s |  -29.72844 |             0.9348 |          0.1684 |     18.2780 |             25.2428 |             0.0011 |           0.0017 |       0.0029 |      0.8422 | \n",
      "   30 | 02m01s |  -30.60461 |             0.8078 |          0.1807 |     38.2619 |             16.6560 |             0.0019 |           0.0035 |       0.0103 |      0.6706 | \n",
      "   31 | 01m42s |  -29.87523 |             0.8604 |          0.1651 |     21.3120 |             17.2848 |             0.0007 |           0.0002 |       0.0185 |      0.6912 | \n",
      "   32 | 02m10s |  -29.05420 |             0.7682 |          0.1026 |     30.9650 |             16.8535 |             0.0034 |           0.0037 |       0.0000 |      0.9944 | \n",
      "   33 | 02m08s |  -30.69601 |             0.8667 |          0.1983 |     46.1167 |             19.9258 |             0.0007 |           0.0029 |       0.0028 |      0.8469 | \n",
      "   34 | 02m14s |  -29.33093 |             0.9740 |          0.1266 |     42.4831 |             18.9715 |             0.0027 |           0.0005 |       0.0072 |      0.8317 | \n",
      "   35 | 01m25s |  -29.52940 |             0.8525 |          0.1404 |     22.3831 |             26.5469 |             0.0041 |           0.0001 |       0.0192 |      0.6950 | \n",
      "   36 | 01m07s |  -29.81287 |             0.9131 |          0.1716 |     12.1974 |             17.0081 |             0.0042 |           0.0029 |       0.0007 |      0.9096 | \n",
      "   37 | 02m04s |  -30.09012 |             0.9255 |          0.1620 |     30.6315 |             17.7488 |             0.0010 |           0.0001 |       0.0124 |      0.8710 | \n",
      "   38 | 01m51s |  -29.68947 |             0.8608 |          0.1666 |     25.3376 |             17.9720 |             0.0027 |           0.0013 |       0.0067 |      0.8176 | \n",
      "   39 | 02m13s |  -28.74800 |             0.7076 |          0.1057 |     44.4434 |             15.8678 |             0.0030 |           0.0025 |       0.0119 |      0.8692 | \n",
      "   40 | 00m45s |  -29.49149 |             0.8257 |          0.1716 |     10.9435 |             17.2822 |             0.0049 |           0.0018 |       0.0200 |      0.7400 | \n",
      "   41 | 01m42s |  -28.99370 |             0.8915 |          0.0890 |     43.2424 |             27.8354 |             0.0027 |           0.0026 |       0.0085 |      0.6263 | \n",
      "   42 | 01m34s |  -29.34975 |             0.6031 |          0.1533 |     23.4245 |             22.1061 |             0.0034 |           0.0012 |       0.0191 |      0.8629 | \n",
      "   43 | 01m31s |  -29.86491 |             0.8365 |          0.1311 |     37.1300 |             26.2841 |             0.0029 |           0.0015 |       0.0031 |      0.5255 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   44 | 01m39s |  -28.98230 |             0.7921 |          0.1081 |     25.0538 |             19.4725 |             0.0047 |           0.0035 |       0.0043 |      0.6442 | \n",
      "   45 | 00m44s |  -29.36821 |             0.8430 |          0.1459 |     12.7252 |             23.4023 |             0.0015 |           0.0036 |       0.0009 |      0.5585 | \n",
      "   46 | 02m06s |  -29.86613 |             0.8070 |          0.1492 |     39.5888 |             19.2136 |             0.0018 |           0.0033 |       0.0012 |      0.9285 | \n",
      "   47 | 00m45s |  -29.33804 |             0.9268 |          0.1458 |     12.6378 |             25.5054 |             0.0028 |           0.0005 |       0.0103 |      0.6133 | \n",
      "   48 | 02m09s | \u001b[35m -28.70211\u001b[0m | \u001b[32m            0.6909\u001b[0m | \u001b[32m         0.0986\u001b[0m | \u001b[32m    49.8906\u001b[0m | \u001b[32m            18.3368\u001b[0m | \u001b[32m            0.0014\u001b[0m | \u001b[32m          0.0048\u001b[0m | \u001b[32m      0.0023\u001b[0m | \u001b[32m     0.8961\u001b[0m | \n",
      "   49 | 01m57s |  -29.72739 |             0.7199 |          0.1559 |     42.4657 |             20.4128 |             0.0007 |           0.0050 |       0.0174 |      0.7257 | \n",
      "   50 | 01m40s | \u001b[35m -28.69685\u001b[0m | \u001b[32m            0.8139\u001b[0m | \u001b[32m         0.0931\u001b[0m | \u001b[32m    26.3029\u001b[0m | \u001b[32m            27.1407\u001b[0m | \u001b[32m            0.0044\u001b[0m | \u001b[32m          0.0005\u001b[0m | \u001b[32m      0.0171\u001b[0m | \u001b[32m     0.7785\u001b[0m | \n",
      "   51 | 01m48s |  -28.96002 |             0.8711 |          0.0938 |     35.4703 |             21.6266 |             0.0016 |           0.0041 |       0.0089 |      0.5968 | \n",
      "   52 | 01m53s |  -29.49812 |             0.7399 |          0.1395 |     31.8903 |             19.5232 |             0.0021 |           0.0018 |       0.0192 |      0.7930 | \n",
      "   53 | 01m39s |  -29.31423 |             0.8317 |          0.1253 |     19.7922 |             17.8023 |             0.0006 |           0.0017 |       0.0189 |      0.7918 | \n",
      "   54 | 00m43s |  -30.76911 |             0.9940 |          0.1975 |     12.3974 |             20.4807 |             0.0028 |           0.0011 |       0.0003 |      0.5139 | \n",
      "   55 | 01m19s |  -29.89489 |             0.7825 |          0.1656 |     25.0841 |             24.5737 |             0.0039 |           0.0000 |       0.0165 |      0.5580 | \n",
      "   56 | 01m45s |  -28.94468 |             0.8351 |          0.0874 |     33.7979 |             26.3114 |             0.0044 |           0.0044 |       0.0122 |      0.7637 | \n",
      "   57 | 01m29s |  -30.77621 |             0.7866 |          0.1880 |     31.4298 |             20.6998 |             0.0013 |           0.0029 |       0.0156 |      0.5046 | \n",
      "   58 | 01m46s |  -31.04145 |             0.8260 |          0.1937 |     34.1555 |             23.2086 |             0.0044 |           0.0044 |       0.0089 |      0.6712 | \n",
      "   59 | 02m03s |  -29.75333 |             0.6716 |          0.1795 |     24.9349 |             15.3856 |             0.0010 |           0.0042 |       0.0142 |      0.9979 | \n",
      "   60 | 01m54s |  -29.13378 |             0.9588 |          0.0985 |     43.8247 |             24.3870 |             0.0036 |           0.0021 |       0.0022 |      0.7553 | \n",
      "   61 | 01m02s |  -29.56269 |             0.8760 |          0.1584 |     12.5042 |             19.6345 |             0.0020 |           0.0032 |       0.0098 |      0.9075 | \n",
      "   62 | 01m23s |  -30.67019 |             0.7988 |          0.1768 |     20.5022 |             17.7142 |             0.0017 |           0.0022 |       0.0055 |      0.5971 | \n",
      "   63 | 01m12s |  -28.71205 |             0.8635 |          0.0921 |     16.0663 |             22.2031 |             0.0036 |           0.0049 |       0.0063 |      0.6500 | \n",
      "   64 | 01m40s |  -28.97012 |             0.9489 |          0.0921 |     31.1574 |             28.1766 |             0.0025 |           0.0035 |       0.0090 |      0.7075 | \n",
      "   65 | 02m00s |  -28.89050 |             0.9041 |          0.1079 |     38.8654 |             20.5769 |             0.0037 |           0.0024 |       0.0037 |      0.8080 | \n",
      "   66 | 01m45s |  -28.96675 |             0.7512 |          0.1390 |     28.8324 |             23.8981 |             0.0007 |           0.0008 |       0.0118 |      0.8737 | \n",
      "   67 | 01m32s |  -29.12904 |             0.9129 |          0.1235 |     22.7591 |             20.8021 |             0.0034 |           0.0012 |       0.0093 |      0.6491 | \n",
      "   68 | 01m16s |  -29.14223 |             0.8946 |          0.0839 |     24.1888 |             29.5168 |             0.0008 |           0.0017 |       0.0081 |      0.5255 | \n",
      "   69 | 02m00s | \u001b[35m -28.69469\u001b[0m | \u001b[32m            0.8241\u001b[0m | \u001b[32m         0.1041\u001b[0m | \u001b[32m    27.3447\u001b[0m | \u001b[32m            16.1454\u001b[0m | \u001b[32m            0.0029\u001b[0m | \u001b[32m          0.0027\u001b[0m | \u001b[32m      0.0155\u001b[0m | \u001b[32m     0.8143\u001b[0m | \n",
      "   70 | 01m35s |  -28.84861 |             0.7571 |          0.1030 |     27.3617 |             22.2885 |             0.0006 |           0.0041 |       0.0115 |      0.6296 | \n",
      "   71 | 01m31s |  -29.51150 |             0.6867 |          0.1276 |     46.3349 |             28.2661 |             0.0033 |           0.0042 |       0.0118 |      0.5485 | \n",
      "   72 | 01m02s |  -28.70380 |             0.8525 |          0.1055 |     14.8109 |             28.8123 |             0.0037 |           0.0014 |       0.0131 |      0.8116 | \n",
      "   73 | 02m01s |  -28.75789 |             0.6644 |          0.1043 |     45.3763 |             17.2625 |             0.0048 |           0.0002 |       0.0086 |      0.7500 | \n",
      "   74 | 02m02s |  -30.36036 |             0.6098 |          0.1904 |     36.3103 |             18.6054 |             0.0042 |           0.0007 |       0.0012 |      0.9859 | \n",
      "   75 | 02m07s |  -29.49087 |             0.6743 |          0.1383 |     46.9285 |             15.4779 |             0.0015 |           0.0036 |       0.0149 |      0.7496 | \n",
      "   76 | 02m15s |  -28.98823 |             0.7992 |          0.1065 |     43.3133 |             16.1110 |             0.0047 |           0.0040 |       0.0181 |      0.9896 | \n",
      "   77 | 01m27s |  -29.23111 |             0.9073 |          0.0933 |     32.1608 |             29.7719 |             0.0024 |           0.0015 |       0.0116 |      0.5416 | \n",
      "   78 | 01m45s |  -29.36448 |             0.9928 |          0.0824 |     21.8325 |             18.4457 |             0.0032 |           0.0041 |       0.0051 |      0.7185 | \n",
      "   79 | 02m16s |  -29.07460 |             0.8273 |          0.1074 |     46.2009 |             17.3272 |             0.0006 |           0.0003 |       0.0081 |      0.9776 | \n",
      "   80 | 01m48s |  -29.31770 |             0.6983 |          0.0818 |     30.1128 |             21.0027 |             0.0040 |           0.0011 |       0.0111 |      0.7931 | \n",
      "   81 | 01m42s |  -29.13082 |             0.6226 |          0.0889 |     29.5645 |             15.5137 |             0.0021 |           0.0036 |       0.0040 |      0.5113 | \n",
      "   82 | 02m02s |  -29.35957 |             0.6473 |          0.1399 |     30.8430 |             15.0328 |             0.0035 |           0.0015 |       0.0140 |      0.8665 | \n",
      "   83 | 01m50s |  -28.80892 |             0.6593 |          0.1160 |     44.3274 |             29.3313 |             0.0019 |           0.0038 |       0.0121 |      0.9293 | \n",
      "   84 | 01m31s |  -29.01849 |             0.6314 |          0.0840 |     23.3705 |             29.8177 |             0.0031 |           0.0016 |       0.0148 |      0.8584 | \n",
      "   85 | 01m14s |  -29.06616 |             0.7866 |          0.0856 |     14.1086 |             21.1354 |             0.0036 |           0.0003 |       0.0063 |      0.8243 | \n",
      "   86 | 02m15s |  -29.43394 |             0.6215 |          0.1528 |     43.3543 |             15.4970 |             0.0016 |           0.0004 |       0.0183 |      0.9721 | \n",
      "   87 | 00m53s |  -29.74969 |             0.9155 |          0.1670 |     11.7254 |             19.3563 |             0.0044 |           0.0034 |       0.0128 |      0.8420 | \n",
      "   88 | 02m02s |  -29.70486 |             0.8495 |          0.1347 |     45.8477 |             15.0116 |             0.0016 |           0.0019 |       0.0192 |      0.5385 | \n",
      "   89 | 00m49s |  -29.07129 |             0.6616 |          0.0873 |     11.7713 |             25.4509 |             0.0006 |           0.0015 |       0.0016 |      0.8220 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   90 | 01m36s |  -28.92105 |             0.9262 |          0.1176 |     17.5505 |             18.9114 |             0.0022 |           0.0049 |       0.0191 |      0.8737 | \n",
      "   91 | 02m05s |  -29.30116 |             0.6298 |          0.0817 |     43.9227 |             17.3082 |             0.0040 |           0.0016 |       0.0179 |      0.9373 | \n",
      "   92 | 01m37s |  -29.88873 |             0.6372 |          0.1776 |     23.2396 |             17.7917 |             0.0029 |           0.0013 |       0.0133 |      0.7529 | \n",
      "   93 | 01m19s | \u001b[35m -28.53989\u001b[0m | \u001b[32m            0.6014\u001b[0m | \u001b[32m         0.0983\u001b[0m | \u001b[32m    24.3895\u001b[0m | \u001b[32m            25.1544\u001b[0m | \u001b[32m            0.0024\u001b[0m | \u001b[32m          0.0044\u001b[0m | \u001b[32m      0.0046\u001b[0m | \u001b[32m     0.5759\u001b[0m | \n",
      "   94 | 00m51s |  -29.06401 |             0.6442 |          0.0898 |     11.9899 |             15.2537 |             0.0043 |           0.0043 |       0.0084 |      0.5784 | \n",
      "   95 | 02m12s |  -28.94863 |             0.9679 |          0.1119 |     46.0504 |             20.9527 |             0.0019 |           0.0008 |       0.0166 |      0.9885 | \n",
      "   96 | 01m58s |  -28.96979 |             0.7926 |          0.1094 |     36.6021 |             17.9261 |             0.0032 |           0.0011 |       0.0183 |      0.7019 | \n",
      "   97 | 02m26s |  -30.57701 |             0.9627 |          0.1922 |     44.9535 |             15.6771 |             0.0048 |           0.0004 |       0.0160 |      0.9103 | \n",
      "   98 | 01m10s |  -28.91474 |             0.8801 |          0.0927 |     13.9022 |             20.6457 |             0.0032 |           0.0031 |       0.0200 |      0.8573 | \n",
      "   99 | 01m08s |  -29.27430 |             0.7671 |          0.1059 |     19.7298 |             22.6613 |             0.0029 |           0.0038 |       0.0187 |      0.5033 | \n",
      "  100 | 01m46s |  -28.72042 |             0.6271 |          0.1042 |     31.6652 |             21.0458 |             0.0026 |           0.0031 |       0.0010 |      0.7868 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n",
      "  101 | 00m59s |  -29.57831 |             1.0000 |          0.0800 |     10.0000 |             30.0000 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': (0.08, 0.2),\n",
    "    'reg_lambda':(0,0.02),\n",
    "    'min_split_gain': (0,0.005),\n",
    "    'max_depth':(10,50),\n",
    "    'colsample_bytree':(0.6,1),\n",
    "    'min_child_samples':(15,30),\n",
    "    'min_child_weight':(0.0005,0.005),\n",
    "    'subsample':(0.5,1)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=100,n_iter=50,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(learning_rate, \n",
    "               reg_lambda,min_split_gain,max_depth,\n",
    "               colsample_bytree,min_child_samples,min_child_weight,subsample):\n",
    "    from sklearn.metrics import median_absolute_error\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(294.54),\n",
    "                              learning_rate= learning_rate,reg_lambda= reg_lambda,\n",
    "                              min_split_gain = min_split_gain, max_depth = int(max_depth),\n",
    "                              colsample_bytree = colsample_bytree,\n",
    "                              min_child_samples = int(min_child_samples),\n",
    "                              min_child_weight = min_child_weight,subsample = subsample,\n",
    "                              n_estimators= int(226.56), reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n",
      "    1 | 01m58s | \u001b[35m -29.53943\u001b[0m | \u001b[32m            0.9356\u001b[0m | \u001b[32m         0.1250\u001b[0m | \u001b[32m    42.3435\u001b[0m | \u001b[32m            17.3298\u001b[0m | \u001b[32m            0.0033\u001b[0m | \u001b[32m          0.0036\u001b[0m | \u001b[32m      0.0135\u001b[0m | \u001b[32m     0.6253\u001b[0m | \n",
      "    2 | 02m04s |  -29.95296 |             0.8311 |          0.1568 |     38.8017 |             16.3328 |             0.0027 |           0.0023 |       0.0033 |      0.8233 | \n",
      "    3 | 01m30s |  -30.49631 |             0.6996 |          0.1940 |     32.6418 |             23.2452 |             0.0022 |           0.0025 |       0.0096 |      0.6406 | \n",
      "    4 | 01m52s | \u001b[35m -29.08578\u001b[0m | \u001b[32m            0.8275\u001b[0m | \u001b[32m         0.0891\u001b[0m | \u001b[32m    36.3681\u001b[0m | \u001b[32m            25.9945\u001b[0m | \u001b[32m            0.0042\u001b[0m | \u001b[32m          0.0015\u001b[0m | \u001b[32m      0.0136\u001b[0m | \u001b[32m     0.9233\u001b[0m | \n",
      "    5 | 01m30s |  -30.61318 |             0.6651 |          0.1732 |     41.4745 |             23.1107 |             0.0045 |           0.0004 |       0.0043 |      0.5450 | \n",
      "    6 | 02m01s |  -30.16985 |             0.7499 |          0.1799 |     41.9683 |             17.4670 |             0.0013 |           0.0026 |       0.0079 |      0.8363 | \n",
      "    7 | 00m50s | \u001b[35m -29.06226\u001b[0m | \u001b[32m            0.9363\u001b[0m | \u001b[32m         0.0866\u001b[0m | \u001b[32m    12.6346\u001b[0m | \u001b[32m            24.5434\u001b[0m | \u001b[32m            0.0013\u001b[0m | \u001b[32m          0.0027\u001b[0m | \u001b[32m      0.0041\u001b[0m | \u001b[32m     0.7096\u001b[0m | \n",
      "    8 | 02m03s |  -30.17945 |             0.7713 |          0.1781 |     43.2717 |             16.8779 |             0.0008 |           0.0032 |       0.0040 |      0.8502 | \n",
      "    9 | 01m04s |  -30.31489 |             0.7669 |          0.1862 |     16.9824 |             19.6596 |             0.0027 |           0.0015 |       0.0069 |      0.6684 | \n",
      "   10 | 00m34s |  -30.32819 |             0.6448 |          0.1667 |     11.0216 |             26.4109 |             0.0008 |           0.0040 |       0.0137 |      0.6155 | \n",
      "   11 | 01m35s |  -29.50982 |             0.8335 |          0.0803 |     33.8375 |             23.3358 |             0.0010 |           0.0018 |       0.0176 |      0.6135 | \n",
      "   12 | 01m40s |  -30.76246 |             0.8395 |          0.1977 |     31.0373 |             15.7278 |             0.0030 |           0.0036 |       0.0090 |      0.5512 | \n",
      "   13 | 01m31s | \u001b[35m -29.05356\u001b[0m | \u001b[32m            0.8018\u001b[0m | \u001b[32m         0.1212\u001b[0m | \u001b[32m    17.6388\u001b[0m | \u001b[32m            16.5435\u001b[0m | \u001b[32m            0.0040\u001b[0m | \u001b[32m          0.0029\u001b[0m | \u001b[32m      0.0164\u001b[0m | \u001b[32m     0.8367\u001b[0m | \n",
      "   14 | 01m23s | \u001b[35m -28.90610\u001b[0m | \u001b[32m            0.9264\u001b[0m | \u001b[32m         0.0914\u001b[0m | \u001b[32m    21.0359\u001b[0m | \u001b[32m            29.8613\u001b[0m | \u001b[32m            0.0032\u001b[0m | \u001b[32m          0.0026\u001b[0m | \u001b[32m      0.0137\u001b[0m | \u001b[32m     0.8001\u001b[0m | \n",
      "   15 | 01m42s |  -29.47039 |             0.9838 |          0.1274 |     42.8848 |             19.8564 |             0.0024 |           0.0015 |       0.0161 |      0.5109 | \n",
      "   16 | 01m11s |  -29.44510 |             0.9526 |          0.0806 |     14.4557 |             24.8236 |             0.0048 |           0.0008 |       0.0052 |      0.8486 | \n",
      "   17 | 01m57s |  -30.28353 |             0.8775 |          0.1684 |     48.8247 |             24.1891 |             0.0039 |           0.0008 |       0.0115 |      0.8728 | \n",
      "   18 | 01m27s |  -31.00421 |             0.9826 |          0.1947 |     46.8444 |             29.4122 |             0.0012 |           0.0016 |       0.0031 |      0.5208 | \n",
      "   19 | 00m50s |  -30.28468 |             0.7960 |          0.1785 |     15.2055 |             27.4165 |             0.0049 |           0.0036 |       0.0190 |      0.6393 | \n",
      "   20 | 01m34s |  -29.00873 |             0.6116 |          0.1214 |     21.0428 |             19.8555 |             0.0019 |           0.0026 |       0.0037 |      0.8752 | \n",
      "   21 | 01m34s |  -29.42075 |             0.8304 |          0.1254 |     37.0940 |             24.6912 |             0.0014 |           0.0000 |       0.0010 |      0.6006 | \n",
      "   22 | 01m35s |  -29.88892 |             0.9054 |          0.1742 |     24.1090 |             19.5265 |             0.0034 |           0.0005 |       0.0052 |      0.7396 | \n",
      "   23 | 01m43s |  -28.97749 |             0.9321 |          0.0903 |     26.7401 |             16.3092 |             0.0024 |           0.0030 |       0.0033 |      0.5628 | \n",
      "   24 | 01m29s |  -29.68363 |             0.9021 |          0.1455 |     20.4532 |             24.5845 |             0.0030 |           0.0007 |       0.0163 |      0.9240 | \n",
      "   25 | 01m14s | \u001b[35m -28.72768\u001b[0m | \u001b[32m            0.9586\u001b[0m | \u001b[32m         0.0995\u001b[0m | \u001b[32m    13.6127\u001b[0m | \u001b[32m            19.1086\u001b[0m | \u001b[32m            0.0041\u001b[0m | \u001b[32m          0.0024\u001b[0m | \u001b[32m      0.0028\u001b[0m | \u001b[32m     0.9659\u001b[0m | \n",
      "   26 | 01m37s |  -28.92453 |             0.6553 |          0.1148 |     30.6065 |             27.9700 |             0.0045 |           0.0016 |       0.0144 |      0.9325 | \n",
      "   27 | 02m00s |  -29.28080 |             0.8849 |          0.0854 |     26.1042 |             16.8731 |             0.0018 |           0.0048 |       0.0085 |      0.9498 | \n",
      "   28 | 01m50s |  -28.78927 |             0.8298 |          0.1199 |     43.4320 |             22.7592 |             0.0045 |           0.0021 |       0.0037 |      0.7979 | \n",
      "   29 | 01m16s |  -29.87184 |             0.9348 |          0.1684 |     18.2780 |             25.2428 |             0.0011 |           0.0017 |       0.0029 |      0.8422 | \n",
      "   30 | 01m53s |  -30.52932 |             0.8078 |          0.1807 |     38.2619 |             16.6560 |             0.0019 |           0.0035 |       0.0103 |      0.6706 | \n",
      "   31 | 01m31s |  -29.84966 |             0.8604 |          0.1651 |     21.3120 |             17.2848 |             0.0007 |           0.0002 |       0.0185 |      0.6912 | \n",
      "   32 | 02m04s |  -29.03862 |             0.7682 |          0.1026 |     30.9650 |             16.8535 |             0.0034 |           0.0037 |       0.0000 |      0.9944 | \n",
      "   33 | 02m01s |  -30.29330 |             0.8667 |          0.1983 |     46.1167 |             19.9258 |             0.0007 |           0.0029 |       0.0028 |      0.8469 | \n",
      "   34 | 02m04s |  -29.04134 |             0.9740 |          0.1266 |     42.4831 |             18.9715 |             0.0027 |           0.0005 |       0.0072 |      0.8317 | \n",
      "   35 | 01m18s |  -29.58355 |             0.8525 |          0.1404 |     22.3831 |             26.5469 |             0.0041 |           0.0001 |       0.0192 |      0.6950 | \n",
      "   36 | 01m01s |  -29.96418 |             0.9131 |          0.1716 |     12.1974 |             17.0081 |             0.0042 |           0.0029 |       0.0007 |      0.9096 | \n",
      "   37 | 01m57s |  -29.58697 |             0.9255 |          0.1620 |     30.6315 |             17.7488 |             0.0010 |           0.0001 |       0.0124 |      0.8710 | \n",
      "   38 | 01m46s |  -29.88960 |             0.8608 |          0.1666 |     25.3376 |             17.9720 |             0.0027 |           0.0013 |       0.0067 |      0.8176 | \n",
      "   39 | 02m05s |  -29.03564 |             0.7076 |          0.1057 |     44.4434 |             15.8678 |             0.0030 |           0.0025 |       0.0119 |      0.8692 | \n",
      "   40 | 00m41s |  -29.87310 |             0.8257 |          0.1716 |     10.9435 |             17.2822 |             0.0049 |           0.0018 |       0.0200 |      0.7400 | \n",
      "   41 | 01m35s |  -29.03307 |             0.8915 |          0.0890 |     43.2424 |             27.8354 |             0.0027 |           0.0026 |       0.0085 |      0.6263 | \n",
      "   42 | 01m28s |  -29.50757 |             0.6031 |          0.1533 |     23.4245 |             22.1061 |             0.0034 |           0.0012 |       0.0191 |      0.8629 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   43 | 01m26s |  -29.79672 |             0.8365 |          0.1311 |     37.1300 |             26.2841 |             0.0029 |           0.0015 |       0.0031 |      0.5255 | \n",
      "   44 | 01m34s |  -28.82613 |             0.7921 |          0.1081 |     25.0538 |             19.4725 |             0.0047 |           0.0035 |       0.0043 |      0.6442 | \n",
      "   45 | 00m40s |  -29.79024 |             0.8430 |          0.1459 |     12.7252 |             23.4023 |             0.0015 |           0.0036 |       0.0009 |      0.5585 | \n",
      "   46 | 02m00s |  -29.69154 |             0.8070 |          0.1492 |     39.5888 |             19.2136 |             0.0018 |           0.0033 |       0.0012 |      0.9285 | \n",
      "   47 | 00m41s |  -29.73565 |             0.9268 |          0.1458 |     12.6378 |             25.5054 |             0.0028 |           0.0005 |       0.0103 |      0.6133 | \n",
      "   48 | 01m59s |  -28.99055 |             0.6909 |          0.0986 |     49.8906 |             18.3368 |             0.0014 |           0.0048 |       0.0023 |      0.8961 | \n",
      "   49 | 01m46s |  -29.66706 |             0.7199 |          0.1559 |     42.4657 |             20.4128 |             0.0007 |           0.0050 |       0.0174 |      0.7257 | \n",
      "   50 | 01m34s |  -28.99033 |             0.8139 |          0.0931 |     26.3029 |             27.1407 |             0.0044 |           0.0005 |       0.0171 |      0.7785 | \n",
      "   51 | 01m41s |  -29.18053 |             0.8711 |          0.0938 |     35.4703 |             21.6266 |             0.0016 |           0.0041 |       0.0089 |      0.5968 | \n",
      "   52 | 01m47s |  -29.28508 |             0.7399 |          0.1395 |     31.8903 |             19.5232 |             0.0021 |           0.0018 |       0.0192 |      0.7930 | \n",
      "   53 | 01m35s |  -29.11469 |             0.8317 |          0.1253 |     19.7922 |             17.8023 |             0.0006 |           0.0017 |       0.0189 |      0.7918 | \n",
      "   54 | 00m41s |  -30.46299 |             0.9940 |          0.1975 |     12.3974 |             20.4807 |             0.0028 |           0.0011 |       0.0003 |      0.5139 | \n",
      "   55 | 01m16s |  -29.89398 |             0.7825 |          0.1656 |     25.0841 |             24.5737 |             0.0039 |           0.0000 |       0.0165 |      0.5580 | \n",
      "   56 | 01m40s |  -29.10235 |             0.8351 |          0.0874 |     33.7979 |             26.3114 |             0.0044 |           0.0044 |       0.0122 |      0.7637 | \n",
      "   57 | 01m25s |  -31.04701 |             0.7866 |          0.1880 |     31.4298 |             20.6998 |             0.0013 |           0.0029 |       0.0156 |      0.5046 | \n",
      "   58 | 01m35s |  -30.62488 |             0.8260 |          0.1937 |     34.1555 |             23.2086 |             0.0044 |           0.0044 |       0.0089 |      0.6712 | \n",
      "   59 | 01m53s |  -29.95044 |             0.6716 |          0.1795 |     24.9349 |             15.3856 |             0.0010 |           0.0042 |       0.0142 |      0.9979 | \n",
      "   60 | 01m48s |  -29.06044 |             0.9588 |          0.0985 |     43.8247 |             24.3870 |             0.0036 |           0.0021 |       0.0022 |      0.7553 | \n",
      "   61 | 00m58s |  -29.60206 |             0.8760 |          0.1584 |     12.5042 |             19.6345 |             0.0020 |           0.0032 |       0.0098 |      0.9075 | \n",
      "   62 | 01m17s |  -30.36202 |             0.7988 |          0.1768 |     20.5022 |             17.7142 |             0.0017 |           0.0022 |       0.0055 |      0.5971 | \n",
      "   63 | 01m08s |  -28.90678 |             0.8635 |          0.0921 |     16.0663 |             22.2031 |             0.0036 |           0.0049 |       0.0063 |      0.6500 | \n",
      "   64 | 01m34s |  -29.10054 |             0.9489 |          0.0921 |     31.1574 |             28.1766 |             0.0025 |           0.0035 |       0.0090 |      0.7075 | \n",
      "   65 | 01m54s |  -28.79095 |             0.9041 |          0.1079 |     38.8654 |             20.5769 |             0.0037 |           0.0024 |       0.0037 |      0.8080 | \n",
      "   66 | 01m40s |  -29.17967 |             0.7512 |          0.1390 |     28.8324 |             23.8981 |             0.0007 |           0.0008 |       0.0118 |      0.8737 | \n",
      "   67 | 01m28s |  -28.99009 |             0.9129 |          0.1235 |     22.7591 |             20.8021 |             0.0034 |           0.0012 |       0.0093 |      0.6491 | \n",
      "   68 | 01m11s |  -29.22558 |             0.8946 |          0.0839 |     24.1888 |             29.5168 |             0.0008 |           0.0017 |       0.0081 |      0.5255 | \n",
      "   69 | 01m53s |  -28.91109 |             0.8241 |          0.1041 |     27.3447 |             16.1454 |             0.0029 |           0.0027 |       0.0155 |      0.8143 | \n",
      "   70 | 01m30s |  -29.20574 |             0.7571 |          0.1030 |     27.3617 |             22.2885 |             0.0006 |           0.0041 |       0.0115 |      0.6296 | \n",
      "   71 | 01m25s |  -29.62106 |             0.6867 |          0.1276 |     46.3349 |             28.2661 |             0.0033 |           0.0042 |       0.0118 |      0.5485 | \n",
      "   72 | 00m57s | \u001b[35m -28.68992\u001b[0m | \u001b[32m            0.8525\u001b[0m | \u001b[32m         0.1055\u001b[0m | \u001b[32m    14.8109\u001b[0m | \u001b[32m            28.8123\u001b[0m | \u001b[32m            0.0037\u001b[0m | \u001b[32m          0.0014\u001b[0m | \u001b[32m      0.0131\u001b[0m | \u001b[32m     0.8116\u001b[0m | \n",
      "   73 | 01m53s |  -28.99666 |             0.6644 |          0.1043 |     45.3763 |             17.2625 |             0.0048 |           0.0002 |       0.0086 |      0.7500 | \n",
      "   74 | 01m54s |  -30.16808 |             0.6098 |          0.1904 |     36.3103 |             18.6054 |             0.0042 |           0.0007 |       0.0012 |      0.9859 | \n",
      "   75 | 02m00s |  -29.38298 |             0.6743 |          0.1383 |     46.9285 |             15.4779 |             0.0015 |           0.0036 |       0.0149 |      0.7496 | \n",
      "   76 | 02m08s |  -28.95741 |             0.7992 |          0.1065 |     43.3133 |             16.1110 |             0.0047 |           0.0040 |       0.0181 |      0.9896 | \n",
      "   77 | 01m25s |  -29.24673 |             0.9073 |          0.0933 |     32.1608 |             29.7719 |             0.0024 |           0.0015 |       0.0116 |      0.5416 | \n",
      "   78 | 01m42s |  -29.44983 |             0.9928 |          0.0824 |     21.8325 |             18.4457 |             0.0032 |           0.0041 |       0.0051 |      0.7185 | \n",
      "   79 | 02m09s |  -28.89470 |             0.8273 |          0.1074 |     46.2009 |             17.3272 |             0.0006 |           0.0003 |       0.0081 |      0.9776 | \n",
      "   80 | 01m42s |  -29.11369 |             0.6983 |          0.0818 |     30.1128 |             21.0027 |             0.0040 |           0.0011 |       0.0111 |      0.7931 | \n",
      "   81 | 01m37s |  -29.05948 |             0.6226 |          0.0889 |     29.5645 |             15.5137 |             0.0021 |           0.0036 |       0.0040 |      0.5113 | \n",
      "   82 | 01m55s |  -29.13724 |             0.6473 |          0.1399 |     30.8430 |             15.0328 |             0.0035 |           0.0015 |       0.0140 |      0.8665 | \n",
      "   83 | 01m43s |  -29.00717 |             0.6593 |          0.1160 |     44.3274 |             29.3313 |             0.0019 |           0.0038 |       0.0121 |      0.9293 | \n",
      "   84 | 01m26s |  -29.13694 |             0.6314 |          0.0840 |     23.3705 |             29.8177 |             0.0031 |           0.0016 |       0.0148 |      0.8584 | \n",
      "   85 | 01m10s |  -29.22391 |             0.7866 |          0.0856 |     14.1086 |             21.1354 |             0.0036 |           0.0003 |       0.0063 |      0.8243 | \n",
      "   86 | 02m07s |  -29.59589 |             0.6215 |          0.1528 |     43.3543 |             15.4970 |             0.0016 |           0.0004 |       0.0183 |      0.9721 | \n",
      "   87 | 00m49s |  -29.69939 |             0.9155 |          0.1670 |     11.7254 |             19.3563 |             0.0044 |           0.0034 |       0.0128 |      0.8420 | \n",
      "   88 | 01m55s |  -29.98836 |             0.8495 |          0.1347 |     45.8477 |             15.0116 |             0.0016 |           0.0019 |       0.0192 |      0.5385 | \n",
      "   89 | 00m46s |  -28.81722 |             0.6616 |          0.0873 |     11.7713 |             25.4509 |             0.0006 |           0.0015 |       0.0016 |      0.8220 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   90 | 01m31s |  -28.79706 |             0.9262 |          0.1176 |     17.5505 |             18.9114 |             0.0022 |           0.0049 |       0.0191 |      0.8737 | \n",
      "   91 | 01m58s |  -29.25875 |             0.6298 |          0.0817 |     43.9227 |             17.3082 |             0.0040 |           0.0016 |       0.0179 |      0.9373 | \n",
      "   92 | 01m31s |  -29.75598 |             0.6372 |          0.1776 |     23.2396 |             17.7917 |             0.0029 |           0.0013 |       0.0133 |      0.7529 | \n",
      "   93 | 01m14s |  -28.82422 |             0.6014 |          0.0983 |     24.3895 |             25.1544 |             0.0024 |           0.0044 |       0.0046 |      0.5759 | \n",
      "   94 | 00m48s |  -29.17533 |             0.6442 |          0.0898 |     11.9899 |             15.2537 |             0.0043 |           0.0043 |       0.0084 |      0.5784 | \n",
      "   95 | 02m05s |  -29.33089 |             0.9679 |          0.1119 |     46.0504 |             20.9527 |             0.0019 |           0.0008 |       0.0166 |      0.9885 | \n",
      "   96 | 01m52s |  -28.96373 |             0.7926 |          0.1094 |     36.6021 |             17.9261 |             0.0032 |           0.0011 |       0.0183 |      0.7019 | \n",
      "   97 | 02m18s |  -30.28474 |             0.9627 |          0.1922 |     44.9535 |             15.6771 |             0.0048 |           0.0004 |       0.0160 |      0.9103 | \n",
      "   98 | 01m06s | \u001b[35m -28.55718\u001b[0m | \u001b[32m            0.8801\u001b[0m | \u001b[32m         0.0927\u001b[0m | \u001b[32m    13.9022\u001b[0m | \u001b[32m            20.6457\u001b[0m | \u001b[32m            0.0032\u001b[0m | \u001b[32m          0.0031\u001b[0m | \u001b[32m      0.0200\u001b[0m | \u001b[32m     0.8573\u001b[0m | \n",
      "   99 | 01m04s |  -28.96789 |             0.7671 |          0.1059 |     19.7298 |             22.6613 |             0.0029 |           0.0038 |       0.0187 |      0.5033 | \n",
      "  100 | 01m39s |  -28.60158 |             0.6271 |          0.1042 |     31.6652 |             21.0458 |             0.0026 |           0.0031 |       0.0010 |      0.7868 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n",
      "  101 | 00m53s |  -29.57831 |             1.0000 |          0.0800 |     10.0000 |             30.0000 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  102 | 01m50s |  -29.65769 |             0.6000 |          0.0800 |     50.0000 |             30.0000 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  103 | 01m54s |  -29.38015 |             0.6000 |          0.0800 |     50.0000 |             15.0000 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  104 | 01m57s |  -29.65089 |             1.0000 |          0.0800 |     39.0152 |             30.0000 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  105 | 00m59s |  -29.66507 |             1.0000 |          0.0800 |     10.0000 |             23.2593 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  106 | 02m25s |  -29.55586 |             1.0000 |          0.0800 |     35.4885 |             15.0000 |             0.0050 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  107 | 01m23s |  -29.40551 |             0.6000 |          0.0800 |     16.9036 |             30.0000 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  108 | 01m11s |  -29.64846 |             1.0000 |          0.0800 |     10.0000 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  109 | 01m52s |  -29.61193 |             1.0000 |          0.0800 |     28.0811 |             30.0000 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  110 | 02m08s |  -29.51414 |             1.0000 |          0.0800 |     19.7855 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  111 | 01m53s |  -29.48550 |             1.0000 |          0.0800 |     15.0040 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  112 | 01m09s |  -29.74351 |             0.6000 |          0.0800 |     13.9126 |             30.0000 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  113 | 02m03s |  -29.39824 |             0.6000 |          0.0800 |     50.0000 |             21.3900 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  114 | 01m59s |  -29.52999 |             0.6000 |          0.0800 |     27.2820 |             19.9012 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  115 | 02m06s |  -29.62674 |             1.0000 |          0.0800 |     40.0956 |             15.0000 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  116 | 01m50s |  -29.42404 |             0.6000 |          0.0800 |     35.3862 |             30.0000 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  117 | 01m29s |  -29.73248 |             0.6000 |          0.0800 |     42.2679 |             30.0000 |             0.0050 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  118 | 02m01s |  -29.53612 |             0.6000 |          0.0800 |     32.8154 |             20.6365 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  119 | 01m58s |  -29.54771 |             0.6000 |          0.0800 |     31.0043 |             21.7715 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  120 | 02m00s |  -29.47446 |             0.6000 |          0.0800 |     30.7734 |             19.6892 |             0.0050 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  121 | 02m24s |  -29.55310 |             1.0000 |          0.0800 |     28.2132 |             15.0000 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  122 | 01m43s |  -29.63187 |             1.0000 |          0.0800 |     50.0000 |             26.1327 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  123 | 01m49s |  -29.54659 |             1.0000 |          0.0800 |     37.2253 |             21.5697 |             0.0005 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  124 | 01m01s |  -29.23834 |             0.6000 |          0.0800 |     10.0000 |             19.2006 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  125 | 02m04s |  -29.60609 |             0.6000 |          0.0800 |     32.4044 |             18.1201 |             0.0050 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  126 | 01m56s |  -29.68653 |             0.6000 |          0.0800 |     45.2930 |             26.2882 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  127 | 01m58s |  -29.77919 |             1.0000 |          0.0800 |     18.6262 |             18.1760 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  128 | 02m02s |  -29.34001 |             0.6000 |          0.0800 |     22.3090 |             15.0000 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  129 | 01m26s |  -29.56050 |             0.6000 |          0.0800 |     28.8010 |             26.1659 |             0.0050 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  130 | 01m40s |  -29.19891 |             1.0000 |          0.0800 |     16.7759 |             23.3345 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  131 | 00m56s |  -29.48237 |             0.6000 |          0.0800 |     10.0000 |             25.1340 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  132 | 01m51s |  -29.38141 |             1.0000 |          0.0800 |     16.2984 |             18.1000 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  133 | 01m25s |  -29.75323 |             0.6000 |          0.0800 |     17.3232 |             15.0000 |             0.0050 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  134 | 01m00s |  -29.67093 |             1.0000 |          0.0800 |     15.3626 |             30.0000 |             0.0050 |           0.0000 |       0.0000 |      0.5000 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  135 | 01m58s |  -29.61118 |             0.6000 |          0.0800 |     28.9270 |             21.4075 |             0.0050 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  136 | 02m14s |  -29.45036 |             1.0000 |          0.0800 |     45.0867 |             22.8028 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  137 | 01m49s |  -29.67421 |             0.6000 |          0.0800 |     30.5751 |             30.0000 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  138 | 01m17s |  -29.50529 |             0.6000 |          0.0800 |     25.7441 |             30.0000 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  139 | 02m02s |  -29.54336 |             0.6000 |          0.0800 |     31.7523 |             20.8936 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  140 | 01m38s |  -29.61644 |             0.6000 |          0.0800 |     32.3066 |             21.6746 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  141 | 01m33s |  -29.41780 |             0.6000 |          0.0800 |     33.0738 |             24.8245 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  142 | 01m43s |  -29.64880 |             0.6000 |          0.0800 |     33.5352 |             19.5608 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  143 | 01m10s |  -29.49025 |             1.0000 |          0.0800 |     19.1961 |             30.0000 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  144 | 02m04s |  -29.60686 |             0.6000 |          0.0800 |     28.6341 |             17.5502 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  145 | 02m08s |  -29.70196 |             1.0000 |          0.0800 |     41.2984 |             26.1799 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  146 | 02m29s |  -29.84027 |             1.0000 |          0.0800 |     38.0684 |             15.0000 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  147 | 01m38s |  -29.54957 |             0.6000 |          0.0800 |     31.3990 |             21.4130 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  148 | 01m47s |  -31.33997 |             1.0000 |          0.2000 |     50.0000 |             23.5535 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  149 | 02m02s |  -29.54915 |             1.0000 |          0.0800 |     31.0939 |             26.4758 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  150 | 02m02s |  -29.59275 |             1.0000 |          0.0800 |     27.6038 |             24.5337 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  151 | 00m59s |  -30.76184 |             0.6000 |          0.2000 |     13.3113 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  152 | 01m34s |  -30.99440 |             1.0000 |          0.2000 |     44.6581 |             30.0000 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  153 | 01m34s |  -29.50477 |             0.6000 |          0.0800 |     25.6402 |             20.9495 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  154 | 02m01s |  -29.55316 |             1.0000 |          0.0800 |     21.3132 |             21.8097 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  155 | 01m19s |  -31.23756 |             1.0000 |          0.2000 |     24.5414 |             26.8427 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  156 | 01m22s |  -29.54353 |             0.6000 |          0.0800 |     22.8064 |             24.5593 |             0.0050 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  157 | 01m46s |  -29.27349 |             0.6000 |          0.0800 |     26.7190 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  158 | 01m48s |  -29.41429 |             0.6000 |          0.0800 |     50.0000 |             19.6762 |             0.0050 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  159 | 01m57s |  -29.50065 |             0.6000 |          0.0800 |     44.3243 |             28.0408 |             0.0050 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  160 | 02m05s |  -30.28880 |             0.6000 |          0.2000 |     47.3831 |             25.9862 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  161 | 01m45s |  -29.72783 |             0.6000 |          0.0800 |     20.2567 |             26.9170 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  162 | 01m51s |  -29.49059 |             0.6000 |          0.0800 |     27.5006 |             27.3455 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  163 | 01m11s |  -30.87163 |             1.0000 |          0.2000 |     22.3451 |             30.0000 |             0.0005 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  164 | 01m40s |  -29.57501 |             0.6000 |          0.0800 |     20.3389 |             30.0000 |             0.0050 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  165 | 01m54s |  -29.58867 |             1.0000 |          0.0800 |     24.5069 |             30.0000 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  166 | 01m39s |  -29.47279 |             1.0000 |          0.0800 |     14.2747 |             20.0759 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  167 | 01m17s |  -29.56292 |             0.6000 |          0.0800 |     18.2143 |             22.0192 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  168 | 02m15s |  -29.61833 |             0.6000 |          0.0800 |     33.2948 |             15.0000 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  169 | 01m28s |  -31.37404 |             1.0000 |          0.2000 |     33.7457 |             30.0000 |             0.0050 |           0.0000 |       0.0000 |      0.5000 | \n",
      "  170 | 01m54s |  -29.48120 |             0.6000 |          0.0800 |     37.0714 |             30.0000 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  171 | 02m08s |  -29.77840 |             1.0000 |          0.0800 |     33.4519 |             24.3113 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  172 | 01m02s |  -29.55944 |             0.6000 |          0.0800 |     13.6558 |             20.1521 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  173 | 01m31s |  -29.69337 |             1.0000 |          0.0800 |     13.4881 |             20.7661 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  174 | 01m02s |  -30.80941 |             1.0000 |          0.2000 |     14.1054 |             20.7133 |             0.0050 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  175 | 00m46s |  -30.67249 |             0.6000 |          0.2000 |     10.0000 |             21.1836 |             0.0050 |           0.0000 |       0.0000 |      0.5000 | \n",
      "  176 | 01m31s |  -29.47178 |             0.6000 |          0.0800 |     14.0335 |             20.5801 |             0.0050 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  177 | 01m04s |  -29.83473 |             1.0000 |          0.0800 |     12.6896 |             18.7711 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  178 | 01m33s |  -29.48998 |             0.6000 |          0.0800 |     15.1096 |             23.1041 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  179 | 02m35s |  -29.71620 |             1.0000 |          0.0800 |     47.8423 |             15.0000 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  180 | 02m08s |  -29.68047 |             0.6000 |          0.0800 |     47.2140 |             22.6285 |             0.0050 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  181 | 00m56s |  -29.54936 |             1.0000 |          0.0800 |     10.0000 |             18.1076 |             0.0050 |           0.0050 |       0.0000 |      0.5000 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  182 | 01m58s |  -29.75296 |             1.0000 |          0.0800 |     18.3259 |             20.3435 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  183 | 02m13s |  -29.78985 |             1.0000 |          0.0800 |     38.8432 |             23.5158 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  184 | 01m53s |  -29.48164 |             0.6000 |          0.0800 |     31.7424 |             18.7727 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  185 | 02m05s |  -29.50105 |             0.6000 |          0.0800 |     29.5009 |             20.0465 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  186 | 01m39s |  -29.47479 |             0.6000 |          0.0800 |     35.0925 |             26.3703 |             0.0005 |           0.0000 |       0.0200 |      0.5000 | \n",
      "  187 | 01m57s |  -29.36732 |             1.0000 |          0.0800 |     40.0828 |             20.7641 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  188 | 02m00s |  -29.39621 |             1.0000 |          0.0800 |     37.7117 |             19.1398 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  189 | 01m50s |  -29.34478 |             1.0000 |          0.0800 |     26.0942 |             19.2945 |             0.0050 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  190 | 02m37s |  -30.29646 |             1.0000 |          0.2000 |     50.0000 |             17.3094 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  191 | 00m54s |  -29.65936 |             0.6000 |          0.0800 |     11.6959 |             24.5121 |             0.0005 |           0.0050 |       0.0000 |      0.5000 | \n",
      "  192 | 01m59s |  -29.51446 |             0.6000 |          0.0800 |     19.3441 |             17.0546 |             0.0005 |           0.0000 |       0.0200 |      1.0000 | \n",
      "  193 | 01m48s |  -29.42249 |             0.6000 |          0.0800 |     21.9677 |             28.1430 |             0.0050 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  194 | 02m02s |  -30.29322 |             0.6000 |          0.2000 |     42.2430 |             27.8711 |             0.0050 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  195 | 01m50s |  -29.86788 |             1.0000 |          0.0800 |     43.2278 |             26.1023 |             0.0005 |           0.0000 |       0.0000 |      0.5000 | \n",
      "  196 | 01m31s |  -29.62177 |             1.0000 |          0.0800 |     14.1103 |             28.3086 |             0.0005 |           0.0000 |       0.0000 |      1.0000 | \n",
      "  197 | 02m13s |  -30.04217 |             0.6000 |          0.2000 |     29.7275 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n",
      "  198 | 01m59s |  -29.68577 |             1.0000 |          0.0800 |     23.5550 |             15.0000 |             0.0005 |           0.0050 |       0.0200 |      0.5000 | \n",
      "  199 | 02m04s |  -29.53949 |             0.6000 |          0.0800 |     25.4757 |             19.6241 |             0.0005 |           0.0050 |       0.0000 |      1.0000 | \n",
      "  200 | 01m30s |  -29.48564 |             0.6000 |          0.0800 |     15.3850 |             28.7142 |             0.0005 |           0.0050 |       0.0200 |      1.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': (0.08, 0.2),\n",
    "    'reg_lambda':(0,0.02),\n",
    "    'min_split_gain': (0,0.005),\n",
    "    'max_depth':(10,50),\n",
    "    'colsample_bytree':(0.6,1),\n",
    "    'min_child_samples':(15,30),\n",
    "    'min_child_weight':(0.0005,0.005),\n",
    "    'subsample':(0.5,1)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=100,n_iter=100,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random initialization stages found a combination of parameters that improved model performance. Let's fit the model to see the performance once again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n",
    "\n",
    "  93 | 01m19s |  -28.53989 |             0.6014 |          0.0983 |     24.3895 |             25.1544 |             0.0024 |           0.0044 |       0.0046 |      0.5759 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  28.874978339393977\n",
      "Completed model fit and predictions in: 1.35 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 294,\n",
    "                              learning_rate= 0.0983,reg_lambda= 0.0046,\n",
    "                              min_split_gain = 0.0044, max_depth = 24,\n",
    "                              colsample_bytree = 0.6014,\n",
    "                              min_child_samples = 25,\n",
    "                              min_child_weight = 0.0024,subsample =  0.5759,\n",
    "                              n_estimators= 226, reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "# Out-of-box performance using validation set\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bechmark model performance on validation set plotted\n",
    "def plot_performance(true, preds, model_name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import median_absolute_error \n",
    "    mae = median_absolute_error(true,np.exp(preds))\n",
    "    plt.scatter(x = np.exp(preds), y = true, c = 'blue', s = 3, alpha = 0.2)\n",
    "    plt.plot([0,1200],[0,1200], \"--k\")\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.ylabel(\"True targets\")\n",
    "    plt.text(0,1100, r\"MAE = %.2f\"%(mae))\n",
    "    plt.title(\"Predictive performance of \"+ model_name + \" model\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a526dd86d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_performance(true= y_val, preds= preds, model_name= \"lgbm1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the impact of increasing n-gram range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree4 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2* (2 ** 18),decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,2), # We will tokenize up to 2-grams\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()) # Scale the features\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 1.3666666666666667 minutes.\n",
      "Completed processing X_val in: 0.31666666666666665 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree4 = pipeline510k_tree4.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree4 = pipeline510k_tree4.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 524290)\n",
      "(15899, 524290)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree4.shape)\n",
    "print(X_val_trans_tree4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model using 300\n",
      "Completed model fit and predictions using 300 in: 0.08333333333333333 minutes.\n",
      "Median Absolute Error:  33.974917755729656\n",
      "**************************************************\n",
      "Training model using 5000\n",
      "Completed model fit and predictions using 5000 in: 1.0833333333333333 minutes.\n",
      "Median Absolute Error:  29.89808183308346\n",
      "**************************************************\n",
      "Training model using 10000\n",
      "Completed model fit and predictions using 10000 in: 1.6833333333333333 minutes.\n",
      "Median Absolute Error:  29.607067567619367\n",
      "**************************************************\n",
      "Training model using 20000\n",
      "Completed model fit and predictions using 20000 in: 2.85 minutes.\n",
      "Median Absolute Error:  29.409841254671164\n",
      "**************************************************\n",
      "Training model using 50000\n",
      "Completed model fit and predictions using 50000 in: 3.6166666666666667 minutes.\n",
      "Median Absolute Error:  29.03743313015282\n",
      "**************************************************\n",
      "Training model using 100000\n",
      "Completed model fit and predictions using 100000 in: 4.3 minutes.\n",
      "Median Absolute Error:  29.09817223553702\n",
      "**************************************************\n",
      "Training model using 200000\n",
      "Completed model fit and predictions using 200000 in: 6.166666666666667 minutes.\n",
      "Median Absolute Error:  29.183854243760806\n",
      "**************************************************\n",
      "Training model using 400000\n",
      "Completed model fit and predictions using 400000 in: 9.483333333333333 minutes.\n",
      "Median Absolute Error:  29.349762511523814\n",
      "**************************************************\n",
      "Training model using 500000\n",
      "Completed model fit and predictions using 500000 in: 10.566666666666666 minutes.\n",
      "Median Absolute Error:  29.367960848202586\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "n_features_list = [300,5000,10000,20000,50000,100000,200000,400000,500000]\n",
    "mae_list = []\n",
    "time_list = []\n",
    "\n",
    "for n_features in n_features_list:\n",
    "    print(\"Training model using \"+ str(n_features))\n",
    "    \n",
    "    # Testing feature selection based on training set\n",
    "    Xt = SelectKBest(f_regression,n_features).fit(X_train_trans_tree4,y_train).transform(X_train_trans_tree4)\n",
    "    Xv = SelectKBest(f_regression,n_features).fit(X_train_trans_tree4,y_train).transform(X_val_trans_tree4)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Fixed model structure \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                              n_estimators=200, reg_alpha= 0.1,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(Xt, np.log(y_train))\n",
    "    preds = lgbm1.predict(Xv)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    mae_list.append(mae)\n",
    "    time_list.append((end-start).seconds/60)\n",
    "\n",
    "    print(\"Completed model fit and predictions using \"+ str(n_features) + \" in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "    print(\"Median Absolute Error: \", str(mae))\n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using up to 2-grams and selecting for 50000 features and test the model performance once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regefrom sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree5 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2* (2 ** 18),decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,2), # We will tokenize up to 2-grams\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression,50000))\n",
    "])\n",
    "\n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree5 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2* (2 ** 18),decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,2), # We will tokenize up to 2-grams\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression,50000))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 1.5 minutes.\n",
      "Completed processing X_val in: 0.35 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree5 = pipeline510k_tree5.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree5 = pipeline510k_tree5.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 50000)\n",
      "(15899, 50000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree5.shape)\n",
    "print(X_val_trans_tree5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  28.977003691917247\n",
      "Completed model fit and predictions in: 2.5 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 294,\n",
    "                              learning_rate= 0.0983,reg_lambda= 0.0046,\n",
    "                              min_split_gain = 0.0044, max_depth = 24,\n",
    "                              colsample_bytree = 0.6014,\n",
    "                              min_child_samples = 25,\n",
    "                              min_child_weight = 0.0024,subsample =  0.5759,\n",
    "                              n_estimators= 226, reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "lgbm1.fit(X_train_trans_tree5, np.log(y_train))\n",
    "preds = lgbm1.predict(X_val_trans_tree5)\n",
    "\n",
    "# Out-of-box performance using validation set\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a547cd24e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_performance(true= y_val, preds= preds, model_name= \"lgbm1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and final model \n",
    "\n",
    "Performance is similar to earlier tuned model. We will re-train and save the tuned model along with the data sets and pipeline 3 we used above. This gives a validation median abosulte error of 28.87 calendar days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  28.874978339393977\n",
      "Completed model fit and predictions in: 1.35 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 294,\n",
    "                              learning_rate= 0.0983,reg_lambda= 0.0046,\n",
    "                              min_split_gain = 0.0044, max_depth = 24,\n",
    "                              colsample_bytree = 0.6014,\n",
    "                              min_child_samples = 25,\n",
    "                              min_child_weight = 0.0024,subsample =  0.5759,\n",
    "                              n_estimators= 226, reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "# Out-of-box performance using validation set\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the pipeline.\n",
      "****************************************\n",
      "Saved the model.\n",
      "****************************************\n",
      "Saved the transformed data sets.\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "# Save the work into a dedicated workspace \"tree\"\n",
    "Rea\n",
    "import pickle\n",
    "import os\n",
    "if not os.path.exists(disk_tree):\n",
    "    os.makedirs(disk_tree)\n",
    "\n",
    "# Save the pipeline\n",
    "with open(disk_tree + \"\\pipeline510k_tree3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipeline510k_tree3,f)\n",
    "\n",
    "print(\"Saved the pipeline.\")\n",
    "print(\"*\" * 40)\n",
    "\n",
    "# Save the model\n",
    "with open(disk_tree + \"/lgbm1_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lgbm1,f)\n",
    "\n",
    "print(\"Saved the model.\")    \n",
    "print(\"*\" * 40)\n",
    "\n",
    "# Save transformed data sets\n",
    "with open(disk_tree + \"\\X_train_trans_tree3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_train_trans_tree3,f)\n",
    "    \n",
    "with open(disk_tree + \"\\X_val_trans_tree3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_val_trans_tree3,f) \n",
    "    \n",
    "print(\"Saved the transformed data sets.\")   \n",
    "print(\"*\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save python 2-friendly pickle model and pipeline\n",
    "\n",
    "Since some systems use python 2 instead of python 3, we need to re-pickle the pipeline and model objects. This is because the protocol python 3 uses is number 3 by default, by python 2 only has protocols 0,1, and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "disk_tree = \"tree\"\n",
    "# Read the pipeline\n",
    "with open(disk_tree + \"\\pipeline510k_tree3.pkl\", \"rb\") as f:\n",
    "    pipeline510k_tree3=pickle.load(f)\n",
    "    \n",
    "# Save python2-friendly version    \n",
    "with open(disk_tree + \"\\pipeline510k_tree3_py2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipeline510k_tree3,f,protocol=1) \n",
    "    \n",
    "# Read the model\n",
    "with open(disk_tree + \"/lgbm1_model.pkl\", \"rb\") as f:\n",
    "    lgbm1=pickle.load(f)\n",
    "    \n",
    "# Save python2-friendly version    \n",
    "with open(disk_tree + \"/lgbm1_model_py2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lgbm1,f,protocol=1)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

---
title: "Exploratory analysis of 510k decision time"
author: "Ozan Aygun"
date: "5/16/2018"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.height= 8, fig.width= 8,message=FALSE,warning=FALSE)
```

#Introduction

# Loading and inspecting data

```{r}
# Download the data from:  #https://www.fda.gov/MedicalDevices/ProductsandMedicalProcedures/DeviceApprovalsandClearances/#510kClearances/ucm089428.htm
wdir <- getwd()

# Data from 96
current_96 <- "http://www.accessdata.fda.gov/premarket/ftparea/pmn96cur.zip"
download.file(url = current_96,destfile = paste0(wdir,"/pmn96cur.zip"))
unzip("pmn96cur.zip")
unlink("pmn96cur.zip")

# Data between 91-95
data91_95 <- "http://www.accessdata.fda.gov/premarket/ftparea/pmn9195.zip"
download.file(url = data91_95,destfile = paste0(wdir,"/pmn9195.zip"))
unzip("pmn9195.zip")
unlink("pmn9195.zip")


# Read the data
data_96_current <- read.csv("pmn96cur.txt", sep = "|", stringsAsFactors = FALSE)
data91_95 <- read.csv("pmn9195.txt", sep = "|", stringsAsFactors = FALSE)

# Same columns
identical(names(data_96_current),names(data91_95))

# rBind the data
current <- rbind(data_96_current,data91_95)
library(lubridate)
current$DATERECEIVED <- mdy(current$DATERECEIVED)
current$DECISIONDATE <- mdy(current$DECISIONDATE)

summary(current)

current$DECISIONTIME <- as.numeric(as.character(current$DECISIONDATE - current$DATERECEIVED))
plot(current$DATERECEIVED,current$DECISIONTIME, pch = 19, cex = 0.05, col = "red" )
```

```{r}
current$YEAR <- year(current$DATERECEIVED)
plot(table(current$YEAR))
```

Let's focus on data between 1997 - 2017. We can use cases from 2018 to test the models later on.

```{r, fig.height=20}
library(lubridate)
current <- current[(current$YEAR >= 1997) & (current$YEAR <= 2017) ,]

summary(current)

# Remove one missing entry
current[is.na(current$YEAR),]
current <- current[!is.na(current$YEAR),]

current$WEEKDAY <- weekdays(current$DATERECEIVED)
current$DAYSOFYEAR <- yday(current$DATERECEIVED)
current$DAYSOFMONTH <- mday(current$DATERECEIVED)
current$DAYSOFWEEK <- wday(current$DATERECEIVED)

summary(current)

library(ggplot2)

ggplot(current, aes(x = DAYSOFYEAR , y = DECISIONTIME, col = factor(DECISION)))+
        geom_point(size = 0.5, alpha = 0.5)+
        facet_grid(YEAR ~ .)+
        scale_fill_discrete()
```

There seems not a relationship between decision date and time of the year a submission is received. As we expected, de novo decisions have different timielines:

```{r}
ggplot(current,aes(x = DECISION, y = DECISIONTIME, fill = DECISION))+
        geom_boxplot()+
        scale_color_discrete()
```

```{r}
table(current$DECISION)
```

We will not use de novo cases in our model since they are not originally intended as 510k submissions but they are converted to 510k.

```{r}
current <- current[current$DECISION != "DENG",]

ggplot(current, aes(x = WEEKDAY, y = DECISIONTIME, fill = WEEKDAY))+
        geom_boxplot()
```

WEEKDAY does not matter. 

```{r}
table(current$EXPEDITEDREVIEW)
ggplot(current, aes(x = EXPEDITEDREVIEW, y = DECISIONTIME))+
        geom_boxplot()
```

It does not seem to matter either.  

```{r}
# There are 2538 product codes in this data set
length(unique(current$PRODUCTCODE))
sum(is.na(current$PRODUCTCODE))

# What is the median decision time for different product codes?
library(dplyr)

pdec <- current %>% group_by(PRODUCTCODE) %>% summarise(med.dec.time = median(DECISIONTIME), count = n()) %>% arrange(desc(med.dec.time))


head(pdec)
tail(pdec)
```
```{r}
hist(log10(pdec$count), breaks = 100, col = "navy")
```

Most product codes are rare, but they may contain important information about the decision time. We would like to keep this feature and eventually convert to dummy variables. However, the caveat is that when we split the data set into training and test sets, then about 700 product codes will only appear in one of the sets. 

```{r}
# How about being ane experienced applicant?
length(unique(current$APPLICANT))

# We notice many applicants have more than one clearances to date, let's see if the prior experience shortens the decision time?

current <- current[order(current$DATERECEIVED),]

for(i in 1:nrow(current)){
 current$APPLICANT_PRIOR_CLEARANCE_TO_DATE[i] <- sum(current$APPLICANT[1:i] == current$APPLICANT[i])     
        
}
```

```{r}
hist(current$APPLICANT_PRIOR_CLEARANCE_TO_DATE, col = "navy")
```


```{r}
library(ggplot2)

ggplot(current,aes(x = APPLICANT_PRIOR_CLEARANCE_TO_DATE, y = DECISIONTIME))+
        geom_point(size = 0.2, col = "navy", alpha = 0.08)+
        geom_smooth(method = "lm")
```

It looks like there is some decreasing trend as we expected.

```{r}
library(dplyr)
results <- current %>% group_by(APPLICANT_PRIOR_CLEARANCE_TO_DATE) %>% summarise(median_decision_time = median(DECISIONTIME)) 

ggplot(current, aes(x = APPLICANT_PRIOR_CLEARANCE_TO_DATE, y = log10(DECISIONTIME)))+
        geom_point(col = "blue", alpha = 0.2, size = 0.02)
```

It was misleading because the skew in the data, it looks like noise instead.

How about folow up instances of the same device, does it make any difference?

```{r}
# Most devices only come once, about 5000 devices have multiple clearances
length(unique(current$DEVICENAME))

for(i in 1:nrow(current)){
 current$DEVICENAME_PRIOR_CLEARANCE_TO_DATE[i] <- sum(current$DEVICENAME[1:i] == current$DEVICENAME[i])     
        
}
```

This one also looks noise:

```{r}
ggplot(current,aes(x = DEVICENAME_PRIOR_CLEARANCE_TO_DATE, y = log10(DECISIONTIME)))+
        geom_point(col = "navy", alpha = 0.3, size = 0.5)
```

We don't expect some features would be useful let's remove them:

```{r}
library(dplyr)
current <- dplyr::select(current, -APPLICANT, - CONTACT, - STREET1,
                         - STREET2,- CITY)
```

```{r}
table(current$STATE)
sum(is.na(current$STATE))
sum(current$STATE == "")
```
```{r}
table(current$COUNTRY_CODE)
```

```{r}
summary(lm(DECISIONTIME ~ factor(COUNTRY_CODE), data = current))
par(mfrow = c(2,2))
plot(lm(DECISIONTIME ~ factor(COUNTRY_CODE), data = current))
```

Let's look at that  outlier data points with over 3000 days of decision time:

```{r}
current[which(current$DECISIONTIME > 3000),]
```

These entries might have something special we could not understand. Almost 9 years of decision time is not very realistic. In order to reduce the chance of analyzing erronous data, we will remove these 3 entries which appear to relate same type of product.

```{r}
current <- current[which(current$DECISIONTIME <3000),]
```


```{r}
# Order the factor levels of COUNTRY_CODE based on median decision time of each group
fac <- with(current, reorder(COUNTRY_CODE, DECISIONTIME, median, order = TRUE))
current$COUNTRY_CODE <- factor(current$COUNTRY_CODE, levels = levels(fac))

ggplot(current, aes(x = factor(COUNTRY_CODE), y = DECISIONTIME))+
        geom_boxplot()+
        coord_flip()
```
Does not look significantly different, but worth to keep in the list of features.

```{r}
# Fill the empty states entries with 'NON_US'
current$STATE[which(current$STATE == "")] <- "NON_US"
table(current$STATE)
```

```{r}
fac <- with(current, reorder(STATE,DECISIONTIME,median,order = TRUE))
current$STATE <- factor(current$STATE, levels = levels(fac))

ggplot(current, aes(x = STATE, y = DECISIONTIME))+
        geom_boxplot()+
        coord_flip()
```

Again, states don't look too different, but we will still keep for binary features.

```{r}
# Remove zipcode features
current <-  dplyr::select(current,- ZIP, POSTAL_CODE )
```

```{r}
current %>% group_by(DECISION) %>% summarise(median_dectime = median(DECISIONTIME),
                                             count = n())

```

We don't need to keep this one since since the exact decision code won't be known before any submission.

```{r}
current <- dplyr::select(current, - DECISION)
```

```{r}
table(current$REVIEWADVISECOMM)
fac <- with(current, reorder(REVIEWADVISECOMM,DECISIONTIME,median,order = TRUE))
current$REVIEWADVISECOMM <- factor(current$REVIEWADVISECOMM, levels = levels(fac))

ggplot(current, aes(x = REVIEWADVISECOMM, y = DECISIONTIME))+
        geom_boxplot()
       
```

Let's keep this feature.

```{r}
ggplot(current, aes(x = STATEORSUMM, y = DECISIONTIME))+
        geom_boxplot()
table(current$STATEORSUMM)       
```

This does not have much value, but keep it for now until we think a strategy to retrieve these statement/summary text.

```{r}
identical(as.character(current$REVIEWADVISECOMM), current$CLASSADVISECOMM)
```
```{r}
table(current$CLASSADVISECOMM)
fac <- with(current, reorder(CLASSADVISECOMM,DECISIONTIME,median,order = TRUE))
current$CLASSADVISECOMM <- factor(current$CLASSADVISECOMM, levels = levels(fac))

ggplot(current, aes(x = CLASSADVISECOMM, y = DECISIONTIME))+
        geom_boxplot()
```

Keep for now.

```{r}
# Remove this feature
unique(current$SSPINDICATOR)
current <- dplyr::select(current,-SSPINDICATOR)
```

```{r}
table(current$TYPE)
fac <- with(current, reorder(TYPE,DECISIONTIME,median,order = TRUE))
current$TYPE <- factor(current$TYPE, levels = levels(fac))

ggplot(current, aes(x = TYPE, y = DECISIONTIME))+
        geom_boxplot()
```
As we expected, special 510k's have shorter review time. This is a good feature to keep.

```{r}
# Remove this one
current <- dplyr::select(current, - THIRDPARTY)
```

```{r}
# Keep this one
table(current$EXPEDITEDREVIEW)
ggplot(current, aes(x = EXPEDITEDREVIEW, y = DECISIONTIME))+
        geom_boxplot()

current$EXPEDITEDREVIEW[current$EXPEDITEDREVIEW == ""] <- "N"
```

We will keep Device names for tokenization and feature extraction using NLP.

This is the version of the data set we will keep for some plotting later.

```{r}
# save the current version of the dataset to continue working later
saveRDS(current,"current.rds")
```

# Processing the data set for predictive modeling

Here we will process the features we currently have, to create a new data set where we will only have:

- Target feature (DECISIONTIME)
- Features to be vectorized into dummy variables:
       - attach the feature name to value , then combine them into a single         feature for a given data entry. Thye will become unique features and will be                binarized using tokenization later.
- Features to be tokenized as n_grams (DEVICENAME)
- Two continuous features (APPLICANT anf DEVICE name prior clearance to date)

We will start by removing all other features we won't use for medeling, expect the KNUMBER, which we will keep to explore decision summary mining later.

```{r}
processing <- dplyr::select(current, KNUMBER,DECISIONTIME,STATE,COUNTRY_CODE,
                            REVIEWADVISECOMM,PRODUCTCODE,STATEORSUMM,
                            CLASSADVISECOMM,TYPE,EXPEDITEDREVIEW,DEVICENAME,
                            APPLICANT_PRIOR_CLEARANCE_TO_DATE,
                            DEVICENAME_PRIOR_CLEARANCE_TO_DATE)

# Save the current version of the processing 
saveRDS(processing,"processing.rds")
```

```{r}
###################################################
# Write a function to process the data as we need:
###################################################

binary_processer <- function(processing){

require(dplyr)

temp <- processing

binary_features <- c("STATE","COUNTRY_CODE",
                            "REVIEWADVISECOMM","PRODUCTCODE","STATEORSUMM",
                            "CLASSADVISECOMM","TYPE","EXPEDITEDREVIEW")
# Attach the binary feature names to values, remove any "_" 
for (i in 1:length(binary_features)){
        feature_index <- which(names(temp) == binary_features[i])
        temp[,feature_index] <-  sapply(temp[,feature_index], function(x){
                return(gsub(pattern = "_",replacement = "",x =paste0(binary_features[i],x)))
        })
}

# For each data point, Combine binary features into a single string feature using a 
# single white space for binary vectorization/tokenization later

temp <- within(temp, binarized_features <- tolower(paste(STATE,COUNTRY_CODE,
                            REVIEWADVISECOMM,PRODUCTCODE,STATEORSUMM,
                            CLASSADVISECOMM,TYPE,EXPEDITEDREVIEW, sep = " ")))
# Remove the combined features
temp <- dplyr::select(temp,-c(STATE,COUNTRY_CODE,
                            REVIEWADVISECOMM,PRODUCTCODE,STATEORSUMM,
                            CLASSADVISECOMM,TYPE,EXPEDITEDREVIEW))

return(temp)

}

```

```{r}
# Process the data set using binary processer and save rds and csv versions

processing <- binary_processer(processing)
saveRDS(processing,"processing.rds")
write.csv(processing,file = "processing.csv", row.names = FALSE)
```

# Exploring strategies to obtain text features from publicly available summary/statement data

```{r}
# For the entries where STATAMENTORSUMM == "Summary"" The main pdf directory seems to be at:

pdfmain <- "https://www.accessdata.fda.gov/cdrh_docs/pdf/"

# For example:
K_number <- "K970011"

pdfurl <- paste0(pdfmain,K_number,".pdf") 

require(pdftools)

# Example to see the pdf_reading function works
txt <- pdf_text("https://arxiv.org/pdf/1406.4806.pdf")

# Note that indeed this function can access to 8 pages of the document, but since
# PDF was not saved in a text searchable format, it can not read the content
pdf_510k <- pdf_text(pdfurl)
pdf_510k

# The pdf is an image object, for this cases we can try to use tesseract
# package for optical text recognition

require(tesseract)
txt <- ocr(image = pdfurl)


# tesserract package can not use pdfs directly, so we need to convert them
# to images beforehand

# This step cpnverts each pdf page into a png file and stores created image file names
# Note that setting dpi (resolution) to a good value is critical to extract useful
# information by using text recognition
images <- pdf_convert(pdf = pdfurl, format = "png", dpi = 400)

txt <- NULL
for(i in 1:length(images)){
txt[i] <- ocr(image = images[i])
cat("Completed extracting image:",i, "\n")
}
txt <- paste0(txt,sep = " ", collapse = "~")
unlink(images)
# This way we can extract useful text from images iteratively

# Retrieves very useful information with text recognition:
cat(txt)
# Once we are done with a file we can delete them using unlink()

# Another 510k Summary example, which is searchable
K_number <- "K173935"

# Note that for 2017, the mainurl is now different
pdfmain <- "https://www.accessdata.fda.gov/cdrh_docs/pdf17/"

pdfurl <- paste0(pdfmain,K_number,".pdf") 
# Note that wrapping the pdf_url into paste and collapse ~ combines all the pages 
# of the pdf text into a single character string object:
pdf_510k <- paste(pdf_text(pdfurl), sep = " ", collapse = "~")
# head(pdf_510k)

# Note that in this case parsing is sucessfull and it is very fast. 

```

Therefore, we conclude that:

 - For the devices where summary statement is available, this can be reached online.
 - Only certain (perhaps more recent) pdfs are searchable.
 - The base url might differ based on the year of the 510k
 
 Based on these observations, we will devise a strategy to programmatically parse these text features and use in NLP.
 
# PDF web scraping for text feature extraction

Our strategy should be parsing the text into a single string for each 510k, and eventually storing in a 2 column data frame, first column is K_number to link the data later on, the secon column is the text. 

This data will be very hard to store in the memory, therefore, we will continuously write into hard drive as we parse the data.

Since we don't know which available PDF files will be readable

```{r}

```


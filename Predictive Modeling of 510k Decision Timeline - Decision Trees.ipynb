{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision-tree based algorithms for prediction\n",
    "\n",
    "In this part of the work, we will try to train decision-tree based algorithms to make predicitions about the timeline.\n",
    "\n",
    "### Building feature extraction pipeline\n",
    "\n",
    "Tree-based algorithms would require feature selection, so we will need to use a reasonable number of features. We will start by including 300 best features using our tokenization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the work into a dedicated workspace \"ridge\"\n",
    "disk_tree = \"tree\"\n",
    "import os\n",
    "if not os.path.exists(disk_tree):\n",
    "    os.makedirs(disk_tree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree1 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression, 300))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load Training and Validation sets\n",
    "disk = \"D:\\Data_science\\GitHub\\Predictive-Modeling-510k-decision-time\"\n",
    "# Validation set \n",
    "with open(disk+\"\\X_val.pkl\",\"rb\") as f:\n",
    "    X_val=pickle.load(f)\n",
    "\n",
    "with open(disk+\"\\y_val.pkl\",\"rb\") as f:\n",
    "    y_val=pickle.load(f)\n",
    "    \n",
    "# Training set (Locked down)\n",
    "with open(disk+\"\\X_train.pkl\",\"rb\") as f:\n",
    "    X_train=pickle.load(f)\n",
    "\n",
    "with open(disk+\"\\y_train.pkl\",\"rb\") as f:\n",
    "    y_train=pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.7833333333333333 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree = pipeline510k_tree1.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree = pipeline510k_tree1.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 300)\n",
      "(15899, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree.shape)\n",
    "print(X_val_trans_tree.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regresion\n",
    "\n",
    "We will first train an untuned Gradient Boosting algorithm to see performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7446            2.62m\n",
      "         2           0.7138            2.48m\n",
      "         3           0.6883            2.46m\n",
      "         4           0.6669            2.41m\n",
      "         5           0.6498            2.41m\n",
      "         6           0.6349            2.42m\n",
      "         7           0.6224            2.42m\n",
      "         8           0.6123            2.42m\n",
      "         9           0.6037            2.40m\n",
      "        10           0.5963            2.40m\n",
      "        20           0.5585            2.29m\n",
      "        30           0.5420            2.15m\n",
      "        40           0.5296            2.01m\n",
      "        50           0.5196            1.89m\n",
      "        60           0.5103            1.76m\n",
      "        70           0.5023            1.63m\n",
      "        80           0.4953            1.48m\n",
      "        90           0.4888            1.35m\n",
      "       100           0.4827            1.21m\n",
      "       200           0.4350            0.00s\n",
      "Median Absolute Error:  37.452053685765335\n",
      "Completed model fit and predictions in: 2.25 minutes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "gbm1 = GradientBoostingRegressor(verbose = 1, n_estimators= 200, max_depth=5)\n",
    "\n",
    "gbm1.fit(X_train_trans_tree, np.log(y_train))\n",
    "preds = gbm1.predict(X_val_trans_tree)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed: 49.8min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 75.3min\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed: 115.8min\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 125.9min\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed: 170.8min\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed: 183.3min\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed: 204.8min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 217.8min\n",
      "[Parallel(n_jobs=6)]: Done 108 out of 108 | elapsed: 250.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 108 out of 108 | elapsed: 250.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed GridSearch in: 259.8333333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,500,1000],\n",
    "    'max_depth':[5,10,15],\n",
    "    'learning_rate': [0.1,0.25,0.75,1]\n",
    "}\n",
    "\n",
    "# We have 8 CPU cores, we will use 6 for this task\n",
    "gbmSearch1 = GridSearchCV(estimator= GradientBoostingRegressor(), \n",
    "                            param_grid= param_grid,\n",
    "                            n_jobs = 6,\n",
    "                            cv = 3,\n",
    "                            verbose = 10, scoring= 'neg_median_absolute_error'\n",
    "                            )\n",
    "gbmSearch1.fit(X_train_trans_tree, np.log(y_train))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed GridSearch in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=10, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbmSearch1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.38059952933966"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "# Let's see the validation MAE (in original target scale) from the best estimator\n",
    "preds = gbmSearch1.best_estimator_.predict(X_val_trans_tree)\n",
    "median_absolute_error(y_val,np.exp(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting\n",
    "\n",
    "Let's try to use relatively new ligtGBM package to see the performance. The package has a sklearn interface we will use to perform hyperparameter optimization as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  36.951519001311084\n",
      "Completed model fit and predictions in: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression')\n",
    "\n",
    "lgbm1.fit(X_train_trans_tree, np.log(y_train))\n",
    "preds = lgbm1.predict(X_val_trans_tree)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that lightGBM is indeed extremely faster and performed better than even the tuned traditional GBM. We will experiment this algorithm and attempt hyperparameter optimization. Let's define a new pipeline without feature selection to also experiment the impact of adding more features on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree2 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()) # Scale the features\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.75 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree2 = pipeline510k_tree2.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree2 = pipeline510k_tree2.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 262146)\n",
      "(15899, 262146)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree2.shape)\n",
    "print(X_val_trans_tree2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  29.467624324930185\n",
      "Completed model fit and predictions in: 1.3666666666666667 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "Xt = SelectKBest(f_regression,10000).fit(X_train_trans_tree2,y_train).transform(X_train_trans_tree2)\n",
    "Xv = SelectKBest(f_regression,10000).fit(X_train_trans_tree2,y_train).transform(X_val_trans_tree2)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                          n_estimators=200, reg_alpha= 0.1,\n",
    "                          boosting_type= 'dart')\n",
    "\n",
    "lgbm1.fit(Xt, np.log(y_train))\n",
    "preds = lgbm1.predict(Xv)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lgbm is robust to overfitting when adding more features. Let's try to first search for the number of features to be included in the fixed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model using 300\n",
      "Completed model fit and predictions using 300 in: 0.1 minutes.\n",
      "Median Absolute Error:  33.107162652136324\n",
      "**************************************************\n",
      "Training model using 5000\n",
      "Completed model fit and predictions using 5000 in: 0.8833333333333333 minutes.\n",
      "Median Absolute Error:  29.633671997998164\n",
      "**************************************************\n",
      "Training model using 10000\n",
      "Completed model fit and predictions using 10000 in: 1.4 minutes.\n",
      "Median Absolute Error:  29.467624324930185\n",
      "**************************************************\n",
      "Training model using 20000\n",
      "Completed model fit and predictions using 20000 in: 1.6666666666666667 minutes.\n",
      "Median Absolute Error:  29.30898855444397\n",
      "**************************************************\n",
      "Training model using 50000\n",
      "Completed model fit and predictions using 50000 in: 1.9 minutes.\n",
      "Median Absolute Error:  29.209825132888795\n",
      "**************************************************\n",
      "Training model using 100000\n",
      "Completed model fit and predictions using 100000 in: 2.0166666666666666 minutes.\n",
      "Median Absolute Error:  29.166814835270287\n",
      "**************************************************\n",
      "Training model using 200000\n",
      "Completed model fit and predictions using 200000 in: 2.066666666666667 minutes.\n",
      "Median Absolute Error:  29.19324890053349\n",
      "**************************************************\n",
      "Training model using 262146\n",
      "Completed model fit and predictions using 262146 in: 2.066666666666667 minutes.\n",
      "Median Absolute Error:  29.478673965840073\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "n_features_list = [300,5000,10000,20000,50000,100000,200000,262146]\n",
    "mae_list = []\n",
    "time_list = []\n",
    "\n",
    "for n_features in n_features_list:\n",
    "    print(\"Training model using \"+ str(n_features))\n",
    "    \n",
    "    # Testing feature selection based on training set\n",
    "    Xt = SelectKBest(f_regression,n_features).fit(X_train_trans_tree2,y_train).transform(X_train_trans_tree2)\n",
    "    Xv = SelectKBest(f_regression,n_features).fit(X_train_trans_tree2,y_train).transform(X_val_trans_tree2)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Fixed model structure \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                              n_estimators=200, reg_alpha= 0.1,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(Xt, np.log(y_train))\n",
    "    preds = lgbm1.predict(Xv)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    mae_list.append(mae)\n",
    "    time_list.append((end-start).seconds/60)\n",
    "\n",
    "    print(\"Completed model fit and predictions using \"+ str(n_features) + \" in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "    print(\"Median Absolute Error: \", str(mae))\n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like model starts to overfit beyond 100000 training features. We will define the new pipeline, transform the data sets before further hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM Feature extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree3 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression, 100000))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.8 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree3 = pipeline510k_tree3.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree3 = pipeline510k_tree3.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 100000)\n",
      "(15899, 100000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree3.shape)\n",
    "print(X_val_trans_tree3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 192 candidates, totalling 1152 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed: 40.0min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 45.6min\n",
      "[Parallel(n_jobs=6)]: Done 101 tasks      | elapsed: 53.6min\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed: 67.4min\n",
      "[Parallel(n_jobs=6)]: Done 133 tasks      | elapsed: 77.6min\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed: 88.1min\n",
      "[Parallel(n_jobs=6)]: Done 169 tasks      | elapsed: 109.2min\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed: 119.2min\n",
      "[Parallel(n_jobs=6)]: Done 209 tasks      | elapsed: 134.5min\n",
      "[Parallel(n_jobs=6)]: Done 230 tasks      | elapsed: 147.4min\n",
      "[Parallel(n_jobs=6)]: Done 253 tasks      | elapsed: 166.2min\n",
      "[Parallel(n_jobs=6)]: Done 276 tasks      | elapsed: 183.2min\n",
      "[Parallel(n_jobs=6)]: Done 301 tasks      | elapsed: 207.8min\n",
      "[Parallel(n_jobs=6)]: Done 326 tasks      | elapsed: 230.0min\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed: 261.4min\n",
      "[Parallel(n_jobs=6)]: Done 380 tasks      | elapsed: 287.1min\n",
      "[Parallel(n_jobs=6)]: Done 409 tasks      | elapsed: 316.0min\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed: 337.7min\n",
      "[Parallel(n_jobs=6)]: Done 469 tasks      | elapsed: 372.9min\n",
      "[Parallel(n_jobs=6)]: Done 500 tasks      | elapsed: 416.7min\n",
      "[Parallel(n_jobs=6)]: Done 533 tasks      | elapsed: 453.6min\n",
      "[Parallel(n_jobs=6)]: Done 566 tasks      | elapsed: 506.1min\n",
      "[Parallel(n_jobs=6)]: Done 601 tasks      | elapsed: 547.0min\n",
      "[Parallel(n_jobs=6)]: Done 636 tasks      | elapsed: 583.7min\n",
      "[Parallel(n_jobs=6)]: Done 673 tasks      | elapsed: 636.1min\n",
      "[Parallel(n_jobs=6)]: Done 710 tasks      | elapsed: 696.0min\n",
      "[Parallel(n_jobs=6)]: Done 749 tasks      | elapsed: 774.8min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed: 840.9min\n",
      "[Parallel(n_jobs=6)]: Done 829 tasks      | elapsed: 926.1min\n",
      "[Parallel(n_jobs=6)]: Done 870 tasks      | elapsed: 1009.0min\n",
      "[Parallel(n_jobs=6)]: Done 913 tasks      | elapsed: 1143.1min\n",
      "[Parallel(n_jobs=6)]: Done 956 tasks      | elapsed: 1269.8min\n",
      "[Parallel(n_jobs=6)]: Done 1001 tasks      | elapsed: 1381.4min\n",
      "[Parallel(n_jobs=6)]: Done 1046 tasks      | elapsed: 1511.5min\n",
      "[Parallel(n_jobs=6)]: Done 1093 tasks      | elapsed: 1682.7min\n",
      "[Parallel(n_jobs=6)]: Done 1140 tasks      | elapsed: 1884.7min\n",
      "[Parallel(n_jobs=6)]: Done 1152 out of 1152 | elapsed: 1904.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed GridSearch in: 467.56666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,150,200,250,400,500],\n",
    "    'num_leaves':[150,200,300,400],\n",
    "    'reg_alpha': [0,0.01,0.05,0.1,0.5,1,5,10],\n",
    "    'objective': ['regression'],\n",
    "    'boosting_type': ['dart']\n",
    "}\n",
    "\n",
    "# We have 8 CPU cores, we will use 6 for this task\n",
    "lgbmSearch1 = GridSearchCV(estimator= lgb.LGBMRegressor(), \n",
    "                            param_grid= param_grid,\n",
    "                            n_jobs = 6,\n",
    "                            cv = 6,\n",
    "                            verbose = 10, scoring= 'neg_median_absolute_error'\n",
    "                            )\n",
    "\n",
    "lgbmSearch1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed GridSearch in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=500,\n",
       "       n_jobs=-1, num_leaves=150, objective='regression',\n",
       "       random_state=None, reg_alpha=0.1, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbmSearch1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.38894595686144295"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbmSearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.504873585145397"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "# Let's see the validation MAE (in original target scale) from the best estimator\n",
    "preds = lgbmSearch1.best_estimator_.predict(X_val_trans_tree3)\n",
    "median_absolute_error(y_val,np.exp(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the 4 parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(num_leaves,n_estimators,reg_alpha):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(num_leaves),\n",
    "                              n_estimators= int(n_estimators), reg_alpha= reg_alpha,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 01m16s | \u001b[35m -35.12445\u001b[0m | \u001b[32m      161.3516\u001b[0m | \u001b[32m    206.2586\u001b[0m | \u001b[32m     0.0548\u001b[0m | \n",
      "    2 | 01m58s | \u001b[35m -31.10728\u001b[0m | \u001b[32m      266.5394\u001b[0m | \u001b[32m    246.0457\u001b[0m | \u001b[32m     0.8177\u001b[0m | \n",
      "    3 | 02m06s | \u001b[35m -30.57605\u001b[0m | \u001b[32m      274.9119\u001b[0m | \u001b[32m    292.5025\u001b[0m | \u001b[32m     0.8854\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    4 | 01m47s | \u001b[35m -30.11682\u001b[0m | \u001b[32m      300.0000\u001b[0m | \u001b[32m    150.0000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "    5 | 03m14s | \u001b[35m -29.74980\u001b[0m | \u001b[32m      300.0000\u001b[0m | \u001b[32m    300.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    6 | 02m16s |  -30.23047 |       300.0000 |     300.0000 |      1.0000 | \n",
      "    7 | 01m38s |  -30.90307 |       150.0000 |     300.0000 |      0.0000 | \n",
      "    8 | 01m05s |  -31.16249 |       150.0000 |     150.0000 |      0.0000 | \n",
      "    9 | 02m34s | \u001b[35m -29.10945\u001b[0m | \u001b[32m      231.1680\u001b[0m | \u001b[32m    300.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "   10 | 02m44s |  -29.77941 |       300.0000 |     227.7304 |      0.0000 | \n",
      "   11 | 02m40s |  -29.13963 |       245.7091 |     300.0000 |      0.0000 | \n",
      "   12 | 02m35s |  -29.20123 |       237.4534 |     300.0000 |      0.0000 | \n",
      "   13 | 01m36s |  -29.24059 |       231.9763 |     150.0000 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (150, 300),\n",
    "    'n_estimators' : (150,300),\n",
    "    'reg_alpha': (0,1)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=3,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian optimization is promising to find parameter that may improve the performance. Let's try another round of optimization cycle by focusing on a narrower hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m51s | \u001b[35m -29.02788\u001b[0m | \u001b[32m      244.9824\u001b[0m | \u001b[32m    331.2563\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    2 | 02m36s |  -29.04927 |       221.6442 |     360.4335 |      0.0098 | \n",
      "    3 | 03m05s |  -29.39139 |       244.5303 |     394.5018 |      0.0034 | \n",
      "    4 | 02m32s |  -29.36064 |       246.5605 |     298.3245 |      0.0009 | \n",
      "    5 | 02m53s |  -29.93163 |       241.6704 |     375.4623 |      0.0039 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    6 | 02m18s |  -29.10648 |       220.0000 |     290.0000 |      0.0000 | \n",
      "    7 | 02m52s |  -29.40106 |       220.0000 |     400.0000 |      0.0000 | \n",
      "    8 | 02m28s | \u001b[35m -29.02445\u001b[0m | \u001b[32m      220.0000\u001b[0m | \u001b[32m    334.1820\u001b[0m | \u001b[32m     0.0100\u001b[0m | \n",
      "    9 | 03m13s |  -29.80390 |       250.0000 |     400.0000 |      0.0100 | \n",
      "   10 | 02m44s |  -29.44771 |       250.0000 |     290.0000 |      0.0000 | \n",
      "   11 | 02m55s |  -29.49083 |       250.0000 |     318.5344 |      0.0000 | \n",
      "   12 | 02m42s |  -29.07049 |       220.0000 |     348.3952 |      0.0000 | \n",
      "   13 | 02m26s | \u001b[35m -28.82805\u001b[0m | \u001b[32m      220.0000\u001b[0m | \u001b[32m    313.9619\u001b[0m | \u001b[32m     0.0100\u001b[0m | \n",
      "   14 | 02m48s |  -29.04563 |       220.0000 |     375.1665 |      0.0000 | \n",
      "   15 | 02m30s |  -29.25785 |       220.0000 |     319.4859 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (290, 400),\n",
    "    'n_estimators' : (220,250),\n",
    "    'reg_alpha': (0,0.01)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to find parameters that further improved performance. Let's try to further narrow down the hyperparameter space and perform another round of Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m29s | \u001b[35m -29.10137\u001b[0m | \u001b[32m      224.9824\u001b[0m | \u001b[32m    312.5034\u001b[0m | \u001b[32m     0.0001\u001b[0m | \n",
      "    2 | 02m17s |  -29.23025 |       201.6442 |     328.4183 |      0.0245 | \n",
      "    3 | 02m34s |  -29.46624 |       224.5303 |     347.0010 |      0.0086 | \n",
      "    4 | 02m21s | \u001b[35m -28.79092\u001b[0m | \u001b[32m      226.5605\u001b[0m | \u001b[32m    294.5406\u001b[0m | \u001b[32m     0.0024\u001b[0m | \n",
      "    5 | 02m30s |  -29.39501 |       221.6704 |     336.6158 |      0.0099 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    6 | 02m05s |  -29.11512 |       200.0636 |     290.0450 |      0.0122 | \n",
      "    7 | 02m30s |  -28.87265 |       230.0000 |     290.0000 |      0.0000 | \n",
      "    8 | 02m22s |  -29.21013 |       200.0000 |     350.0000 |      0.0250 | \n",
      "    9 | 02m20s |  -29.08370 |       220.0908 |     292.8533 |      0.0250 | \n",
      "   10 | 02m28s |  -29.09202 |       230.0000 |     300.9593 |      0.0250 | \n",
      "   11 | 02m50s |  -29.08896 |       230.0000 |     350.0000 |      0.0000 | \n",
      "   12 | 02m14s |  -29.23849 |       200.0000 |     311.5937 |      0.0000 | \n",
      "   13 | 02m28s |  -29.01811 |       225.4564 |     296.3131 |      0.0010 | \n",
      "   14 | 02m35s |  -29.08392 |       230.0000 |     324.0520 |      0.0250 | \n",
      "   15 | 02m19s |  -29.30205 |       224.3218 |     290.0000 |      0.0250 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (290, 350),\n",
    "    'n_estimators' : (200,230),\n",
    "    'reg_alpha': (0,0.025)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization stage give us another lead for optimization. We will perform another round of optimization cycle using these 3 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m20s | \u001b[35m -28.92014\u001b[0m | \u001b[32m      224.9824\u001b[0m | \u001b[32m    279.3764\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    2 | 02m01s |  -29.27668 |       201.6442 |     286.0076 |      0.0049 | \n",
      "    3 | 02m20s |  -29.15800 |       224.5303 |     293.7504 |      0.0017 | \n",
      "    4 | 02m15s |  -29.11658 |       226.5605 |     271.8919 |      0.0005 | \n",
      "    5 | 02m16s |  -29.14482 |       221.6704 |     289.4232 |      0.0020 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    6 | 02m03s |  -28.96436 |       200.0000 |     270.0000 |      0.0000 | \n",
      "    7 | 02m21s |  -29.26499 |       230.0000 |     283.7620 |      0.0050 | \n",
      "    8 | 02m11s |  -28.95894 |       200.0000 |     295.0000 |      0.0000 | \n",
      "    9 | 02m18s |  -29.04792 |       217.1388 |     276.4416 |      0.0000 | \n",
      "   10 | 02m15s |  -29.25602 |       221.5866 |     280.0949 |      0.0050 | \n",
      "   11 | 02m18s | \u001b[35m -28.83104\u001b[0m | \u001b[32m      214.6859\u001b[0m | \u001b[32m    295.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "   12 | 02m12s |  -29.01606 |       213.2324 |     270.0000 |      0.0000 | \n",
      "   13 | 02m25s |  -28.97788 |       230.0000 |     276.3328 |      0.0000 | \n",
      "   14 | 02m24s |  -29.31890 |       218.5193 |     295.0000 |      0.0000 | \n",
      "   15 | 02m29s |  -29.03612 |       230.0000 |     295.0000 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (270, 295),\n",
    "    'n_estimators' : (200,230),\n",
    "    'reg_alpha': (0,0.005)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we are converging to some optimal values with these parameters. Now let's try to assign the optimal values for these 3 parameters and see if we can tune the learning rate to further improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(learning_rate):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(294.54),\n",
    "                              learning_rate= learning_rate,\n",
    "                              n_estimators= int(226.56), reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   learning_rate | \n",
      "    1 | 02m20s | \u001b[35m -29.10472\u001b[0m | \u001b[32m         0.0938\u001b[0m | \n",
      "    2 | 02m24s |  -29.20536 |          0.0964 | \n",
      "    3 | 02m27s | \u001b[35m -28.94624\u001b[0m | \u001b[32m         0.0995\u001b[0m | \n",
      "    4 | 02m23s |  -29.10463 |          0.0908 | \n",
      "    5 | 02m24s |  -29.15096 |          0.0978 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   learning_rate | \n",
      "    6 | 02m23s |  -29.25657 |          0.1000 | \n",
      "    7 | 02m27s | \u001b[35m -28.81794\u001b[0m | \u001b[32m         0.0900\u001b[0m | \n",
      "    8 | 02m24s |  -28.95310 |          0.0989 | \n",
      "    9 | 02m24s |  -28.84389 |          0.0983 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "   10 | 02m21s |  -29.08047 |          0.0922 | \n",
      "   11 | 02m21s |  -29.17043 |          0.0951 | \n",
      "   12 | 02m21s |  -29.41761 |          0.0905 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "   13 | 02m22s |  -29.06929 |          0.0914 | \n",
      "   14 | 02m22s |  -29.40204 |          0.0930 | \n",
      "   15 | 02m22s |  -29.21425 |          0.0944 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': (0.09, 0.15)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try a slightly wider learning rate space along with other parameters to see if we can get some lead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(learning_rate, \n",
    "               reg_lambda,min_split_gain,max_depth,\n",
    "               colsample_bytree,min_child_samples,min_child_weight,subsample):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(294.54),\n",
    "                              learning_rate= learning_rate,reg_lambda= reg_lambda,\n",
    "                              min_split_gain = min_split_gain, max_depth = max_depth,\n",
    "                              colsample_bytree = colsample_bytree,\n",
    "                              min_child_samples = min_child_samples,\n",
    "                              min_child_weight = min_child_weight,subsample = subsample,\n",
    "                              n_estimators= int(226.56), reg_alpha= 0.0024,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_samples |   min_child_weight |   min_split_gain |   reg_lambda |   subsample | \n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "b'Parameter min_data_in_leaf should be of type int, got \"17.329789109857913\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-c0620cd3a04f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# The algorithm will randomly choose 3 points to establish a 'prior', then will perform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# 10 interations to maximize the value of estimator function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mlgbmBO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ucb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgp_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0my_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(self, init_points)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Evaluate target function at all initialization points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_observe_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# Add the points from `self.initialize` to the observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36m_observe_point\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_observe_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mobserve_point\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;31m# measure the target function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-fb75b57cf5e6>\u001b[0m in \u001b[0;36mlgbm_score\u001b[1;34m(learning_rate, reg_lambda, min_split_gain, max_depth, colsample_bytree, min_child_samples, min_child_weight, subsample)\u001b[0m\n\u001b[0;32m     20\u001b[0m                               boosting_type= 'dart')\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mlgbm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_trans_tree3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgbm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_trans_tree3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    612\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m                                        callbacks=callbacks)\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    467\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[0;32m   1301\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[1;32m-> 1303\u001b[1;33m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1304\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m                 ctypes.byref(self.handle)))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    854\u001b[0m                                 \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                                 \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m    857\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 ctypes.byref(self.handle)))\n\u001b[0;32m    705\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_csr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init_from_csr\u001b[1;34m(self, csr, params_str, ref_dataset)\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m             \u001b[0mref_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m             ctypes.byref(self.handle)))\n\u001b[0m\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init_from_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \"\"\"\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: b'Parameter min_data_in_leaf should be of type int, got \"17.329789109857913\"'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': (0.08, 0.2),\n",
    "    'reg_lambda':(0,0.02),\n",
    "    'min_split_gain': (0,0.005),\n",
    "    'max_depth':(10,50),\n",
    "    'colsample_bytree':(0.6,1),\n",
    "    'min_child_samples':(15,30),\n",
    "    'min_child_weight':(0.0005,0.005),\n",
    "    'subsample':(0.5,1)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=100,n_iter=50,acq='ucb', kappa= 3, **gp_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision-tree based algorithms for prediction\n",
    "\n",
    "In this part of the work, we will try to train decision-tree based algorithms to make predicitions about the timeline.\n",
    "\n",
    "### Building feature extraction pipeline\n",
    "\n",
    "Tree-based algorithms would require feature selection, so we will need to use a reasonable number of features. We will start by including 300 best features using our tokenization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the work into a dedicated workspace \"ridge\"\n",
    "disk_tree = \"tree\"\n",
    "import os\n",
    "if not os.path.exists(disk_tree):\n",
    "    os.makedirs(disk_tree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree1 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression, 300))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load Training and Validation sets\n",
    "disk = \"D:\\Data_science\\GitHub\\Predictive-Modeling-510k-decision-time\"\n",
    "# Validation set \n",
    "with open(disk+\"\\X_val.pkl\",\"rb\") as f:\n",
    "    X_val=pickle.load(f)\n",
    "\n",
    "with open(disk+\"\\y_val.pkl\",\"rb\") as f:\n",
    "    y_val=pickle.load(f)\n",
    "    \n",
    "# Training set (Locked down)\n",
    "with open(disk+\"\\X_train.pkl\",\"rb\") as f:\n",
    "    X_train=pickle.load(f)\n",
    "\n",
    "with open(disk+\"\\y_train.pkl\",\"rb\") as f:\n",
    "    y_train=pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.7833333333333333 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree = pipeline510k_tree1.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree = pipeline510k_tree1.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 300)\n",
      "(15899, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree.shape)\n",
    "print(X_val_trans_tree.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regresion\n",
    "\n",
    "We will first train an untuned Gradient Boosting algorithm to see performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7446            2.62m\n",
      "         2           0.7138            2.48m\n",
      "         3           0.6883            2.46m\n",
      "         4           0.6669            2.41m\n",
      "         5           0.6498            2.41m\n",
      "         6           0.6349            2.42m\n",
      "         7           0.6224            2.42m\n",
      "         8           0.6123            2.42m\n",
      "         9           0.6037            2.40m\n",
      "        10           0.5963            2.40m\n",
      "        20           0.5585            2.29m\n",
      "        30           0.5420            2.15m\n",
      "        40           0.5296            2.01m\n",
      "        50           0.5196            1.89m\n",
      "        60           0.5103            1.76m\n",
      "        70           0.5023            1.63m\n",
      "        80           0.4953            1.48m\n",
      "        90           0.4888            1.35m\n",
      "       100           0.4827            1.21m\n",
      "       200           0.4350            0.00s\n",
      "Median Absolute Error:  37.452053685765335\n",
      "Completed model fit and predictions in: 2.25 minutes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "gbm1 = GradientBoostingRegressor(verbose = 1, n_estimators= 200, max_depth=5)\n",
    "\n",
    "gbm1.fit(X_train_trans_tree, np.log(y_train))\n",
    "preds = gbm1.predict(X_val_trans_tree)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed: 49.8min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 75.3min\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed: 115.8min\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 125.9min\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed: 170.8min\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed: 183.3min\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed: 204.8min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 217.8min\n",
      "[Parallel(n_jobs=6)]: Done 108 out of 108 | elapsed: 250.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 108 out of 108 | elapsed: 250.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed GridSearch in: 259.8333333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,500,1000],\n",
    "    'max_depth':[5,10,15],\n",
    "    'learning_rate': [0.1,0.25,0.75,1]\n",
    "}\n",
    "\n",
    "# We have 8 CPU cores, we will use 6 for this task\n",
    "gbmSearch1 = GridSearchCV(estimator= GradientBoostingRegressor(), \n",
    "                            param_grid= param_grid,\n",
    "                            n_jobs = 6,\n",
    "                            cv = 3,\n",
    "                            verbose = 10, scoring= 'neg_median_absolute_error'\n",
    "                            )\n",
    "gbmSearch1.fit(X_train_trans_tree, np.log(y_train))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed GridSearch in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=10, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbmSearch1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.38059952933966"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "# Let's see the validation MAE (in original target scale) from the best estimator\n",
    "preds = gbmSearch1.best_estimator_.predict(X_val_trans_tree)\n",
    "median_absolute_error(y_val,np.exp(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting\n",
    "\n",
    "Let's try to use relatively new ligtGBM package to see the performance. The package has a sklearn interface we will use to perform hyperparameter optimization as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  36.951519001311084\n",
      "Completed model fit and predictions in: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression')\n",
    "\n",
    "lgbm1.fit(X_train_trans_tree, np.log(y_train))\n",
    "preds = lgbm1.predict(X_val_trans_tree)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that lightGBM is indeed extremely faster and performed better than even the tuned traditional GBM. We will experiment this algorithm and attempt hyperparameter optimization. Let's define a new pipeline without feature selection to also experiment the impact of adding more features on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree2 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()) # Scale the features\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.75 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree2 = pipeline510k_tree2.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree2 = pipeline510k_tree2.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 262146)\n",
      "(15899, 262146)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree2.shape)\n",
    "print(X_val_trans_tree2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  29.467624324930185\n",
      "Completed model fit and predictions in: 1.3666666666666667 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "Xt = SelectKBest(f_regression,10000).fit(X_train_trans_tree2,y_train).transform(X_train_trans_tree2)\n",
    "Xv = SelectKBest(f_regression,10000).fit(X_train_trans_tree2,y_train).transform(X_val_trans_tree2)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                          n_estimators=200, reg_alpha= 0.1,\n",
    "                          boosting_type= 'dart')\n",
    "\n",
    "lgbm1.fit(Xt, np.log(y_train))\n",
    "preds = lgbm1.predict(Xv)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lgbm is robust to overfitting when adding more features. Let's try to first search for the number of features to be included in the fixed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model using 300\n",
      "Completed model fit and predictions using 300 in: 0.1 minutes.\n",
      "Median Absolute Error:  33.107162652136324\n",
      "**************************************************\n",
      "Training model using 5000\n"
     ]
    }
   ],
   "source": [
    "n_features_list = [300,5000,10000,20000,50000,100000,200000,262146]\n",
    "mae_list = []\n",
    "time_list = []\n",
    "\n",
    "for n_features in n_features_list:\n",
    "    print(\"Training model using \"+ str(n_features))\n",
    "    \n",
    "    # Testing feature selection based on training set\n",
    "    Xt = SelectKBest(f_regression,n_features).fit(X_train_trans_tree2,y_train).transform(X_train_trans_tree2)\n",
    "    Xv = SelectKBest(f_regression,n_features).fit(X_train_trans_tree2,y_train).transform(X_val_trans_tree2)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Fixed model structure \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                              n_estimators=200, reg_alpha= 0.1,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(Xt, np.log(y_train))\n",
    "    preds = lgbm1.predict(Xv)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    mae_list.append(mae)\n",
    "    time_list.append((end-start).seconds/60)\n",
    "\n",
    "    print(\"Completed model fit and predictions using \"+ str(n_features) + \" in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "    print(\"Median Absolute Error: \", str(mae))\n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

---
title: "Exploratory analysis of 510k decision time"
author: "Ozan Aygun"
date: "5/16/2018"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.height= 8, fig.width= 8,message=FALSE,warning=FALSE)
```

#Introduction

# Loading and inspecting data

```{r}
# Download the data from:  #https://www.fda.gov/MedicalDevices/ProductsandMedicalProcedures/DeviceApprovalsandClearances/#510kClearances/ucm089428.htm
wdir <- getwd()

# Data from 96
current_96 <- "http://www.accessdata.fda.gov/premarket/ftparea/pmn96cur.zip"
download.file(url = current_96,destfile = paste0(wdir,"/pmn96cur.zip"))
unzip("pmn96cur.zip")
unlink("pmn96cur.zip")

# Data between 91-95
data91_95 <- "http://www.accessdata.fda.gov/premarket/ftparea/pmn9195.zip"
download.file(url = data91_95,destfile = paste0(wdir,"/pmn9195.zip"))
unzip("pmn9195.zip")
unlink("pmn9195.zip")


# Read the data
data_96_current <- read.csv("pmn96cur.txt", sep = "|", stringsAsFactors = FALSE)
data91_95 <- read.csv("pmn9195.txt", sep = "|", stringsAsFactors = FALSE)

# Same columns
identical(names(data_96_current),names(data91_95))

# rBind the data
current <- rbind(data_96_current,data91_95)
library(lubridate)
current$DATERECEIVED <- mdy(current$DATERECEIVED)
current$DECISIONDATE <- mdy(current$DECISIONDATE)

summary(current)

current$DECISIONTIME <- as.numeric(as.character(current$DECISIONDATE - current$DATERECEIVED))
plot(current$DATERECEIVED,current$DECISIONTIME, pch = 19, cex = 0.05, col = "red" )
```

```{r}
current$YEAR <- year(current$DATERECEIVED)
plot(table(current$YEAR))
```

Let's focus on data between 1997 - 2017. We can use cases from 2018 to test the models later on.

```{r, fig.height=20}
library(lubridate)
current <- current[(current$YEAR >= 1997) & (current$YEAR <= 2017) ,]

summary(current)

# Remove one missing entry
current[is.na(current$YEAR),]
current <- current[!is.na(current$YEAR),]

current$WEEKDAY <- weekdays(current$DATERECEIVED)
current$DAYSOFYEAR <- yday(current$DATERECEIVED)
current$DAYSOFMONTH <- mday(current$DATERECEIVED)
current$DAYSOFWEEK <- wday(current$DATERECEIVED)

summary(current)

library(ggplot2)

ggplot(current, aes(x = DAYSOFYEAR , y = DECISIONTIME, col = factor(DECISION)))+
        geom_point(size = 0.5, alpha = 0.5)+
        facet_grid(YEAR ~ .)+
        scale_fill_discrete()
```

There seems not a relationship between decision date and time of the year a submission is received. As we expected, de novo decisions have different timielines:

```{r}
ggplot(current,aes(x = DECISION, y = DECISIONTIME, fill = DECISION))+
        geom_boxplot()+
        scale_color_discrete()
```

```{r}
table(current$DECISION)
```

We will not use de novo cases in our model since they are not originally intended as 510k submissions but they are converted to 510k.

```{r}
current <- current[current$DECISION != "DENG",]

ggplot(current, aes(x = WEEKDAY, y = DECISIONTIME, fill = WEEKDAY))+
        geom_boxplot()
```

WEEKDAY does not matter. 

```{r}
table(current$EXPEDITEDREVIEW)
ggplot(current, aes(x = EXPEDITEDREVIEW, y = DECISIONTIME))+
        geom_boxplot()
```

It does not seem to matter either.  

```{r}
# There are 2538 product codes in this data set
length(unique(current$PRODUCTCODE))
sum(is.na(current$PRODUCTCODE))

# What is the median decision time for different product codes?
library(dplyr)

pdec <- current %>% group_by(PRODUCTCODE) %>% summarise(med.dec.time = median(DECISIONTIME), count = n()) %>% arrange(desc(med.dec.time))


head(pdec)
tail(pdec)
```
```{r}
hist(log10(pdec$count), breaks = 100, col = "navy")
```

Most product codes are rare, but they may contain important information about the decision time. We would like to keep this feature and eventually convert to dummy variables. However, the caveat is that when we split the data set into training and test sets, then about 700 product codes will only appear in one of the sets. 

```{r}
# How about being ane experienced applicant?
length(unique(current$APPLICANT))

# We notice many applicants have more than one clearances to date, let's see if the prior experience shortens the decision time?

current <- current[order(current$DATERECEIVED),]

for(i in 1:nrow(current)){
 current$APPLICANT_PRIOR_CLEARANCE_TO_DATE[i] <- sum(current$APPLICANT[1:i] == current$APPLICANT[i])     
        
}
```

```{r}
hist(current$APPLICANT_PRIOR_CLEARANCE_TO_DATE, col = "navy")
```


```{r}
library(ggplot2)

ggplot(current,aes(x = APPLICANT_PRIOR_CLEARANCE_TO_DATE, y = DECISIONTIME))+
        geom_point(size = 0.2, col = "navy", alpha = 0.08)+
        geom_smooth(method = "lm")
```

It looks like there is some decreasing trend as we expected.

```{r}
library(dplyr)
results <- current %>% group_by(APPLICANT_PRIOR_CLEARANCE_TO_DATE) %>% summarise(median_decision_time = median(DECISIONTIME)) 

ggplot(current, aes(x = APPLICANT_PRIOR_CLEARANCE_TO_DATE, y = log10(DECISIONTIME)))+
        geom_point(col = "blue", alpha = 0.2, size = 0.02)
```

It was misleading because the skew in the data, it looks like noise instead.

How about folow up instances of the same device, does it make any difference?

```{r}
# Most devices only come once, about 5000 devices have multiple clearances
length(unique(current$DEVICENAME))

for(i in 1:nrow(current)){
 current$DEVICENAME_PRIOR_CLEARANCE_TO_DATE[i] <- sum(current$DEVICENAME[1:i] == current$DEVICENAME[i])     
        
}
```

This one also looks noise:

```{r}
ggplot(current,aes(x = DEVICENAME_PRIOR_CLEARANCE_TO_DATE, y = log10(DECISIONTIME)))+
        geom_point(col = "navy", alpha = 0.3, size = 0.5)
```

We don't expect some features would be useful let's remove them:

```{r}
library(dplyr)
current <- dplyr::select(current, -APPLICANT, - CONTACT, - STREET1,
                         - STREET2,- CITY)
```

```{r}
table(current$STATE)
sum(is.na(current$STATE))
sum(current$STATE == "")
```
```{r}
table(current$COUNTRY_CODE)
```

```{r}
summary(lm(DECISIONTIME ~ factor(COUNTRY_CODE), data = current))
par(mfrow = c(2,2))
plot(lm(DECISIONTIME ~ factor(COUNTRY_CODE), data = current))
```

Let's look at that  outlier data points with over 3000 days of decision time:

```{r}
current[which(current$DECISIONTIME > 3000),]
```

These entries might have something special we could not understand. Almost 9 years of decision time is not very realistic. In order to reduce the chance of anayzing erronous data, we will remove these 3 entries which appear to relate same type of product.

```{r}
ggplot(current, aes(x = factor(COUNTRY_CODE), y = DECISIONTIME))+
        geom_boxplot()+
        coord_flip()
```

```{r}
# save the current version of the dataset to continue working later
saveRDS(current,"current.rds")
```


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "After partitioining the validation data set and processing with the pipeline, we will now implement our streaming data strategy to both train some online regressors using incremental learning, and process the accumulating training data simultaneously. This way we will also understand benchmark performance of incremental learning regressors. This is intended to link R processing pipeline that is extracting training data real-time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Note that this code will reset the cache\n",
    "############################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Cache KNUMBERs to be skipped: initially these are documents in the validation set\n",
    "disk = \"/Volumes/Iomega_HDD/2016/Data science/Predictive-Modeling-510k-decision-time/\"\n",
    "with open(disk+\"X_val.pkl\", \"rb\") as f:\n",
    "    X_val = pickle.load(f)\n",
    "skip_knumber = list(X_val.KNUMBER)\n",
    "\n",
    "with open(disk+\"y_val.pkl\", \"rb\") as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "# Pickle these Knumbers to use and dynamically update later\n",
    "with open(disk+\"skip_knumber.pkl\", \"wb\") as f:\n",
    "    pickle.dump(skip_knumber,f)\n",
    "\n",
    "# Just to obtain column names:\n",
    "current_chunk = pd.read_csv(disk+\"processed510kdata.csv\", nrows=1)\n",
    "column_names = list(current_chunk.columns)\n",
    "\n",
    "# Initiate the skiprows to use and dynamically update later\n",
    "skiprows = 0\n",
    "with open(disk+\"skiprows.pkl\", \"wb\") as f:\n",
    "    pickle.dump(skiprows,f)\n",
    "    \n",
    "# Initiate chunk_number to use and update later\n",
    "chunk_number = 1\n",
    "with open(disk+\"chunk_number.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunk_number,f)\n",
    "    \n",
    "# A data frame to hold validation results to be updated\n",
    "results = pd.DataFrame(columns=['chunk', 'nsamples', \"SGDRegressor\", 'PassiveAggressiveRegressor'])\n",
    "with open(disk+\"processed_chunks/results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results,f) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction pipeline and utility functions\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "# Function to add feature interactions in Sparse matrix\n",
    "# From: https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
    "        self.degree = degree\n",
    "        self.feature_name_separator = feature_name_separator\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not sparse.isspmatrix_csc(X):\n",
    "            X = sparse.csc_matrix(X)\n",
    "\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.orig_col_names = X.columns\n",
    "        else:\n",
    "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
    "\n",
    "        spi = self._create_sparse_interactions(X)\n",
    "        return spi\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "    def _create_sparse_interactions(self, X):\n",
    "        out_mat = []\n",
    "        self.feature_names = self.orig_col_names.tolist()\n",
    "\n",
    "        for sub_degree in range(2, self.degree + 1):\n",
    "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
    "                # add name for new column\n",
    "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
    "                self.feature_names.append(name)\n",
    "\n",
    "                # get column multiplications value\n",
    "                out = X[:, col_ixs[0]]\n",
    "                for j in col_ixs[1:]:\n",
    "                    out = out.multiply(X[:, j])\n",
    "\n",
    "                out_mat.append(out)\n",
    "\n",
    "        return sparse.hstack([X] + out_mat)\n",
    "\n",
    "# Definition of the actual pipeline\n",
    "#  We will try to optimize for these values in feature selection\n",
    "n_tokens = 1000\n",
    "n_features = 600\n",
    "\n",
    "pipeline510k = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1),decode_error='ignore', # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True,alternate_sign=False  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                            #('dim_red1', SelectKBest(f_regression, k = n_tokens)) # Step3: use dimension reduction to select n_tokens of best features\n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    #(\"int\", SparseInteractions(degree=2)), # Add polynomial interaction terms \n",
    "    (\"scaler\",MaxAbsScaler()) # Scale the features\n",
    "    #('dim_red2', SelectKBest(f_regression, k =n_features)) # Add another dimension reduction step at the end\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 0.26666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Process the validation set and save into disk\n",
    "disk = \"/Volumes/Iomega_HDD/2016/Data science/Predictive-Modeling-510k-decision-time/\"\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "start = datetime.now()\n",
    "X_val_proc = pipeline510k.fit_transform(X_val)\n",
    "process = datetime.now() - start\n",
    "print(\"It took: \" + str(process.seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8083, 262146)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_proc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the processed validation set.\n"
     ]
    }
   ],
   "source": [
    "with open(disk+\"X_val_proc.pkl\",\"wb\") as f:\n",
    "        pickle.dump(X_val_proc,f)\n",
    "print(\"Saved the processed validation set.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of regressors\n",
    "from sklearn.linear_model import SGDRegressor,PassiveAggressiveRegressor\n",
    "sgd = SGDRegressor()\n",
    "pagg = PassiveAggressiveRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed validation set.\n",
      "Loaded targets for validation set.\n",
      ">> Started processing chunk: 1\n",
      ">> Loaded chunk: 1\n",
      ">> Filtered chunk: 1\n",
      ">> Pipeline-processed chunk: 1\n",
      ">> Shape of the processed chunk: (7, 262146)\n",
      ">> Saved features and target for chunk: 1\n",
      ">> Completed processing chunk: 1\n",
      "It took: 0.0 minutes to process this chunk.\n",
      " >>> Completed Sgd regressor with a validation rmse of 459.187675774\n",
      " >>> Completed Pagg regressor with a validation rmse of 122.803106328\n",
      "****************************************\n",
      ">> Completed incremental learning for chunk: 1\n",
      "********************************************************************************\n",
      ">> Started processing chunk: 2\n",
      ">> Loaded chunk: 2\n",
      ">> Filtered chunk: 2\n",
      ">> Pipeline-processed chunk: 2\n",
      ">> Shape of the processed chunk: (8, 262146)\n",
      ">> Saved features and target for chunk: 2\n",
      ">> Completed processing chunk: 2\n",
      "It took: 0.0 minutes to process this chunk.\n",
      " >>> Completed Sgd regressor with a validation rmse of 315.535562446\n",
      " >>> Completed Pagg regressor with a validation rmse of 107.325724987\n",
      "****************************************\n",
      ">> Completed incremental learning for chunk: 2\n",
      "********************************************************************************\n",
      ">> Started processing chunk: 3\n",
      ">> Loaded chunk: 3\n",
      ">> Filtered chunk: 3\n",
      ">> Pipeline-processed chunk: 3\n",
      ">> Shape of the processed chunk: (9, 262146)\n",
      ">> Saved features and target for chunk: 3\n",
      ">> Completed processing chunk: 3\n",
      "It took: 0.0 minutes to process this chunk.\n",
      " >>> Completed Sgd regressor with a validation rmse of 164.508815501\n",
      " >>> Completed Pagg regressor with a validation rmse of 111.814229313\n",
      "****************************************\n",
      ">> Completed incremental learning for chunk: 3\n",
      "********************************************************************************\n",
      ">> Started processing chunk: 4\n",
      ">> Loaded chunk: 4\n",
      ">> Filtered chunk: 4\n",
      ">> Pipeline-processed chunk: 4\n",
      ">> Shape of the processed chunk: (6, 262146)\n",
      ">> Saved features and target for chunk: 4\n",
      ">> Completed processing chunk: 4\n",
      "It took: 0.0 minutes to process this chunk.\n",
      " >>> Completed Sgd regressor with a validation rmse of 285.010397983\n",
      " >>> Completed Pagg regressor with a validation rmse of 108.359855065\n",
      "****************************************\n",
      ">> Completed incremental learning for chunk: 4\n",
      "********************************************************************************\n",
      ">> Started processing chunk: 5\n",
      ">> Loaded chunk: 5\n",
      ">> Filtered chunk: 5\n",
      ">> Pipeline-processed chunk: 5\n",
      ">> Shape of the processed chunk: (4, 262146)\n",
      ">> Saved features and target for chunk: 5\n",
      ">> Completed processing chunk: 5\n",
      "It took: 0.0 minutes to process this chunk.\n",
      " >>> Completed Sgd regressor with a validation rmse of 232.933945966\n",
      " >>> Completed Pagg regressor with a validation rmse of 105.065836714\n",
      "****************************************\n",
      ">> Completed incremental learning for chunk: 5\n",
      "********************************************************************************\n",
      ">> Started processing chunk: 6\n",
      ">> Loaded chunk: 6\n",
      ">> Filtered chunk: 6\n",
      ">> Pipeline-processed chunk: 6\n",
      ">> Shape of the processed chunk: (5, 262146)\n",
      ">> Saved features and target for chunk: 6\n",
      ">> Completed processing chunk: 6\n",
      "It took: 0.0 minutes to process this chunk.\n",
      " >>> Completed Sgd regressor with a validation rmse of 151.314867217\n",
      " >>> Completed Pagg regressor with a validation rmse of 107.196631998\n",
      "****************************************\n",
      ">> Completed incremental learning for chunk: 6\n",
      "********************************************************************************\n",
      ">> Started processing chunk: 7\n",
      ">> Loaded chunk: 7\n",
      ">> Filtered chunk: 7\n",
      ">> Pipeline-processed chunk: 7\n",
      ">> Shape of the processed chunk: (8, 262146)\n",
      ">> Saved features and target for chunk: 7\n",
      ">> Completed processing chunk: 7\n",
      "It took: 0.0 minutes to process this chunk.\n",
      " >>> Completed Sgd regressor with a validation rmse of 146.914131985\n",
      " >>> Completed Pagg regressor with a validation rmse of 111.946632367\n",
      "****************************************\n",
      ">> Completed incremental learning for chunk: 7\n",
      "********************************************************************************\n",
      ">> Started processing chunk: 8\n",
      ">> Loaded chunk: 8\n",
      ">> Filtered chunk: 8\n",
      ">> Pipeline-processed chunk: 8\n",
      ">> Shape of the processed chunk: (8, 262146)\n",
      ">> Saved features and target for chunk: 8\n",
      ">> Completed processing chunk: 8\n",
      "It took: 0.0 minutes to process this chunk.\n",
      " >>> Completed Sgd regressor with a validation rmse of 164.091252457\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "chunksize = 10 # This is the lmiting factor for the memory\n",
    "iterations = 3400\n",
    "\n",
    "# Function to return validation rmse scores\n",
    "def calculate_rmse(X_val_trans,y_val,reg):\n",
    "    preds = reg.predict(X_val_trans)\n",
    "    return np.sqrt(mean_squared_error(y_true=y_val,y_pred=preds))    \n",
    "\n",
    "# Load previously procesed and loacked-down validation data\n",
    "with open(disk+\"X_val_proc.pkl\",\"rb\") as f:\n",
    "    X_val_proc = pickle.load(f)\n",
    "print(\"Loaded processed validation set.\")\n",
    "\n",
    "\n",
    "with open(disk+\"y_val.pkl\",\"rb\") as f:\n",
    "    y_val = pickle.load(f)\n",
    "print(\"Loaded targets for validation set.\") \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    disk = \"/Volumes/Iomega_HDD/2016/Data science/Predictive-Modeling-510k-decision-time/\"\n",
    "\n",
    "    #####################################################################\n",
    "    # Streaming training data processing section\n",
    "    #####################################################################\n",
    "    with open(disk+\"chunk_number.pkl\", \"rb\") as f:\n",
    "        chunk_number = pickle.load(f)\n",
    "    print(\">> Started processing chunk: \"+ str(chunk_number))\n",
    "    start = datetime.now()\n",
    "    \n",
    "    # Get the number of rows to be skipped\n",
    "    with open(disk+\"skiprows.pkl\", \"rb\") as f:\n",
    "        skiprows=pickle.load(f)\n",
    "\n",
    "    # Read current_chunk from accumluating file: \"processed510kdata.csv\"\n",
    "    current_chunk = pd.read_csv(disk+\"processed510kdata.csv\", header= 0,\n",
    "                                skiprows= skiprows,nrows= chunksize,\n",
    "                               names = column_names)\n",
    "    print(\">> Loaded chunk: \"+ str(chunk_number))\n",
    "\n",
    "    # Determine the chunks to be skipped and update current_chunk\n",
    "    with open(disk+\"skip_knumber.pkl\", \"rb\") as f:\n",
    "        skip_knumber = pickle.load(f)\n",
    "\n",
    "    difference = pd.DataFrame()\n",
    "    difference[\"KNUMBER\"] = np.setdiff1d(current_chunk.KNUMBER,skip_knumber)\n",
    "    current_chunk = pd.merge(current_chunk,difference,on = 'KNUMBER')\n",
    "    print(\">> Filtered chunk: \"+ str(chunk_number))\n",
    "\n",
    "\n",
    "    # Seperate features and target before the pipeline step\n",
    "    X = current_chunk.drop(\"DECISIONTIME\", axis=1) # Features\n",
    "    y = current_chunk.DECISIONTIME # Target values\n",
    "\n",
    "    # Process the current chunk using pipeline\n",
    "    current_chunk_proc = pipeline510k.transform(X)\n",
    "    print(\">> Pipeline-processed chunk: \"+ str(chunk_number))\n",
    "    print(\">> Shape of the processed chunk: \"+ str(current_chunk_proc.shape))\n",
    "    \n",
    "    # Save the procesed current_chunk features and target into \"processed_chunks\" folder\n",
    "    # Note that we will use chunk_number for indexing these chunks so that we can aggregate them in the right order later\n",
    "    # Note that we are saving all 1048578 features without feature selection at the moment\n",
    "    \n",
    "    with open(disk+\"processed_chunks/X_train_chunk_proc_\"+str(chunk_number)+\".pkl\", \"wb\") as f:\n",
    "        pickle.dump(current_chunk_proc,f)\n",
    "\n",
    "    with open(disk+\"processed_chunks/y_train_chunk_proc_\"+str(chunk_number)+\".pkl\", \"wb\") as f:\n",
    "        pickle.dump(y,f)      \n",
    "\n",
    "    print(\">> Saved features and target for chunk: \"+ str(chunk_number))    \n",
    "    print(\">> Completed processing chunk: \"+ str(chunk_number))\n",
    "    process = datetime.now() - start\n",
    "    print(\"It took: \" + str(process.seconds/60) + \" minutes to process this chunk.\")\n",
    "    \n",
    "    #####################################################################\n",
    "    # Online (incremental learning) section\n",
    "    #####################################################################\n",
    "    ###########################\n",
    "    # Train online regressors\n",
    "    ##########################\n",
    "    \n",
    "    # Container to collect current rmse scores\n",
    "    scores = pd.DataFrame()\n",
    "    scores[\"chunk\"] = pd.Series(chunk_number)\n",
    "    scores[\"nsamples\"] = pd.Series(skiprows+chunksize)\n",
    "    \n",
    "    sgd.partial_fit(current_chunk_proc.toarray(),y)\n",
    "    sgd_val_score = calculate_rmse(X_val_proc.toarray(),y_val,sgd)\n",
    "    print(\" >>> Completed Sgd regressor with a validation rmse of \"+str(sgd_val_score))\n",
    "    pagg.partial_fit(current_chunk_proc.toarray(),y)    \n",
    "    pagg_val_score = calculate_rmse(X_val_proc.toarray(),y_val,pagg)          \n",
    "    print(\" >>> Completed Pagg regressor with a validation rmse of \"+str(pagg_val_score))\n",
    "    print(\"*\" * 40)\n",
    "    \n",
    "    scores[\"SGD\"] = pd.Series(sgd_val_score)\n",
    "    scores[\"PAGG\"] = pd.Series(pagg_val_score)\n",
    "    \n",
    "    # Update results data frame by adding scores\n",
    "    with open(disk+\"processed_chunks/results.pkl\", \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "    results = pd.concat([results,scores], axis = 0)\n",
    "    with open(disk+\"processed_chunks/results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results,f)\n",
    "        \n",
    "    print(\">> Completed incremental learning for chunk: \"+ str(chunk_number))\n",
    "    \n",
    "    #######################################\n",
    "    # Upon each sucessful process, update:\n",
    "    #######################################\n",
    "\n",
    "    # skip_knumber\n",
    "    skip_knumber.extend(current_chunk.KNUMBER.tolist())\n",
    "    with open(disk+\"skip_knumber.pkl\", \"wb\") as f:\n",
    "        pickle.dump(skip_knumber,f)\n",
    "\n",
    "    # skiprows\n",
    "    skiprows += chunksize\n",
    "    with open(disk+\"skiprows.pkl\", \"wb\") as f:\n",
    "        pickle.dump(skiprows,f)\n",
    "\n",
    "    # chunk_number\n",
    "    chunk_number += 1\n",
    "    with open(disk+\"chunk_number.pkl\", \"wb\") as f:\n",
    "        pickle.dump(chunk_number,f)\n",
    "    print(\"*\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>nsamples</th>\n",
       "      <th>SGDRegressor</th>\n",
       "      <th>PassiveAggressiveRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>122.006116</td>\n",
       "      <td>106.630278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>113.268200</td>\n",
       "      <td>155.717459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk nsamples  SGDRegressor  PassiveAggressiveRegressor\n",
       "0    NaN      100    122.006116                  106.630278\n",
       "0    NaN      200    113.268200                  155.717459"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(disk+\"processed_chunks/results.pkl\", \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "results        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'score_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-05d3b9c53cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_val_proc100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_proc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mf_regression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'score_func'"
     ]
    }
   ],
   "source": [
    "X_val_proc100 = SelectKBest(score_func=f_regression,k = 100)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

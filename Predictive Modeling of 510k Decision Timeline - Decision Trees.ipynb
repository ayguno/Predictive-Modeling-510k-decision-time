{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision-tree based algorithms for prediction\n",
    "\n",
    "In this part of the work, we will try to train decision-tree based algorithms to make predicitions about the timeline.\n",
    "\n",
    "### Building feature extraction pipeline\n",
    "\n",
    "Tree-based algorithms would require feature selection, so we will need to use a reasonable number of features. We will start by including 300 best features using our tokenization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the work into a dedicated workspace \"ridge\"\n",
    "disk_tree = \"tree\"\n",
    "import os\n",
    "if not os.path.exists(disk_tree):\n",
    "    os.makedirs(disk_tree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree1 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression, 300))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load Training and Validation sets\n",
    "disk = \"D:\\Data_science\\GitHub\\Predictive-Modeling-510k-decision-time\"\n",
    "# Validation set \n",
    "with open(disk+\"\\X_val.pkl\",\"rb\") as f:\n",
    "    X_val=pickle.load(f)\n",
    "\n",
    "with open(disk+\"\\y_val.pkl\",\"rb\") as f:\n",
    "    y_val=pickle.load(f)\n",
    "    \n",
    "# Training set (Locked down)\n",
    "with open(disk+\"\\X_train.pkl\",\"rb\") as f:\n",
    "    X_train=pickle.load(f)\n",
    "\n",
    "with open(disk+\"\\y_train.pkl\",\"rb\") as f:\n",
    "    y_train=pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.7833333333333333 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree = pipeline510k_tree1.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree = pipeline510k_tree1.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 300)\n",
      "(15899, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree.shape)\n",
    "print(X_val_trans_tree.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regresion\n",
    "\n",
    "We will first train an untuned Gradient Boosting algorithm to see performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7446            2.62m\n",
      "         2           0.7138            2.48m\n",
      "         3           0.6883            2.46m\n",
      "         4           0.6669            2.41m\n",
      "         5           0.6498            2.41m\n",
      "         6           0.6349            2.42m\n",
      "         7           0.6224            2.42m\n",
      "         8           0.6123            2.42m\n",
      "         9           0.6037            2.40m\n",
      "        10           0.5963            2.40m\n",
      "        20           0.5585            2.29m\n",
      "        30           0.5420            2.15m\n",
      "        40           0.5296            2.01m\n",
      "        50           0.5196            1.89m\n",
      "        60           0.5103            1.76m\n",
      "        70           0.5023            1.63m\n",
      "        80           0.4953            1.48m\n",
      "        90           0.4888            1.35m\n",
      "       100           0.4827            1.21m\n",
      "       200           0.4350            0.00s\n",
      "Median Absolute Error:  37.452053685765335\n",
      "Completed model fit and predictions in: 2.25 minutes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "gbm1 = GradientBoostingRegressor(verbose = 1, n_estimators= 200, max_depth=5)\n",
    "\n",
    "gbm1.fit(X_train_trans_tree, np.log(y_train))\n",
    "preds = gbm1.predict(X_val_trans_tree)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed: 49.8min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 75.3min\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed: 115.8min\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 125.9min\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed: 170.8min\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed: 183.3min\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed: 204.8min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 217.8min\n",
      "[Parallel(n_jobs=6)]: Done 108 out of 108 | elapsed: 250.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 108 out of 108 | elapsed: 250.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed GridSearch in: 259.8333333333333 minutes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,500,1000],\n",
    "    'max_depth':[5,10,15],\n",
    "    'learning_rate': [0.1,0.25,0.75,1]\n",
    "}\n",
    "\n",
    "# We have 8 CPU cores, we will use 6 for this task\n",
    "gbmSearch1 = GridSearchCV(estimator= GradientBoostingRegressor(), \n",
    "                            param_grid= param_grid,\n",
    "                            n_jobs = 6,\n",
    "                            cv = 3,\n",
    "                            verbose = 10, scoring= 'neg_median_absolute_error'\n",
    "                            )\n",
    "gbmSearch1.fit(X_train_trans_tree, np.log(y_train))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed GridSearch in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=10, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbmSearch1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.38059952933966"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "# Let's see the validation MAE (in original target scale) from the best estimator\n",
    "preds = gbmSearch1.best_estimator_.predict(X_val_trans_tree)\n",
    "median_absolute_error(y_val,np.exp(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting\n",
    "\n",
    "Let's try to use relatively new ligtGBM package to see the performance. The package has a sklearn interface we will use to perform hyperparameter optimization as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  36.951519001311084\n",
      "Completed model fit and predictions in: 0.016666666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression')\n",
    "\n",
    "lgbm1.fit(X_train_trans_tree, np.log(y_train))\n",
    "preds = lgbm1.predict(X_val_trans_tree)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that lightGBM is indeed extremely faster and performed better than even the tuned traditional GBM. We will experiment this algorithm and attempt hyperparameter optimization. Let's define a new pipeline without feature selection to also experiment the impact of adding more features on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree2 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()) # Scale the features\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.75 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree2 = pipeline510k_tree2.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree2 = pipeline510k_tree2.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 262146)\n",
      "(15899, 262146)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree2.shape)\n",
    "print(X_val_trans_tree2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error:  29.467624324930185\n",
      "Completed model fit and predictions in: 1.3666666666666667 minutes.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "Xt = SelectKBest(f_regression,10000).fit(X_train_trans_tree2,y_train).transform(X_train_trans_tree2)\n",
    "Xv = SelectKBest(f_regression,10000).fit(X_train_trans_tree2,y_train).transform(X_val_trans_tree2)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                          n_estimators=200, reg_alpha= 0.1,\n",
    "                          boosting_type= 'dart')\n",
    "\n",
    "lgbm1.fit(Xt, np.log(y_train))\n",
    "preds = lgbm1.predict(Xv)\n",
    "\n",
    "mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Median Absolute Error: \", str(mae))\n",
    "print(\"Completed model fit and predictions in: \" + str((end-start).seconds/60) + \" minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lgbm is robust to overfitting when adding more features. Let's try to first search for the number of features to be included in the fixed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model using 300\n",
      "Completed model fit and predictions using 300 in: 0.1 minutes.\n",
      "Median Absolute Error:  33.107162652136324\n",
      "**************************************************\n",
      "Training model using 5000\n",
      "Completed model fit and predictions using 5000 in: 0.8833333333333333 minutes.\n",
      "Median Absolute Error:  29.633671997998164\n",
      "**************************************************\n",
      "Training model using 10000\n",
      "Completed model fit and predictions using 10000 in: 1.4 minutes.\n",
      "Median Absolute Error:  29.467624324930185\n",
      "**************************************************\n",
      "Training model using 20000\n",
      "Completed model fit and predictions using 20000 in: 1.6666666666666667 minutes.\n",
      "Median Absolute Error:  29.30898855444397\n",
      "**************************************************\n",
      "Training model using 50000\n",
      "Completed model fit and predictions using 50000 in: 1.9 minutes.\n",
      "Median Absolute Error:  29.209825132888795\n",
      "**************************************************\n",
      "Training model using 100000\n",
      "Completed model fit and predictions using 100000 in: 2.0166666666666666 minutes.\n",
      "Median Absolute Error:  29.166814835270287\n",
      "**************************************************\n",
      "Training model using 200000\n",
      "Completed model fit and predictions using 200000 in: 2.066666666666667 minutes.\n",
      "Median Absolute Error:  29.19324890053349\n",
      "**************************************************\n",
      "Training model using 262146\n",
      "Completed model fit and predictions using 262146 in: 2.066666666666667 minutes.\n",
      "Median Absolute Error:  29.478673965840073\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "n_features_list = [300,5000,10000,20000,50000,100000,200000,262146]\n",
    "mae_list = []\n",
    "time_list = []\n",
    "\n",
    "for n_features in n_features_list:\n",
    "    print(\"Training model using \"+ str(n_features))\n",
    "    \n",
    "    # Testing feature selection based on training set\n",
    "    Xt = SelectKBest(f_regression,n_features).fit(X_train_trans_tree2,y_train).transform(X_train_trans_tree2)\n",
    "    Xv = SelectKBest(f_regression,n_features).fit(X_train_trans_tree2,y_train).transform(X_val_trans_tree2)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Fixed model structure \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= 300,\n",
    "                              n_estimators=200, reg_alpha= 0.1,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(Xt, np.log(y_train))\n",
    "    preds = lgbm1.predict(Xv)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    mae_list.append(mae)\n",
    "    time_list.append((end-start).seconds/60)\n",
    "\n",
    "    print(\"Completed model fit and predictions using \"+ str(n_features) + \" in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "    print(\"Median Absolute Error: \", str(mae))\n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like model starts to overfit beyond 100000 training features. We will define the new pipeline, transform the data sets before further hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM Feature extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MaxAbsScaler,FunctionTransformer, Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# First we build two utility functions to parse numeric and text data, \n",
    "# and wrap them using FunctionTransformer, so that they can be integrated into a sklearn pipeline:\n",
    "def text_columns(X_train):\n",
    "    return X_train.TEXT_FEATURES\n",
    "\n",
    "def numeric_columns(X_train):\n",
    "    numeric = ['APPLICANT_PRIOR_CLEARANCE_TO_DATE','DEVICENAME_PRIOR_CLEARANCE_TO_DATE']\n",
    "    temp = X_train[numeric]\n",
    "    return temp\n",
    "\n",
    "get_numeric_data = FunctionTransformer(func = numeric_columns, validate=False) \n",
    "get_text_data = FunctionTransformer(func = text_columns,validate=False) \n",
    "# Note how we avoid putting any arguments into text_columns and numeric_columns\n",
    "\n",
    "# We also need to create our regex token pattern to use in HashingVectorizer. \n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'   \n",
    "#Note this regex will match either a whitespace or a punctuation to tokenize the string vector on these preferences  \n",
    "\n",
    "# We also need to redefine the default feature selection function for regression to properly place into our pipeline:\n",
    "def f_regression(X,Y):\n",
    "    import sklearn\n",
    "    return sklearn.feature_selection.f_regression(X,Y,center = False) # default is center = True\n",
    "\n",
    "\n",
    "pipeline510k_tree3 = Pipeline([\n",
    "    \n",
    "    (\"union\",FeatureUnion( # Note that FeatureUnion() accepts list of tuples, the first half of each tuple is the name of the transformer\n",
    "        \n",
    "        transformer_list = [\n",
    "            \n",
    "            (\"numeric_subpipeline\", Pipeline([ # Note we have subpipeline branches inside the main pipeline\n",
    "                \n",
    "                (\"parser\",get_numeric_data), # Step1: parse the numeric data (note how we avoid () when using FunctionTransformer objects)\n",
    "                (\"imputer\",Imputer()), # Step2: impute missing values (we don't expect any)\n",
    "            \n",
    "            ])), # Branching point of the FeatureUnion\n",
    "            \n",
    "            (\"text_subpipeline\",Pipeline([\n",
    "            \n",
    "                (\"parser\",get_text_data), # Step1: parse the text data \n",
    "                (\"tokenizer\",HashingVectorizer(token_pattern= TOKENS_ALPHANUMERIC,n_features= 2 ** 18,decode_error='ignore',\n",
    "                                             stop_words = \"english\",# We will remove English stop words before tokenization\n",
    "                                             ngram_range = (1,1), # We will tokenize to single words only\n",
    "                                             non_negative=True, norm=None, binary=True  \n",
    "                                            )) # Step2: use HashingVectorizer for automated tokenization and feature extraction\n",
    "                                           \n",
    "                \n",
    "            ]))\n",
    "        ]\n",
    "    \n",
    "    )),# Branching point to the main pipeline: at this point all features are numeric\n",
    "    \n",
    "    (\"scaler\",MaxAbsScaler()), # Scale the features\n",
    "    (\"dim_red\", SelectKBest(f_regression, 100000))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing X_train in: 0.8 minutes.\n",
      "Completed processing X_val in: 0.18333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_train_trans_tree3 = pipeline510k_tree3.fit(X_train, y_train).transform(X_train)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_train in: \" + str((end-start).seconds/60) + \" minutes.\")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "X_val_trans_tree3 = pipeline510k_tree3.transform(X_val)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed processing X_val in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32275, 100000)\n",
      "(15899, 100000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trans_tree3.shape)\n",
    "print(X_val_trans_tree3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 192 candidates, totalling 1152 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed: 40.0min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 45.6min\n",
      "[Parallel(n_jobs=6)]: Done 101 tasks      | elapsed: 53.6min\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed: 67.4min\n",
      "[Parallel(n_jobs=6)]: Done 133 tasks      | elapsed: 77.6min\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed: 88.1min\n",
      "[Parallel(n_jobs=6)]: Done 169 tasks      | elapsed: 109.2min\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed: 119.2min\n",
      "[Parallel(n_jobs=6)]: Done 209 tasks      | elapsed: 134.5min\n",
      "[Parallel(n_jobs=6)]: Done 230 tasks      | elapsed: 147.4min\n",
      "[Parallel(n_jobs=6)]: Done 253 tasks      | elapsed: 166.2min\n",
      "[Parallel(n_jobs=6)]: Done 276 tasks      | elapsed: 183.2min\n",
      "[Parallel(n_jobs=6)]: Done 301 tasks      | elapsed: 207.8min\n",
      "[Parallel(n_jobs=6)]: Done 326 tasks      | elapsed: 230.0min\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed: 261.4min\n",
      "[Parallel(n_jobs=6)]: Done 380 tasks      | elapsed: 287.1min\n",
      "[Parallel(n_jobs=6)]: Done 409 tasks      | elapsed: 316.0min\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed: 337.7min\n",
      "[Parallel(n_jobs=6)]: Done 469 tasks      | elapsed: 372.9min\n",
      "[Parallel(n_jobs=6)]: Done 500 tasks      | elapsed: 416.7min\n",
      "[Parallel(n_jobs=6)]: Done 533 tasks      | elapsed: 453.6min\n",
      "[Parallel(n_jobs=6)]: Done 566 tasks      | elapsed: 506.1min\n",
      "[Parallel(n_jobs=6)]: Done 601 tasks      | elapsed: 547.0min\n",
      "[Parallel(n_jobs=6)]: Done 636 tasks      | elapsed: 583.7min\n",
      "[Parallel(n_jobs=6)]: Done 673 tasks      | elapsed: 636.1min\n",
      "[Parallel(n_jobs=6)]: Done 710 tasks      | elapsed: 696.0min\n",
      "[Parallel(n_jobs=6)]: Done 749 tasks      | elapsed: 774.8min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed: 840.9min\n",
      "[Parallel(n_jobs=6)]: Done 829 tasks      | elapsed: 926.1min\n",
      "[Parallel(n_jobs=6)]: Done 870 tasks      | elapsed: 1009.0min\n",
      "[Parallel(n_jobs=6)]: Done 913 tasks      | elapsed: 1143.1min\n",
      "[Parallel(n_jobs=6)]: Done 956 tasks      | elapsed: 1269.8min\n",
      "[Parallel(n_jobs=6)]: Done 1001 tasks      | elapsed: 1381.4min\n",
      "[Parallel(n_jobs=6)]: Done 1046 tasks      | elapsed: 1511.5min\n",
      "[Parallel(n_jobs=6)]: Done 1093 tasks      | elapsed: 1682.7min\n",
      "[Parallel(n_jobs=6)]: Done 1140 tasks      | elapsed: 1884.7min\n",
      "[Parallel(n_jobs=6)]: Done 1152 out of 1152 | elapsed: 1904.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed GridSearch in: 467.56666666666666 minutes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,150,200,250,400,500],\n",
    "    'num_leaves':[150,200,300,400],\n",
    "    'reg_alpha': [0,0.01,0.05,0.1,0.5,1,5,10],\n",
    "    'objective': ['regression'],\n",
    "    'boosting_type': ['dart']\n",
    "}\n",
    "\n",
    "# We have 8 CPU cores, we will use 6 for this task\n",
    "lgbmSearch1 = GridSearchCV(estimator= lgb.LGBMRegressor(), \n",
    "                            param_grid= param_grid,\n",
    "                            n_jobs = 6,\n",
    "                            cv = 6,\n",
    "                            verbose = 10, scoring= 'neg_median_absolute_error'\n",
    "                            )\n",
    "\n",
    "lgbmSearch1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Completed GridSearch in: \" + str((end-start).seconds/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=500,\n",
       "       n_jobs=-1, num_leaves=150, objective='regression',\n",
       "       random_state=None, reg_alpha=0.1, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbmSearch1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.38894595686144295"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbmSearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.504873585145397"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "# Let's see the validation MAE (in original target scale) from the best estimator\n",
    "preds = lgbmSearch1.best_estimator_.predict(X_val_trans_tree3)\n",
    "median_absolute_error(y_val,np.exp(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the score we want to be maximized using Bayesian Optimization\n",
    "# Return MEAN validated 'roc_auc' score from xgb Classifier\n",
    "# Note that the 4 parameters we will optimize are called as generic arguments\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "def lgbm_score(num_leaves,n_estimators,reg_alpha):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import numpy as np\n",
    "    \n",
    "    lgbm1 = lgb.LGBMRegressor(objective= 'regression', num_leaves= int(num_leaves),\n",
    "                              n_estimators= int(n_estimators), reg_alpha= reg_alpha,\n",
    "                              boosting_type= 'dart')\n",
    "\n",
    "    lgbm1.fit(X_train_trans_tree3, np.log(y_train))\n",
    "    preds = lgbm1.predict(X_val_trans_tree3)\n",
    "\n",
    "    # Out-of-box performance using validation set\n",
    "    mae = median_absolute_error(y_val,np.exp(preds))\n",
    "    \n",
    "    # return the - mae to be maximized\n",
    "    return - mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 01m16s | \u001b[35m -35.12445\u001b[0m | \u001b[32m      161.3516\u001b[0m | \u001b[32m    206.2586\u001b[0m | \u001b[32m     0.0548\u001b[0m | \n",
      "    2 | 01m58s | \u001b[35m -31.10728\u001b[0m | \u001b[32m      266.5394\u001b[0m | \u001b[32m    246.0457\u001b[0m | \u001b[32m     0.8177\u001b[0m | \n",
      "    3 | 02m06s | \u001b[35m -30.57605\u001b[0m | \u001b[32m      274.9119\u001b[0m | \u001b[32m    292.5025\u001b[0m | \u001b[32m     0.8854\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    4 | 01m47s | \u001b[35m -30.11682\u001b[0m | \u001b[32m      300.0000\u001b[0m | \u001b[32m    150.0000\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "    5 | 03m14s | \u001b[35m -29.74980\u001b[0m | \u001b[32m      300.0000\u001b[0m | \u001b[32m    300.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    6 | 02m16s |  -30.23047 |       300.0000 |     300.0000 |      1.0000 | \n",
      "    7 | 01m38s |  -30.90307 |       150.0000 |     300.0000 |      0.0000 | \n",
      "    8 | 01m05s |  -31.16249 |       150.0000 |     150.0000 |      0.0000 | \n",
      "    9 | 02m34s | \u001b[35m -29.10945\u001b[0m | \u001b[32m      231.1680\u001b[0m | \u001b[32m    300.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "   10 | 02m44s |  -29.77941 |       300.0000 |     227.7304 |      0.0000 | \n",
      "   11 | 02m40s |  -29.13963 |       245.7091 |     300.0000 |      0.0000 | \n",
      "   12 | 02m35s |  -29.20123 |       237.4534 |     300.0000 |      0.0000 | \n",
      "   13 | 01m36s |  -29.24059 |       231.9763 |     150.0000 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (150, 300),\n",
    "    'n_estimators' : (150,300),\n",
    "    'reg_alpha': (0,1)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=3,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian optimization is promising to find parameter that may improve the performance. Let's try another round of optimization cycle by focusing on a narrower hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m51s | \u001b[35m -29.02788\u001b[0m | \u001b[32m      244.9824\u001b[0m | \u001b[32m    331.2563\u001b[0m | \u001b[32m     0.0000\u001b[0m | \n",
      "    2 | 02m36s |  -29.04927 |       221.6442 |     360.4335 |      0.0098 | \n",
      "    3 | 03m05s |  -29.39139 |       244.5303 |     394.5018 |      0.0034 | \n",
      "    4 | 02m32s |  -29.36064 |       246.5605 |     298.3245 |      0.0009 | \n",
      "    5 | 02m53s |  -29.93163 |       241.6704 |     375.4623 |      0.0039 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    6 | 02m18s |  -29.10648 |       220.0000 |     290.0000 |      0.0000 | \n",
      "    7 | 02m52s |  -29.40106 |       220.0000 |     400.0000 |      0.0000 | \n",
      "    8 | 02m28s | \u001b[35m -29.02445\u001b[0m | \u001b[32m      220.0000\u001b[0m | \u001b[32m    334.1820\u001b[0m | \u001b[32m     0.0100\u001b[0m | \n",
      "    9 | 03m13s |  -29.80390 |       250.0000 |     400.0000 |      0.0100 | \n",
      "   10 | 02m44s |  -29.44771 |       250.0000 |     290.0000 |      0.0000 | \n",
      "   11 | 02m55s |  -29.49083 |       250.0000 |     318.5344 |      0.0000 | \n",
      "   12 | 02m42s |  -29.07049 |       220.0000 |     348.3952 |      0.0000 | \n",
      "   13 | 02m26s | \u001b[35m -28.82805\u001b[0m | \u001b[32m      220.0000\u001b[0m | \u001b[32m    313.9619\u001b[0m | \u001b[32m     0.0100\u001b[0m | \n",
      "   14 | 02m48s |  -29.04563 |       220.0000 |     375.1665 |      0.0000 | \n",
      "   15 | 02m30s |  -29.25785 |       220.0000 |     319.4859 |      0.0000 | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (290, 400),\n",
    "    'n_estimators' : (220,250),\n",
    "    'reg_alpha': (0,0.01)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to find parameters that further improved performance. Let's try to further narrow down the hyperparameter space and perform another round of Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   n_estimators |   num_leaves |   reg_alpha | \n",
      "    1 | 02m29s | \u001b[35m -29.10137\u001b[0m | \u001b[32m      224.9824\u001b[0m | \u001b[32m    312.5034\u001b[0m | \u001b[32m     0.0001\u001b[0m | \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# alpha is a parameter for the gaussian process\n",
    "# Note that this is itself a hyperparemter that can be optimized.\n",
    "gp_params = {\"alpha\": 1e-10}\n",
    "\n",
    "# We create the BayesianOptimization objects using the functions that utilize\n",
    "# the respective classifiers and return cross-validated scores to be optimized.\n",
    "\n",
    "seed = 112 # Random seed\n",
    "\n",
    "# We create the bayes_opt object and pass the function to be maximized\n",
    "# together with the parameters names and their bounds.\n",
    "# Note the syntax of bayes_opt package: bounds of hyperparameters are passed as two-tuples\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'num_leaves': (290, 350),\n",
    "    'n_estimators' : (200,230),\n",
    "    'reg_alpha': (0,0.025)\n",
    "}\n",
    "\n",
    "lgbmBO = BayesianOptimization(f = lgbm_score, \n",
    "                             pbounds =  hyperparameter_space,\n",
    "                             random_state = seed,\n",
    "                             verbose = 10)\n",
    "\n",
    "# Finally we call .maximize method of the optimizer with the appropriate arguments\n",
    "# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n",
    "# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n",
    "# 10 interations to maximize the value of estimator function\n",
    "lgbmBO.maximize(init_points=5,n_iter=10,acq='ucb', kappa= 3, **gp_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
